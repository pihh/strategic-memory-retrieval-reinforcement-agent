{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f92b2a72-5fe0-4b47-8868-34cfacaf0d34",
   "metadata": {},
   "outputs": [],
   "source": [
    "from environments import MemoryTaskEnv\n",
    "from agent import StrategicMemoryAgent\n",
    "from memory import StrategicMemoryBuffer,MemoryTransformerPolicy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bc12f4e9-a95c-439c-9efa-69678cf4bb62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SETUP ===================================\n",
    "DELAY = 8\n",
    "MEM_DIM = 32\n",
    "N_EPISODES = 2500\n",
    "N_MEMORIES = 16\n",
    "\n",
    "AGENT_KWARGS = dict(\n",
    "    device=\"cpu\",\n",
    "    verbose=1,\n",
    "    lam=0.95, \n",
    "    gamma=0.99, \n",
    "    ent_coef=0.01,\n",
    "    learning_rate=1e-3, \n",
    "    \n",
    "    her=False,\n",
    "    reward_norm=False,\n",
    "    aux_modules=None,\n",
    "    \n",
    "    intrinsic_expl=True,\n",
    "    intrinsic_eta=0.01,\n",
    "    \n",
    "    use_rnd=True, \n",
    "    rnd_emb_dim=32, \n",
    "    rnd_lr=1e-3,\n",
    ")\n",
    "# HELPERS =================================\n",
    "def total_timesteps(delay,n_episodes):\n",
    "    return delay * n_episodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fb83c049-bf57-461e-acfc-21ba913f42ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\filip_a58djhu\\Dev\\strategic-memory-retrieval-reinforcement-agent\\memory.py:53: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at C:\\b\\abs_dd5ga5p9x7\\croot\\libtorch_1746637520098\\work\\torch\\csrc\\utils\\tensor_new.cpp:281.)\n",
      "  traj = torch.tensor(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        |    8.000  |\n",
      "|    ep_rew_mean        |    0.146  |\n",
      "|    ep_rew_std         |    0.990  |\n",
      "|    policy_entropy     |    0.175  |\n",
      "|    advantage_mean     |   -0.905  |\n",
      "|    advantage_std      |    0.234  |\n",
      "|    aux_loss_mean      |    0.000  |\n",
      "| time/                 |           |\n",
      "|    fps                |      199  |\n",
      "|    episodes           |      100  |\n",
      "|    time_elapsed       |        4  |\n",
      "|    total_timesteps    |      800  |\n",
      "| train/                |           |\n",
      "|    loss               |    0.110  |\n",
      "|    policy_loss        |   -0.321  |\n",
      "|    value_loss         |    0.866  |\n",
      "|    explained_variance |   -2.438  |\n",
      "|    n_updates          |      100  |\n",
      "|    progress           |  4.00%    |\n",
      "| rnd_net_dist/         |           |\n",
      "|    mean_rnd_bonus     |    0.002  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        |    8.000  |\n",
      "|    ep_rew_mean        |    0.262  |\n",
      "|    ep_rew_std         |    0.966  |\n",
      "|    policy_entropy     |    0.224  |\n",
      "|    advantage_mean     |    0.362  |\n",
      "|    advantage_std      |    0.085  |\n",
      "|    aux_loss_mean      |    0.000  |\n",
      "| time/                 |           |\n",
      "|    fps                |      199  |\n",
      "|    episodes           |      200  |\n",
      "|    time_elapsed       |        8  |\n",
      "|    total_timesteps    |     1600  |\n",
      "| train/                |           |\n",
      "|    loss               |    0.679  |\n",
      "|    policy_loss        |    0.612  |\n",
      "|    value_loss         |    0.137  |\n",
      "|    explained_variance |   -0.742  |\n",
      "|    n_updates          |      200  |\n",
      "|    progress           |  8.00%    |\n",
      "| rnd_net_dist/         |           |\n",
      "|    mean_rnd_bonus     |    0.000  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        |    8.000  |\n",
      "|    ep_rew_mean        |    0.842  |\n",
      "|    ep_rew_std         |    0.543  |\n",
      "|    policy_entropy     |    0.155  |\n",
      "|    advantage_mean     |    0.015  |\n",
      "|    advantage_std      |    0.080  |\n",
      "|    aux_loss_mean      |    0.000  |\n",
      "| time/                 |           |\n",
      "|    fps                |      199  |\n",
      "|    episodes           |      300  |\n",
      "|    time_elapsed       |       12  |\n",
      "|    total_timesteps    |     2400  |\n",
      "| train/                |           |\n",
      "|    loss               |    0.017  |\n",
      "|    policy_loss        |    0.016  |\n",
      "|    value_loss         |    0.006  |\n",
      "|    explained_variance |   -7.865  |\n",
      "|    n_updates          |      300  |\n",
      "|    progress           |  12.00%   |\n",
      "| rnd_net_dist/         |           |\n",
      "|    mean_rnd_bonus     |    0.000  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        |    8.000  |\n",
      "|    ep_rew_mean        |    0.982  |\n",
      "|    ep_rew_std         |    0.199  |\n",
      "|    policy_entropy     |    0.162  |\n",
      "|    advantage_mean     |    0.001  |\n",
      "|    advantage_std      |    0.041  |\n",
      "|    aux_loss_mean      |    0.000  |\n",
      "| time/                 |           |\n",
      "|    fps                |      199  |\n",
      "|    episodes           |      400  |\n",
      "|    time_elapsed       |       16  |\n",
      "|    total_timesteps    |     3200  |\n",
      "| train/                |           |\n",
      "|    loss               |    0.003  |\n",
      "|    policy_loss        |    0.004  |\n",
      "|    value_loss         |    0.001  |\n",
      "|    explained_variance |   -2.065  |\n",
      "|    n_updates          |      400  |\n",
      "|    progress           |  16.00%   |\n",
      "| rnd_net_dist/         |           |\n",
      "|    mean_rnd_bonus     |    0.000  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        |    8.000  |\n",
      "|    ep_rew_mean        |    0.981  |\n",
      "|    ep_rew_std         |    0.199  |\n",
      "|    policy_entropy     |    0.208  |\n",
      "|    advantage_mean     |   -0.034  |\n",
      "|    advantage_std      |    0.095  |\n",
      "|    aux_loss_mean      |    0.000  |\n",
      "| time/                 |           |\n",
      "|    fps                |      199  |\n",
      "|    episodes           |      500  |\n",
      "|    time_elapsed       |       20  |\n",
      "|    total_timesteps    |     4000  |\n",
      "| train/                |           |\n",
      "|    loss               |    0.506  |\n",
      "|    policy_loss        |    0.504  |\n",
      "|    value_loss         |    0.009  |\n",
      "|    explained_variance |  -34.750  |\n",
      "|    n_updates          |      500  |\n",
      "|    progress           |  20.00%   |\n",
      "| rnd_net_dist/         |           |\n",
      "|    mean_rnd_bonus     |    0.000  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        |    8.000  |\n",
      "|    ep_rew_mean        |    0.961  |\n",
      "|    ep_rew_std         |    0.280  |\n",
      "|    policy_entropy     |    0.153  |\n",
      "|    advantage_mean     |   -0.053  |\n",
      "|    advantage_std      |    0.049  |\n",
      "|    aux_loss_mean      |    0.000  |\n",
      "| time/                 |           |\n",
      "|    fps                |      199  |\n",
      "|    episodes           |      600  |\n",
      "|    time_elapsed       |       24  |\n",
      "|    total_timesteps    |     4800  |\n",
      "| train/                |           |\n",
      "|    loss               |   -0.017  |\n",
      "|    policy_loss        |   -0.018  |\n",
      "|    value_loss         |    0.005  |\n",
      "|    explained_variance |   -8.933  |\n",
      "|    n_updates          |      600  |\n",
      "|    progress           |  24.00%   |\n",
      "| rnd_net_dist/         |           |\n",
      "|    mean_rnd_bonus     |    0.000  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        |    8.000  |\n",
      "|    ep_rew_mean        |    0.981  |\n",
      "|    ep_rew_std         |    0.199  |\n",
      "|    policy_entropy     |    0.145  |\n",
      "|    advantage_mean     |    0.108  |\n",
      "|    advantage_std      |    0.039  |\n",
      "|    aux_loss_mean      |    0.000  |\n",
      "| time/                 |           |\n",
      "|    fps                |      199  |\n",
      "|    episodes           |      700  |\n",
      "|    time_elapsed       |       28  |\n",
      "|    total_timesteps    |     5600  |\n",
      "| train/                |           |\n",
      "|    loss               |    0.109  |\n",
      "|    policy_loss        |    0.104  |\n",
      "|    value_loss         |    0.013  |\n",
      "|    explained_variance |   -0.160  |\n",
      "|    n_updates          |      700  |\n",
      "|    progress           |  28.00%   |\n",
      "| rnd_net_dist/         |           |\n",
      "|    mean_rnd_bonus     |    0.000  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        |    8.000  |\n",
      "|    ep_rew_mean        |    1.001  |\n",
      "|    ep_rew_std         |    0.000  |\n",
      "|    policy_entropy     |    0.139  |\n",
      "|    advantage_mean     |    0.011  |\n",
      "|    advantage_std      |    0.040  |\n",
      "|    aux_loss_mean      |    0.000  |\n",
      "| time/                 |           |\n",
      "|    fps                |      193  |\n",
      "|    episodes           |      800  |\n",
      "|    time_elapsed       |       33  |\n",
      "|    total_timesteps    |     6400  |\n",
      "| train/                |           |\n",
      "|    loss               |    0.385  |\n",
      "|    policy_loss        |    0.386  |\n",
      "|    value_loss         |    0.002  |\n",
      "|    explained_variance |   -1.438  |\n",
      "|    n_updates          |      800  |\n",
      "|    progress           |  32.00%   |\n",
      "| rnd_net_dist/         |           |\n",
      "|    mean_rnd_bonus     |    0.000  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        |    8.000  |\n",
      "|    ep_rew_mean        |    1.001  |\n",
      "|    ep_rew_std         |    0.000  |\n",
      "|    policy_entropy     |    0.142  |\n",
      "|    advantage_mean     |   -0.007  |\n",
      "|    advantage_std      |    0.044  |\n",
      "|    aux_loss_mean      |    0.000  |\n",
      "| time/                 |           |\n",
      "|    fps                |      194  |\n",
      "|    episodes           |      900  |\n",
      "|    time_elapsed       |       37  |\n",
      "|    total_timesteps    |     7200  |\n",
      "| train/                |           |\n",
      "|    loss               |    0.032  |\n",
      "|    policy_loss        |    0.032  |\n",
      "|    value_loss         |    0.002  |\n",
      "|    explained_variance |   -2.958  |\n",
      "|    n_updates          |      900  |\n",
      "|    progress           |  36.00%   |\n",
      "| rnd_net_dist/         |           |\n",
      "|    mean_rnd_bonus     |    0.000  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        |    8.000  |\n",
      "|    ep_rew_mean        |    1.001  |\n",
      "|    ep_rew_std         |    0.000  |\n",
      "|    policy_entropy     |    0.140  |\n",
      "|    advantage_mean     |   -0.018  |\n",
      "|    advantage_std      |    0.054  |\n",
      "|    aux_loss_mean      |    0.000  |\n",
      "| time/                 |           |\n",
      "|    fps                |      195  |\n",
      "|    episodes           |     1000  |\n",
      "|    time_elapsed       |       41  |\n",
      "|    total_timesteps    |     8000  |\n",
      "| train/                |           |\n",
      "|    loss               |   -0.000  |\n",
      "|    policy_loss        |   -0.000  |\n",
      "|    value_loss         |    0.003  |\n",
      "|    explained_variance |   -5.555  |\n",
      "|    n_updates          |     1000  |\n",
      "|    progress           |  40.00%   |\n",
      "| rnd_net_dist/         |           |\n",
      "|    mean_rnd_bonus     |    0.000  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        |    8.000  |\n",
      "|    ep_rew_mean        |    1.001  |\n",
      "|    ep_rew_std         |    0.000  |\n",
      "|    policy_entropy     |    0.155  |\n",
      "|    advantage_mean     |   -0.013  |\n",
      "|    advantage_std      |    0.039  |\n",
      "|    aux_loss_mean      |    0.000  |\n",
      "| time/                 |           |\n",
      "|    fps                |      195  |\n",
      "|    episodes           |     1100  |\n",
      "|    time_elapsed       |       45  |\n",
      "|    total_timesteps    |     8800  |\n",
      "| train/                |           |\n",
      "|    loss               |   -0.020  |\n",
      "|    policy_loss        |   -0.019  |\n",
      "|    value_loss         |    0.001  |\n",
      "|    explained_variance |   -1.931  |\n",
      "|    n_updates          |     1100  |\n",
      "|    progress           |  44.00%   |\n",
      "| rnd_net_dist/         |           |\n",
      "|    mean_rnd_bonus     |    0.000  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        |    8.000  |\n",
      "|    ep_rew_mean        |    0.981  |\n",
      "|    ep_rew_std         |    0.199  |\n",
      "|    policy_entropy     |    0.158  |\n",
      "|    advantage_mean     |    0.016  |\n",
      "|    advantage_std      |    0.041  |\n",
      "|    aux_loss_mean      |    0.000  |\n",
      "| time/                 |           |\n",
      "|    fps                |      191  |\n",
      "|    episodes           |     1200  |\n",
      "|    time_elapsed       |       50  |\n",
      "|    total_timesteps    |     9600  |\n",
      "| train/                |           |\n",
      "|    loss               |    0.060  |\n",
      "|    policy_loss        |    0.060  |\n",
      "|    value_loss         |    0.002  |\n",
      "|    explained_variance |   -1.722  |\n",
      "|    n_updates          |     1200  |\n",
      "|    progress           |  48.00%   |\n",
      "| rnd_net_dist/         |           |\n",
      "|    mean_rnd_bonus     |    0.000  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        |    8.000  |\n",
      "|    ep_rew_mean        |    0.981  |\n",
      "|    ep_rew_std         |    0.199  |\n",
      "|    policy_entropy     |    0.114  |\n",
      "|    advantage_mean     |   -0.001  |\n",
      "|    advantage_std      |    0.048  |\n",
      "|    aux_loss_mean      |    0.000  |\n",
      "| time/                 |           |\n",
      "|    fps                |      192  |\n",
      "|    episodes           |     1300  |\n",
      "|    time_elapsed       |       54  |\n",
      "|    total_timesteps    |    10400  |\n",
      "| train/                |           |\n",
      "|    loss               |   -0.002  |\n",
      "|    policy_loss        |   -0.002  |\n",
      "|    value_loss         |    0.002  |\n",
      "|    explained_variance |   -3.351  |\n",
      "|    n_updates          |     1300  |\n",
      "|    progress           |  52.00%   |\n",
      "| rnd_net_dist/         |           |\n",
      "|    mean_rnd_bonus     |    0.000  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        |    8.000  |\n",
      "|    ep_rew_mean        |    0.981  |\n",
      "|    ep_rew_std         |    0.199  |\n",
      "|    policy_entropy     |    0.124  |\n",
      "|    advantage_mean     |    0.025  |\n",
      "|    advantage_std      |    0.067  |\n",
      "|    aux_loss_mean      |    0.000  |\n",
      "| time/                 |           |\n",
      "|    fps                |      193  |\n",
      "|    episodes           |     1400  |\n",
      "|    time_elapsed       |       58  |\n",
      "|    total_timesteps    |    11200  |\n",
      "| train/                |           |\n",
      "|    loss               |    0.000  |\n",
      "|    policy_loss        |   -0.001  |\n",
      "|    value_loss         |    0.005  |\n",
      "|    explained_variance |   -6.089  |\n",
      "|    n_updates          |     1400  |\n",
      "|    progress           |  56.00%   |\n",
      "| rnd_net_dist/         |           |\n",
      "|    mean_rnd_bonus     |    0.000  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        |    8.000  |\n",
      "|    ep_rew_mean        |    0.981  |\n",
      "|    ep_rew_std         |    0.199  |\n",
      "|    policy_entropy     |    0.112  |\n",
      "|    advantage_mean     |    0.046  |\n",
      "|    advantage_std      |    0.042  |\n",
      "|    aux_loss_mean      |    0.000  |\n",
      "| time/                 |           |\n",
      "|    fps                |      190  |\n",
      "|    episodes           |     1500  |\n",
      "|    time_elapsed       |       63  |\n",
      "|    total_timesteps    |    12000  |\n",
      "| train/                |           |\n",
      "|    loss               |    0.056  |\n",
      "|    policy_loss        |    0.055  |\n",
      "|    value_loss         |    0.004  |\n",
      "|    explained_variance |   -1.224  |\n",
      "|    n_updates          |     1500  |\n",
      "|    progress           |  60.00%   |\n",
      "| rnd_net_dist/         |           |\n",
      "|    mean_rnd_bonus     |    0.000  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        |    8.000  |\n",
      "|    ep_rew_mean        |    1.001  |\n",
      "|    ep_rew_std         |    0.000  |\n",
      "|    policy_entropy     |    0.088  |\n",
      "|    advantage_mean     |    0.011  |\n",
      "|    advantage_std      |    0.055  |\n",
      "|    aux_loss_mean      |    0.000  |\n",
      "| time/                 |           |\n",
      "|    fps                |      191  |\n",
      "|    episodes           |     1600  |\n",
      "|    time_elapsed       |       67  |\n",
      "|    total_timesteps    |    12800  |\n",
      "| train/                |           |\n",
      "|    loss               |    0.030  |\n",
      "|    policy_loss        |    0.029  |\n",
      "|    value_loss         |    0.003  |\n",
      "|    explained_variance |   -3.911  |\n",
      "|    n_updates          |     1600  |\n",
      "|    progress           |  64.00%   |\n",
      "| rnd_net_dist/         |           |\n",
      "|    mean_rnd_bonus     |    0.000  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        |    8.000  |\n",
      "|    ep_rew_mean        |    0.961  |\n",
      "|    ep_rew_std         |    0.280  |\n",
      "|    policy_entropy     |    0.104  |\n",
      "|    advantage_mean     |    0.049  |\n",
      "|    advantage_std      |    0.044  |\n",
      "|    aux_loss_mean      |    0.000  |\n",
      "| time/                 |           |\n",
      "|    fps                |      191  |\n",
      "|    episodes           |     1700  |\n",
      "|    time_elapsed       |       71  |\n",
      "|    total_timesteps    |    13600  |\n",
      "| train/                |           |\n",
      "|    loss               |   -0.004  |\n",
      "|    policy_loss        |   -0.005  |\n",
      "|    value_loss         |    0.004  |\n",
      "|    explained_variance |   -1.193  |\n",
      "|    n_updates          |     1700  |\n",
      "|    progress           |  68.00%   |\n",
      "| rnd_net_dist/         |           |\n",
      "|    mean_rnd_bonus     |    0.000  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        |    8.000  |\n",
      "|    ep_rew_mean        |    0.981  |\n",
      "|    ep_rew_std         |    0.199  |\n",
      "|    policy_entropy     |    0.094  |\n",
      "|    advantage_mean     |   -0.005  |\n",
      "|    advantage_std      |    0.041  |\n",
      "|    aux_loss_mean      |    0.000  |\n",
      "| time/                 |           |\n",
      "|    fps                |      191  |\n",
      "|    episodes           |     1800  |\n",
      "|    time_elapsed       |       75  |\n",
      "|    total_timesteps    |    14400  |\n",
      "| train/                |           |\n",
      "|    loss               |   -0.026  |\n",
      "|    policy_loss        |   -0.025  |\n",
      "|    value_loss         |    0.002  |\n",
      "|    explained_variance |   -2.219  |\n",
      "|    n_updates          |     1800  |\n",
      "|    progress           |  72.00%   |\n",
      "| rnd_net_dist/         |           |\n",
      "|    mean_rnd_bonus     |    0.000  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        |    8.000  |\n",
      "|    ep_rew_mean        |    1.001  |\n",
      "|    ep_rew_std         |    0.000  |\n",
      "|    policy_entropy     |    0.095  |\n",
      "|    advantage_mean     |    0.006  |\n",
      "|    advantage_std      |    0.033  |\n",
      "|    aux_loss_mean      |    0.000  |\n",
      "| time/                 |           |\n",
      "|    fps                |      192  |\n",
      "|    episodes           |     1900  |\n",
      "|    time_elapsed       |       79  |\n",
      "|    total_timesteps    |    15200  |\n",
      "| train/                |           |\n",
      "|    loss               |    0.044  |\n",
      "|    policy_loss        |    0.045  |\n",
      "|    value_loss         |    0.001  |\n",
      "|    explained_variance |   -1.131  |\n",
      "|    n_updates          |     1900  |\n",
      "|    progress           |  76.00%   |\n",
      "| rnd_net_dist/         |           |\n",
      "|    mean_rnd_bonus     |    0.000  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        |    8.000  |\n",
      "|    ep_rew_mean        |    1.001  |\n",
      "|    ep_rew_std         |    0.000  |\n",
      "|    policy_entropy     |    0.094  |\n",
      "|    advantage_mean     |   -0.004  |\n",
      "|    advantage_std      |    0.023  |\n",
      "|    aux_loss_mean      |    0.000  |\n",
      "| time/                 |           |\n",
      "|    fps                |      192  |\n",
      "|    episodes           |     2000  |\n",
      "|    time_elapsed       |       83  |\n",
      "|    total_timesteps    |    16000  |\n",
      "| train/                |           |\n",
      "|    loss               |    0.013  |\n",
      "|    policy_loss        |    0.013  |\n",
      "|    value_loss         |    0.000  |\n",
      "|    explained_variance |    0.012  |\n",
      "|    n_updates          |     2000  |\n",
      "|    progress           |  80.00%   |\n",
      "| rnd_net_dist/         |           |\n",
      "|    mean_rnd_bonus     |    0.000  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        |    8.000  |\n",
      "|    ep_rew_mean        |    1.001  |\n",
      "|    ep_rew_std         |    0.000  |\n",
      "|    policy_entropy     |    0.092  |\n",
      "|    advantage_mean     |   -0.025  |\n",
      "|    advantage_std      |    0.019  |\n",
      "|    aux_loss_mean      |    0.000  |\n",
      "| time/                 |           |\n",
      "|    fps                |      190  |\n",
      "|    episodes           |     2100  |\n",
      "|    time_elapsed       |       88  |\n",
      "|    total_timesteps    |    16800  |\n",
      "| train/                |           |\n",
      "|    loss               |    0.000  |\n",
      "|    policy_loss        |    0.001  |\n",
      "|    value_loss         |    0.001  |\n",
      "|    explained_variance |    0.099  |\n",
      "|    n_updates          |     2100  |\n",
      "|    progress           |  84.00%   |\n",
      "| rnd_net_dist/         |           |\n",
      "|    mean_rnd_bonus     |    0.000  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        |    8.000  |\n",
      "|    ep_rew_mean        |    1.001  |\n",
      "|    ep_rew_std         |    0.000  |\n",
      "|    policy_entropy     |    0.094  |\n",
      "|    advantage_mean     |   -0.021  |\n",
      "|    advantage_std      |    0.033  |\n",
      "|    aux_loss_mean      |    0.000  |\n",
      "| time/                 |           |\n",
      "|    fps                |      193  |\n",
      "|    episodes           |     2200  |\n",
      "|    time_elapsed       |       91  |\n",
      "|    total_timesteps    |    17600  |\n",
      "| train/                |           |\n",
      "|    loss               |   -0.025  |\n",
      "|    policy_loss        |   -0.025  |\n",
      "|    value_loss         |    0.001  |\n",
      "|    explained_variance |   -1.679  |\n",
      "|    n_updates          |     2200  |\n",
      "|    progress           |  88.00%   |\n",
      "| rnd_net_dist/         |           |\n",
      "|    mean_rnd_bonus     |    0.000  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        |    8.000  |\n",
      "|    ep_rew_mean        |    1.001  |\n",
      "|    ep_rew_std         |    0.000  |\n",
      "|    policy_entropy     |    0.094  |\n",
      "|    advantage_mean     |   -0.023  |\n",
      "|    advantage_std      |    0.037  |\n",
      "|    aux_loss_mean      |    0.000  |\n",
      "| time/                 |           |\n",
      "|    fps                |      191  |\n",
      "|    episodes           |     2300  |\n",
      "|    time_elapsed       |       96  |\n",
      "|    total_timesteps    |    18400  |\n",
      "| train/                |           |\n",
      "|    loss               |    0.000  |\n",
      "|    policy_loss        |    0.000  |\n",
      "|    value_loss         |    0.002  |\n",
      "|    explained_variance |   -2.286  |\n",
      "|    n_updates          |     2300  |\n",
      "|    progress           |  92.00%   |\n",
      "| rnd_net_dist/         |           |\n",
      "|    mean_rnd_bonus     |    0.000  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        |    8.000  |\n",
      "|    ep_rew_mean        |    1.001  |\n",
      "|    ep_rew_std         |    0.000  |\n",
      "|    policy_entropy     |    0.094  |\n",
      "|    advantage_mean     |   -0.025  |\n",
      "|    advantage_std      |    0.018  |\n",
      "|    aux_loss_mean      |    0.000  |\n",
      "| time/                 |           |\n",
      "|    fps                |      191  |\n",
      "|    episodes           |     2400  |\n",
      "|    time_elapsed       |      100  |\n",
      "|    total_timesteps    |    19200  |\n",
      "| train/                |           |\n",
      "|    loss               |   -0.015  |\n",
      "|    policy_loss        |   -0.014  |\n",
      "|    value_loss         |    0.001  |\n",
      "|    explained_variance |    0.233  |\n",
      "|    n_updates          |     2400  |\n",
      "|    progress           |  96.00%   |\n",
      "| rnd_net_dist/         |           |\n",
      "|    mean_rnd_bonus     |    0.000  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        |    8.000  |\n",
      "|    ep_rew_mean        |    1.001  |\n",
      "|    ep_rew_std         |    0.000  |\n",
      "|    policy_entropy     |    0.094  |\n",
      "|    advantage_mean     |   -0.017  |\n",
      "|    advantage_std      |    0.037  |\n",
      "|    aux_loss_mean      |    0.000  |\n",
      "| time/                 |           |\n",
      "|    fps                |      192  |\n",
      "|    episodes           |     2500  |\n",
      "|    time_elapsed       |      104  |\n",
      "|    total_timesteps    |    20000  |\n",
      "| train/                |           |\n",
      "|    loss               |   -0.031  |\n",
      "|    policy_loss        |   -0.031  |\n",
      "|    value_loss         |    0.002  |\n",
      "|    explained_variance |   -1.753  |\n",
      "|    n_updates          |     2500  |\n",
      "|    progress           | 100.00%   |\n",
      "| rnd_net_dist/         |           |\n",
      "|    mean_rnd_bonus     |    0.000  |\n",
      "-------------------------------------\n",
      "Training complete. Total episodes: 2500, total steps: 20000\n"
     ]
    }
   ],
   "source": [
    "# ENVIRONMENT =============================\n",
    "env = MemoryTaskEnv(delay=DELAY, difficulty=0)\n",
    "\n",
    "# MEMORY BUFFER ===========================\n",
    "memory = StrategicMemoryBuffer(\n",
    "    obs_dim=env.observation_space.shape[0],\n",
    "    action_dim=1,          # For Discrete(2)\n",
    "    mem_dim=MEM_DIM,\n",
    "    max_entries=N_MEMORIES,\n",
    "    device=\"cpu\"\n",
    ")\n",
    "\n",
    "# POLICY NETWORK (use class) ==============\n",
    "policy = MemoryTransformerPolicy\n",
    "\n",
    "# (Optional) Auxiliary Modules \n",
    "# aux_modules = [\n",
    "#     CueAuxModule(feat_dim=MEM_DIM*2, n_classes=2),\n",
    "#     ConfidenceAuxModule(feat_dim=MEM_DIM*2)\n",
    "# ]\n",
    "\n",
    "# AGENT SETUP =============================\n",
    "agent = StrategicMemoryAgent(\n",
    "    policy_class=policy,\n",
    "    env=env,\n",
    "    memory=memory,\n",
    "    # aux_modules=aux_modules,  \n",
    "    **AGENT_KWARGS\n",
    ")\n",
    "\n",
    "# TRAIN THE AGENT =========================\n",
    "agent.learn(\n",
    "    total_timesteps=total_timesteps(DELAY, N_EPISODES),\n",
    "    log_interval=100\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7e26209-9156-4ebd-b90a-77aef5e4b5c8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:trading]",
   "language": "python",
   "name": "conda-env-trading-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
