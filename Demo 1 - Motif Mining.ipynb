{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "073e0b13-4d37-4135-a2cd-f49bc22f6a17",
   "metadata": {},
   "source": [
    "# TraceRL\n",
    "#### Strategic Recall for Reinforcement Learning. An Agent With Active, Learnable Memory\n",
    "\n",
    "---\n",
    "\n",
    "# Motif Mining / Combined Memory Module\n",
    "\n",
    "### What is a Motif (in time series/finance context):\n",
    "A motif is a short, distinctive, recurring pattern in time series data (e.g., “three rising candles”, “hammer candlestick”).\n",
    "\n",
    "Motif mining is about discovering these patterns that frequently occur, possibly before some important event (like price spikes).\n",
    "\n",
    "Motifs can be fixed length (e.g., always 3 bars) or variable length, but they're usually not tied to any particular \"agent experience\" or reward—they're just patterns that are statistically common or relevant to outcomes.\n",
    "\n",
    "### How does this differ from current agent’s memory?\n",
    "Motifs could become part of memory if they prove useful, but in RL, memory entries are scored and selected by usefulness to the agent’s task, not just frequency.\n",
    "\n",
    "* **Motif:**\n",
    "  * **Purely pattern-based:** “What sequence shapes show up often in the market?”\n",
    "  * **Unsupervised:** Does not depend on what the agent did or the rewards/outcomes.\n",
    "  * Often discovered with algorithms like matrix profile, SAX, or clustering over subsequences.\n",
    "\n",
    "* **Strategic RL Memory:**\n",
    "  * Stores sequences of observations, actions, rewards from actual episodes, tied to what the agent did and what outcome it got.\n",
    "  * Is used for retrieval during decision-making, not just for pattern mining.\n",
    "  * Memory can be trained to only keep those episodes/patterns that are useful for policy improvement, not just frequent.\n",
    "\n",
    "* **Summary:**\n",
    "  * **Motif:**  Statistically recurring pattern in the world\n",
    "  * **Memory_** Agent’s own experienced or retained pattern, which it can choose to use, forget, or score for future use\n",
    "\n",
    "\n",
    "### Goal:\n",
    "\n",
    "* Get both kinds of retrieval in one single process.\n",
    "\n",
    "### Summary:\n",
    "* All memory retrieval (episodic and motif) is neural, attention-based, and trainable.\n",
    "\n",
    "* Motif memory can be used for either unsupervised mining (offline DTW) or end-to-end learned patterns.\n",
    "\n",
    "* Everything is differentiable and ready for RL + auxiliary losses.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "**Author:** Filipe Sá  \n",
    "**Contact:** filipemotasa@hotmail.com | [GitHub](https://github.com/pihh/)\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f92b2a72-5fe0-4b47-8868-34cfacaf0d34",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\filip_a58djhu\\anaconda3\\envs\\trading\\lib\\site-packages\\gym\\envs\\registration.py:307: DeprecationWarning: The package name gym_minigrid has been deprecated in favor of minigrid. Please uninstall gym_minigrid and install minigrid with `pip install minigrid`. Future releases will be maintained under the new package name minigrid.\n",
      "  fn()\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from agent import TraceRL\n",
    "from environments import MemoryTaskEnv\n",
    "from benchmark import AgentPerformanceBenchmark\n",
    "from memory import StrategicMemoryBuffer,StrategicMemoryTransformerPolicy, CombinedMemoryModule, StrategicCombinedMemoryPolicy\n",
    "from motif import mine_motifs_from_buffer, MotifMemoryBank\n",
    "from constants import DEFAULT_DEVICE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8b8ab75-12fc-4ed8-bd0a-38f09aad5344",
   "metadata": {},
   "source": [
    "# Agent v1.4.0 \n",
    "Introduces motif mining and motif recall to support memory recall process\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "eb63fc1a-3ae8-47ed-967f-409e690af031",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import time\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.distributions import Categorical\n",
    "from tabulate import tabulate\n",
    "\n",
    "from core_modules import RewardNormalizer, StateCounter, RNDModule\n",
    "from core_calculations import compute_gae, compute_explained_variance\n",
    "from callbacks import print_sb3_style_log_box\n",
    "MINING_INTERVAL = 100 \n",
    "\n",
    "class TraceRL:\n",
    "    \"\"\"\n",
    "    Proximal Policy Optimization (PPO) agent with integrated external memory retrieval.\n",
    "\n",
    "    Features:\n",
    "        - Supports auxiliary losses, HER, reward normalization, and RND-based exploration.\n",
    "        - Episodic or contextual memory (passed as `memory`) for strategic RL.\n",
    "        - Plug-and-play auxiliary modules (e.g., cue, event, confidence).\n",
    "        - Stable training with reward normalization and intrinsic/extrinsic reward mixing.\n",
    "\n",
    "    Args:\n",
    "        policy_class (nn.Module): Policy network class (should accept obs_dim, memory, aux_modules).\n",
    "        env (gym.Env): Gymnasium environment.\n",
    "        verbose (int): Logging verbosity (0 = silent, 1 = logs).\n",
    "        learning_rate (float): Adam optimizer learning rate.\n",
    "        gamma (float): Discount factor.\n",
    "        lam (float): GAE lambda.\n",
    "        device (str): Torch device.\n",
    "        her (bool): Enable Hindsight Experience Replay (if supported by env).\n",
    "        reward_norm (bool): Normalize reward with running stats.\n",
    "        intrinsic_expl (bool): Use count-based intrinsic reward.\n",
    "        intrinsic_eta (float): Scaling for intrinsic bonus.\n",
    "        ent_coef (float): Entropy coefficient.\n",
    "        memory: Memory module for contextual/episodic learning (optional).\n",
    "        aux_modules (list): List of auxiliary task modules (optional).\n",
    "        use_rnd (bool): Enable Random Network Distillation intrinsic reward.\n",
    "        rnd_emb_dim (int): Embedding dim for RND networks.\n",
    "        rnd_lr (float): Learning rate for RND predictor.\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    __version__ = \"1.4.0\"\n",
    "\n",
    "    def __init__(\n",
    "        self, \n",
    "        policy_class, \n",
    "        env, \n",
    "        verbose=0,\n",
    "        learning_rate=1e-3, \n",
    "        gamma=0.99, \n",
    "        lam=0.95, \n",
    "        ent_coef=0.01,\n",
    "        device=DEFAULT_DEVICE,\n",
    "        her=False,\n",
    "        reward_norm=False,\n",
    "        intrinsic_expl=True,\n",
    "        intrinsic_eta=0.01,\n",
    "        memory=None,\n",
    "        aux_modules=None,\n",
    "        use_rnd=False, \n",
    "        rnd_emb_dim=32, \n",
    "        rnd_lr=1e-3,\n",
    "        memory_learn_retention=False,      \n",
    "        memory_retention_coef=0.01,\n",
    "        early_stop=True,\n",
    "        early_stop_n_samples=100,\n",
    "        early_stop_mean_threshold=0.95,\n",
    "        early_stop_std_threshold=0.05,\n",
    "    ):\n",
    "        self.env = env\n",
    "        self.device = torch.device(device)\n",
    "        self.gamma = gamma\n",
    "        self.lam = lam\n",
    "        self.ent_coef = ent_coef\n",
    "        self.verbose = verbose\n",
    "        self.memory = memory\n",
    "        self.memory_learn_retention = memory_learn_retention\n",
    "        self.memory_retention_coef = memory_retention_coef\n",
    "        self.aux_modules = aux_modules if aux_modules is not None else []\n",
    "        self.aux = len(self.aux_modules) > 0\n",
    "        self.early_stop= early_stop\n",
    "        self.early_stop_n_samples=early_stop_n_samples\n",
    "        self.early_stop_mean_threshold= early_stop_mean_threshold\n",
    "        self.early_stop_std_threshold= early_stop_std_threshold\n",
    "        # Policy: must accept obs_dim, memory, aux_modules\n",
    "        self.policy = policy_class(\n",
    "            obs_dim=env.observation_space.shape[0], \n",
    "            memory=memory,\n",
    "            aux_modules=self.aux_modules\n",
    "        ).to(self.device)\n",
    "\n",
    "        # PATCH: include modular learning parameters to the optimizer \n",
    "\n",
    "        params = list(self.policy.parameters())\n",
    "        if self.memory_learn_retention and hasattr(self.memory, \"usefulness_parameters\"):\n",
    "            params += list(self.memory.usefulness_parameters())\n",
    "        params = list({id(p): p for p in params}.values())  # REMOVE DUPLICATES\n",
    "        self.optimizer = torch.optim.Adam(params, lr=learning_rate)\n",
    "\n",
    "        self.training_steps = 0\n",
    "        self.episode_rewards = []\n",
    "        self.episode_lengths = []\n",
    "        self.her = her\n",
    "        self.reward_norm = reward_norm\n",
    "        self.intrinsic_expl = intrinsic_expl\n",
    "        self.intrinsic_eta = intrinsic_eta\n",
    "        self.reward_normalizer = RewardNormalizer()\n",
    "        self.state_counter = StateCounter()\n",
    "        self.use_rnd = use_rnd\n",
    "        if self.use_rnd:\n",
    "            self.rnd = RNDModule(env.observation_space.shape[0], emb_dim=rnd_emb_dim).to(self.device)\n",
    "            self.rnd_optimizer = torch.optim.Adam(self.rnd.predictor.parameters(), lr=rnd_lr)\n",
    "        self.trajectory_buffer = []\n",
    "\n",
    "    def reset_trajectory(self):\n",
    "        self.trajectory_buffer = []\n",
    "\n",
    "    def run_episode(self, her_target=None):\n",
    "        obs, _ = self.env.reset()\n",
    "        if her_target is not None:\n",
    "            obs[0] = her_target\n",
    "\n",
    "        done = False\n",
    "        trajectory, actions, rewards, log_probs, values = [], [], [], [], []\n",
    "        entropies_ep, aux_preds_list = [], []\n",
    "        gate_history, memory_size_history = [], []\n",
    "        attn_weights = None\n",
    "        initial_cue = int(obs[0])\n",
    "        aux_targets_ep = {aux.name: [] for aux in self.aux_modules}\n",
    "        context_traj = []  # For memory module\n",
    "\n",
    "        while not done:\n",
    "            obs_t = torch.tensor(obs, dtype=torch.float32, device=self.device)\n",
    "            trajectory.append(obs_t)\n",
    "            traj = torch.stack(trajectory)\n",
    "            action_for_mem = actions[-1].item() if len(actions) > 0 else 0\n",
    "            reward_for_mem = rewards[-1].item() if len(rewards) > 0 else 0.0\n",
    "            context_traj.append((obs_t.cpu().numpy(), action_for_mem, reward_for_mem))\n",
    "\n",
    "            logits, value, aux_preds = self.policy(\n",
    "                traj, obs_t,\n",
    "                actions=torch.tensor([a.item() for a in actions], device=self.device) if actions else None,\n",
    "                rewards=torch.tensor([r.item() for r in rewards], device=self.device) if rewards else None\n",
    "            )\n",
    "            dist = Categorical(logits=logits)\n",
    "            action = dist.sample()\n",
    "            log_prob = dist.log_prob(action)\n",
    "            entropy = dist.entropy()\n",
    "            obs, reward, done, _, _ = self.env.step(action.item())\n",
    "\n",
    "            # Intrinsic reward: count-based and/or RND\n",
    "            if self.intrinsic_expl:\n",
    "                reward += self.intrinsic_eta * self.state_counter.intrinsic_reward(obs)\n",
    "            rnd_intrinsic = 0.0\n",
    "            if self.use_rnd:\n",
    "                with torch.no_grad():\n",
    "                    obs_rnd = obs_t.unsqueeze(0)\n",
    "                    rnd_intrinsic = self.rnd(obs_rnd).item()\n",
    "                    reward += self.intrinsic_eta * rnd_intrinsic\n",
    "\n",
    "            actions.append(action)\n",
    "            log_probs.append(log_prob)\n",
    "            rewards.append(torch.tensor(reward, dtype=torch.float32, device=self.device))\n",
    "            values.append(value)\n",
    "            entropies_ep.append(entropy)\n",
    "            aux_preds_list.append(aux_preds)\n",
    "\n",
    "            # Auxiliary targets (for supervised heads)\n",
    "            for aux in self.aux_modules:\n",
    "                if aux.name == \"cue\":\n",
    "                    aux_targets_ep[aux.name].append(initial_cue)\n",
    "                elif aux.name == \"next_obs\":\n",
    "                    aux_targets_ep[aux.name].append(torch.tensor(obs, dtype=torch.float32))\n",
    "                elif aux.name == \"confidence\":\n",
    "                    dist = Categorical(logits=logits)\n",
    "                    entropy = dist.entropy().item()\n",
    "                    confidence = 1.0 - entropy  # Heuristic; can be improved\n",
    "                    aux_targets_ep[aux.name].append(confidence)\n",
    "                elif aux.name == \"event\":\n",
    "                    event_flag = getattr(self.env, \"event_flag\", 0)\n",
    "                    aux_targets_ep[aux.name].append(event_flag)\n",
    "                elif aux.name == \"oracle_action\":\n",
    "                    oracle_action = getattr(self.env, \"oracle_action\", None)\n",
    "                    aux_targets_ep[aux.name].append(oracle_action)\n",
    "                else:\n",
    "                    aux_targets_ep[aux.name].append(0)\n",
    "\n",
    "        # Store full trajectory in memory module (episodic buffer)\n",
    "        if self.memory is not None:\n",
    "            outcome = sum([r.item() for r in rewards])\n",
    "            # Modular handling: always update episode buffer if available, else fallback\n",
    "            \n",
    "            if hasattr(self.memory, \"episodic_buffer\") and hasattr(self.memory.episodic_buffer, \"add_entry\"):\n",
    "                self.memory.episodic_buffer.add_entry(context_traj, outcome)\n",
    "            elif hasattr(self.memory, \"add_entry\"):\n",
    "                self.memory.add_entry(context_traj, outcome)\n",
    "            # Optionally: update motifs if needed (usually not online, but up to you)\n",
    "            # if hasattr(self.memory, \"motif_bank\") and hasattr(self.memory.motif_bank, \"add_entry\"):\n",
    "            #     self.memory.motif_bank.add_entry(context_traj, outcome)\n",
    "        if self.memory is not None and hasattr(self.memory, 'get_last_attention'):\n",
    "            attn_weights = self.memory.get_last_attention()\n",
    "\n",
    "        # RND predictor update (only predictor trained)\n",
    "        if self.use_rnd:\n",
    "            obs_batch = torch.stack([torch.tensor(np.array(o), dtype=torch.float32, device=self.device) for o in trajectory])\n",
    "            rnd_loss = self.rnd(obs_batch).mean()\n",
    "            self.rnd_optimizer.zero_grad()\n",
    "            rnd_loss.backward()\n",
    "            self.rnd_optimizer.step()\n",
    "\n",
    "        return {\n",
    "            \"trajectory\": trajectory,\n",
    "            \"actions\": actions,\n",
    "            \"rewards\": rewards,\n",
    "            \"log_probs\": log_probs,\n",
    "            \"values\": values,\n",
    "            \"entropies\": entropies_ep,\n",
    "            \"aux_preds\": aux_preds_list,\n",
    "            \"aux_targets\": aux_targets_ep,\n",
    "            \"initial_cue\": initial_cue,\n",
    "            \"gate_history\": gate_history,\n",
    "            \"memory_size_history\": memory_size_history,\n",
    "            \"attn_weights\": attn_weights\n",
    "        }\n",
    "\n",
    "    def get_episodic_buffer(self):\n",
    "        episodic_buffer = None\n",
    "        if self.memory :\n",
    "            episodic_buffer = self.memory.episodic_buffer if hasattr(self.memory,\"episodic_buffer\") else  self.memory\n",
    "        return episodic_buffer\n",
    "\n",
    "        \n",
    "        \n",
    "    def learn(self, total_timesteps=2000, log_interval=100):\n",
    "        steps = 0\n",
    "        episodes = 0\n",
    "        all_returns = []\n",
    "        start_time = time.time()\n",
    "        aux_losses = []\n",
    "        unlock_early_stopping = len(self.episode_rewards)+self.early_stop_n_samples\n",
    "        while steps < total_timesteps:\n",
    "            try:\n",
    "                #if hasattr(sys, 'last_traceback'):  # Quick hack: set by IPython on error/stop\n",
    "                #    print(\"Interrupted in Jupyter (sys.last_traceback). Exiting.\")\n",
    "                #    break\n",
    "                episode = self.run_episode()\n",
    "                if self.reward_norm:\n",
    "                    self.reward_normalizer.update([r.item() for r in episode[\"rewards\"]])\n",
    "                    episode[\"rewards\"] = [\n",
    "                        torch.tensor(rn, dtype=torch.float32, device=self.device)\n",
    "                        for rn in self.reward_normalizer.normalize([r.item() for r in episode[\"rewards\"]])\n",
    "                    ]\n",
    "    \n",
    "                trajectory = episode[\"trajectory\"]\n",
    "                actions = episode[\"actions\"]\n",
    "                rewards = episode[\"rewards\"]\n",
    "                log_probs = episode[\"log_probs\"]\n",
    "                values = episode[\"values\"]\n",
    "                entropies_ep = episode[\"entropies\"]\n",
    "                aux_preds = episode[\"aux_preds\"]\n",
    "                aux_targets = episode[\"aux_targets\"]\n",
    "                T = len(rewards)\n",
    "                rewards_t = torch.stack(rewards)\n",
    "                values_t = torch.stack(values)\n",
    "                log_probs_t = torch.stack(log_probs)\n",
    "                actions_t = torch.stack(actions)\n",
    "                last_value = 0.0\n",
    "                advantages = compute_gae(rewards_t, values_t, gamma=self.gamma, lam=self.lam, last_value=last_value)\n",
    "                returns = advantages + values_t.detach()\n",
    "    \n",
    "                policy_loss = -(log_probs_t * advantages.detach()).sum()\n",
    "                value_loss = F.mse_loss(values_t, returns)\n",
    "                entropy_mean = torch.stack(entropies_ep).mean()\n",
    "                explained_var = compute_explained_variance(values_t, returns)\n",
    "    \n",
    "                # Auxiliary losses\n",
    "                aux_loss_total = torch.tensor(0.0, device=self.device)\n",
    "                aux_metrics_log = {}\n",
    "                if self.aux:\n",
    "                    for aux in self.aux_modules:\n",
    "                        preds = torch.stack([ap[aux.name] for ap in aux_preds])\n",
    "                        targets = torch.tensor(aux_targets[aux.name], device=self.device)\n",
    "                        if preds.dim() != targets.dim():\n",
    "                            targets = targets.squeeze(-1)\n",
    "                        loss = aux.aux_loss(preds, targets)\n",
    "                        aux_loss_total += loss\n",
    "                        metrics = aux.aux_metrics(preds, targets)\n",
    "                        aux_metrics_log[aux.name] = metrics\n",
    "                    aux_losses.append(aux_loss_total.item())\n",
    "    \n",
    "                # Memory usefullness (if enabled) =====\n",
    "                episodic_buffer = self.get_episodic_buffer()\n",
    "                if (\n",
    "                    self.memory_learn_retention\n",
    "                    and self.memory is not None\n",
    "                    and hasattr(episodic_buffer, 'get_last_attention')\n",
    "                    and episodic_buffer.last_attn is not None\n",
    "                    and len(episodic_buffer.usefulness_vec) == len(episodic_buffer.last_attn)\n",
    "                    and len(episodic_buffer.usefulness_vec) > 0\n",
    "                ):\n",
    "                    total_reward = sum([r.item() for r in rewards])\n",
    "                    if hasattr(self.memory,'episodic_buffer'):\n",
    "                        \n",
    "                        attn_tensor = torch.tensor(self.memory.episodic_buffer.last_attn, dtype=torch.float32, device=self.device)\n",
    "                        mem_loss = self.memory.episodic_buffer.usefulness_loss(attn_tensor, total_reward)\n",
    "                    else:\n",
    "                      \n",
    "                        attn_tensor = torch.tensor(self.memory.last_attn, dtype=torch.float32, device=self.device)\n",
    "                        mem_loss = self.memory.usefulness_loss(attn_tensor, total_reward)\n",
    "                else:\n",
    "                    mem_loss = torch.tensor(0.0, device=self.device)\n",
    "                    \n",
    "    \n",
    "                loss = (\n",
    "                    policy_loss \n",
    "                    + 0.5 * value_loss \n",
    "                    + 0.1 * aux_loss_total \n",
    "                    - self.ent_coef * entropy_mean\n",
    "                    + (self.memory_retention_coef * mem_loss if self.memory_learn_retention else 0.0)\n",
    "                )\n",
    "    \n",
    "                self.optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "    \n",
    "                total_reward = sum([r.item() for r in rewards])\n",
    "                self.episode_rewards.append(total_reward)\n",
    "                self.episode_lengths.append(T)\n",
    "                episodes += 1\n",
    "                steps += T\n",
    "\n",
    "                if self.early_stop and len(self.episode_rewards) >= unlock_early_stopping:\n",
    "                    mean_rew = np.mean(self.episode_rewards[-self.early_stop_n_samples:])\n",
    "                    std_rew = np.std(self.episode_rewards[-self.early_stop_n_samples:])\n",
    "                    if mean_rew >self.early_stop_mean_threshold and std_rew <= self.early_stop_std_threshold:\n",
    "                        mean_len = np.mean(self.episode_lengths[-log_interval:])\n",
    "                        elapsed = int(time.time() - start_time)\n",
    "                        ep_duration = elapsed/episodes\n",
    "                        table = [\n",
    "                                [\"Train duration\",f\"{elapsed}s\"],\n",
    "                                [\"Avg episode duration\",f\"{ep_duration:.2f}s\"],\n",
    "                                [\"Rolling ep rew mean\", f\"{mean_rew:.2f}\"],\n",
    "                                [\"Rolling ep rew std\",f\"{std_rew:.2f}\"],\n",
    "                                [\"Rolling ep length\",f\"{mean_len:.2f}\"],\n",
    "                                [\"N updates\", episodes]]\n",
    "                        \n",
    "                        print(tabulate(table ,tablefmt=\"rounded_outline\" , headers=[\"Early Stop\",\"\"]))\n",
    "                        return steps\n",
    "\n",
    "                if hasattr(self.memory, 'motif_bank') and hasattr(self, 'get_episodic_buffer'):\n",
    "                    if episodes > 0 and episodes % MINING_INTERVAL == 0:\n",
    "                        mine_motifs_from_buffer(\n",
    "                            self.get_episodic_buffer(),\n",
    "                            self.memory.motif_bank,\n",
    "                            motif_len=self.memory.motif_bank.motif_len,\n",
    "                            n_motifs=self.memory.motif_bank.n_motifs\n",
    "                        )\n",
    "                                # LOGGING (SB3-STYLE) =====================\n",
    "                if episodes % log_interval == 0 and self.verbose == 1:\n",
    "                    elapsed = int(time.time() - start_time)\n",
    "                    mean_rew = np.mean(self.episode_rewards[-log_interval:])\n",
    "                    std_rew = np.std(self.episode_rewards[-log_interval:])\n",
    "                    mean_len = np.mean(self.episode_lengths[-log_interval:])\n",
    "                \n",
    "                    fps = int(steps / (elapsed + 1e-8))\n",
    "                    adv_mean = advantages.mean().item()\n",
    "                    adv_std = advantages.std().item()\n",
    "                    mean_entropy = entropy_mean.item()\n",
    "                    mean_aux = np.mean(aux_losses[-log_interval:]) if aux_losses else 0.0\n",
    "                    stats = [{\n",
    "                        \"header\": \"rollout\",\n",
    "                        \"stats\": dict(\n",
    "                            ep_len_mean=mean_len,\n",
    "                            ep_rew_mean=mean_rew,\n",
    "                            ep_rew_std=std_rew,\n",
    "                            policy_entropy=mean_entropy,\n",
    "                            advantage_mean=adv_mean,\n",
    "                            advantage_std=adv_std,\n",
    "                            aux_loss_mean=mean_aux\n",
    "                        )}, {\n",
    "                        \"header\": \"time\",\n",
    "                        \"stats\": dict(\n",
    "                            fps=fps,\n",
    "                            episodes=episodes,\n",
    "                            time_elapsed=elapsed,\n",
    "                            total_timesteps=steps\n",
    "                        )}, {\n",
    "                        \"header\": \"train\",\n",
    "                        \"stats\": dict(\n",
    "                            loss=loss.item(),\n",
    "                            policy_loss=policy_loss.item(),\n",
    "                            value_loss=value_loss.item(),\n",
    "                            explained_variance=explained_var.item(),\n",
    "                            n_updates=episodes,\n",
    "                            progress=100 * steps / total_timesteps\n",
    "                        )}\n",
    "                    ]\n",
    "                    if len(aux_metrics_log.items()) > 0:\n",
    "                        aux_stats = {\n",
    "                            \"header\": \"aux_train\",\n",
    "                            \"stats\": {}\n",
    "                        }\n",
    "                        for aux_name, metrics in aux_metrics_log.items():\n",
    "                            for k, v in metrics.items():\n",
    "                                aux_stats[\"stats\"][f\"aux_{aux_name}_{k}\"] = v\n",
    "                        stats.append(aux_stats)\n",
    "                    if self.use_rnd:\n",
    "                        mean_rnd_bonus = np.mean([self.rnd(torch.tensor(np.array(o), dtype=torch.float32, device=self.device).unsqueeze(0)).item() for o in trajectory])\n",
    "                        stats.append({\n",
    "                            \"header\": \"rnd_net_dist\",\n",
    "                            \"stats\": {\"mean_rnd_bonus\": mean_rnd_bonus}\n",
    "                        })\n",
    "                    if self.memory_learn_retention:\n",
    "                        stats.append({\n",
    "                            \"header\": \"memory\",\n",
    "                            \"stats\": {\n",
    "                                \"usefulness_loss\": mem_loss.item()}\n",
    "                        })\n",
    "                    \n",
    "                    print_sb3_style_log_box(stats)\n",
    "                    \n",
    "            except KeyboardInterrupt:\n",
    "                print(\"\\n[Stopped by user] Gracefully exiting training loop...\")\n",
    "                return\n",
    "            \n",
    "        if self.verbose == 1:\n",
    "            print(f\"Training complete. Total episodes: {episodes}, total steps: {steps}\")\n",
    "\n",
    "    def predict(self, obs, deterministic=False, done=False, reward=0.0):\n",
    "        \"\"\"\n",
    "        Computes action for a given observation, with support for memory context.\n",
    "\n",
    "        Args:\n",
    "            obs (np.ndarray): Environment observation.\n",
    "            deterministic (bool): Use argmax instead of sampling.\n",
    "            done (bool): If episode ended, will reset trajectory buffer.\n",
    "            reward (float): Last received reward (for memory context).\n",
    "\n",
    "        Returns:\n",
    "            int: Action index.\n",
    "        \"\"\"\n",
    "        obs_t = torch.tensor(obs, dtype=torch.float32, device=self.device)\n",
    "        # Track full trajectory for memory\n",
    "        if not hasattr(self, \"trajectory_buffer\") or self.trajectory_buffer is None:\n",
    "            self.trajectory_buffer = []\n",
    "        if len(self.trajectory_buffer) == 0:\n",
    "            self.trajectory_buffer.append((obs_t.cpu().numpy(), 0, 0.0))\n",
    "        else:\n",
    "            last_action = self.last_action if hasattr(self, \"last_action\") else 0\n",
    "            last_reward = self.last_reward if hasattr(self, \"last_reward\") else 0.0\n",
    "            self.trajectory_buffer.append((obs_t.cpu().numpy(), last_action, last_reward))\n",
    "        context_traj = self.trajectory_buffer.copy()\n",
    "        actions_int = [a for _, a, _ in context_traj]\n",
    "        rewards_float = [r for _, _, r in context_traj]\n",
    "        obs_stack = torch.stack([torch.tensor(o, dtype=torch.float32, device=self.device) for o, _, _ in context_traj])\n",
    "        logits, _, _ = self.policy(\n",
    "            obs_stack, obs_t,\n",
    "            actions=torch.tensor(actions_int, device=self.device),\n",
    "            rewards=torch.tensor(rewards_float, device=self.device)\n",
    "        )\n",
    "        if deterministic:\n",
    "            action = torch.argmax(logits).item()\n",
    "        else:\n",
    "            dist = Categorical(logits=logits)\n",
    "            action = dist.sample().item()\n",
    "        self.last_action = action\n",
    "        self.last_reward = reward\n",
    "        if done:\n",
    "            self.trajectory_buffer = []\n",
    "        return action\n",
    "\n",
    "    def save(self, path=\"memoryppo.pt\"):\n",
    "        \"\"\"Save policy weights to file.\"\"\"\n",
    "        torch.save(self.policy.state_dict(), path)\n",
    "\n",
    "    def load(self, path=\"memoryppo.pt\"):\n",
    "        \"\"\"Load policy weights from file.\"\"\"\n",
    "        self.policy.load_state_dict(torch.load(path, map_location=self.device))\n",
    "\n",
    "    def evaluate(self, n_episodes=10, deterministic=False, verbose=True):\n",
    "        \"\"\"\n",
    "        Evaluates policy over several episodes, reporting mean/std return.\n",
    "\n",
    "        Args:\n",
    "            n_episodes (int): Number of test episodes.\n",
    "            deterministic (bool): Use argmax instead of sampling.\n",
    "            verbose (bool): Print results to console.\n",
    "\n",
    "        Returns:\n",
    "            mean_return (float): Average reward.\n",
    "            std_return (float): Std deviation of rewards.\n",
    "        \"\"\"\n",
    "        returns = []\n",
    "        for _ in range(n_episodes):\n",
    "            obs, _ = self.env.reset()\n",
    "            self.trajectory_buffer = []\n",
    "            done = False\n",
    "            total_reward = 0.0\n",
    "            last_reward = 0.0\n",
    "            while not done:\n",
    "                action = self.predict(obs, deterministic=deterministic, reward=last_reward)\n",
    "                obs, reward, done, _, _ = self.env.step(action)\n",
    "                total_reward += reward\n",
    "                last_reward = reward\n",
    "                if done:\n",
    "                    self.trajectory_buffer = []\n",
    "            returns.append(total_reward)\n",
    "        mean_return = np.mean(returns)\n",
    "        std_return = np.std(returns)\n",
    "        if verbose:\n",
    "            print(f\"Evaluation over {n_episodes} episodes: mean return {mean_return:.2f}, std {std_return:.2f}\")\n",
    "        return mean_return, std_return\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9af2188b-d2b4-40f3-9de9-220c1bc935f5",
   "metadata": {},
   "source": [
    "## Benchmark Overview\n",
    "This benchmark systematically evaluates this agent on a synthetic memory task with varying levels of difficulty and delay. Each experiment defines a unique scenario (e.g., short vs. long memory delays, easy vs. hard distractors) and trains the agent to solve it using a fixed number of episodes and timesteps.\n",
    "\n",
    "The goal is to compare the performance and generalization as task complexity increases. \n",
    "\n",
    "Optionally, the benchmark can compare this agent againts baseline models (e.g., standard PPO , Recurrent PPO (LstmPPO) under the same conditions. \n",
    "\n",
    "Results can be used to diagnose strengths and limitations of the agent, inform ablations, and guide further development.\n",
    "\n",
    "\n",
    "### Notes:\n",
    "\n",
    "* TraceRL is a Strategic agent so he transfers learning and learns by curriculum learning\n",
    "* Will use curriculum learning in this experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "bba68066-46f6-4701-a935-70b3081b781d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SETUP ===================================\n",
    "DELAY = 4\n",
    "MEM_DIM = 32\n",
    "N_EPISODES = 1_000_00\n",
    "N_MEMORIES = 32\n",
    "\n",
    "CURRICULUM = [2, 4, 8, 16] #,32,64,128,256]\n",
    "\n",
    "AGENT_KWARGS = dict(\n",
    "    device=DEFAULT_DEVICE,\n",
    "    verbose=0,\n",
    "    lam=0.95, \n",
    "    gamma=0.99, \n",
    "    ent_coef=0.01,\n",
    "    learning_rate=1e-3, \n",
    "    \n",
    ")\n",
    "MEMORY_AGENT_KWARGS=dict(\n",
    "    her=False,\n",
    "    reward_norm=False,\n",
    "    aux_modules=None,\n",
    "    intrinsic_expl=False,\n",
    "    use_rnd=False, \n",
    ")\n",
    "\n",
    "# HELPERS =================================\n",
    "def total_timesteps(delay,n_episodes):\n",
    "    return delay * n_episodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0eb00544-9550-4b94-9ce5-3a22776651b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Training with delay of 2\n",
      "╭──────────────────────┬───────╮\n",
      "│ Early Stop           │       │\n",
      "├──────────────────────┼───────┤\n",
      "│ Train duration       │ 12s   │\n",
      "│ Avg episode duration │ 0.03s │\n",
      "│ Rolling ep rew mean  │ 1.00  │\n",
      "│ Rolling ep rew std   │ 0.00  │\n",
      "│ Rolling ep length    │ 2.00  │\n",
      "│ N updates            │ 458   │\n",
      "╰──────────────────────┴───────╯\n",
      "╭──────────────┬────╮\n",
      "│ Evaluation   │    │\n",
      "├──────────────┼────┤\n",
      "│ Avg reward   │  1 │\n",
      "│ Std reward   │  0 │\n",
      "╰──────────────┴────╯\n",
      "\n",
      " Training with delay of 4\n",
      "╭──────────────────────┬───────╮\n",
      "│ Early Stop           │       │\n",
      "├──────────────────────┼───────┤\n",
      "│ Train duration       │ 20s   │\n",
      "│ Avg episode duration │ 0.06s │\n",
      "│ Rolling ep rew mean  │ 1.00  │\n",
      "│ Rolling ep rew std   │ 0.00  │\n",
      "│ Rolling ep length    │ 4.00  │\n",
      "│ N updates            │ 358   │\n",
      "╰──────────────────────┴───────╯\n",
      "╭──────────────┬────╮\n",
      "│ Evaluation   │    │\n",
      "├──────────────┼────┤\n",
      "│ Avg reward   │  1 │\n",
      "│ Std reward   │  0 │\n",
      "╰──────────────┴────╯\n",
      "\n",
      " Training with delay of 8\n",
      "╭──────────────────────┬───────╮\n",
      "│ Early Stop           │       │\n",
      "├──────────────────────┼───────┤\n",
      "│ Train duration       │ 39s   │\n",
      "│ Avg episode duration │ 0.11s │\n",
      "│ Rolling ep rew mean  │ 1.00  │\n",
      "│ Rolling ep rew std   │ 0.00  │\n",
      "│ Rolling ep length    │ 8.00  │\n",
      "│ N updates            │ 359   │\n",
      "╰──────────────────────┴───────╯\n",
      "╭──────────────┬────╮\n",
      "│ Evaluation   │    │\n",
      "├──────────────┼────┤\n",
      "│ Avg reward   │  1 │\n",
      "│ Std reward   │  0 │\n",
      "╰──────────────┴────╯\n",
      "\n",
      " Training with delay of 16\n",
      "╭──────────────────────┬───────╮\n",
      "│ Early Stop           │       │\n",
      "├──────────────────────┼───────┤\n",
      "│ Train duration       │ 22s   │\n",
      "│ Avg episode duration │ 0.22s │\n",
      "│ Rolling ep rew mean  │ 1.00  │\n",
      "│ Rolling ep rew std   │ 0.00  │\n",
      "│ Rolling ep length    │ 16.00 │\n",
      "│ N updates            │ 100   │\n",
      "╰──────────────────────┴───────╯\n",
      "╭──────────────┬────╮\n",
      "│ Evaluation   │    │\n",
      "├──────────────┼────┤\n",
      "│ Avg reward   │  1 │\n",
      "│ Std reward   │  0 │\n",
      "╰──────────────┴────╯\n"
     ]
    }
   ],
   "source": [
    "from benchmark import AgentPerformanceBenchmark\n",
    "from tabulate import tabulate\n",
    "env = MemoryTaskEnv(delay=4, difficulty=0)\n",
    "\n",
    "# MEMORY BUFFER ===========================\n",
    "episodic_buffer = StrategicMemoryBuffer(\n",
    "    obs_dim=env.observation_space.shape[0],\n",
    "    action_dim=1,\n",
    "    mem_dim=MEM_DIM,\n",
    "    max_entries=N_MEMORIES,\n",
    "\n",
    ")\n",
    "motif_bank = MotifMemoryBank(\n",
    "    obs_dim=env.observation_space.shape[0],\n",
    "    action_dim=1,\n",
    "    mem_dim=MEM_DIM,\n",
    "    n_motifs=32,\n",
    "\n",
    ")\n",
    "\n",
    "combined_memory = CombinedMemoryModule(episodic_buffer, motif_bank)\n",
    "\n",
    "    \n",
    "    \n",
    "# POLICY NETWORK (use class) ==============\n",
    "#policy = StrategicMemoryTransformerPolicy\n",
    "policy = StrategicCombinedMemoryPolicy\n",
    "agent = TraceRL(\n",
    "        policy_class=policy,\n",
    "        env=env,\n",
    "        memory=episodic_buffer,\n",
    "        memory_learn_retention=True,    \n",
    "        memory_retention_coef=0.01,   \n",
    "        \n",
    "        verbose=0, \n",
    "        \n",
    "        **MEMORY_AGENT_KWARGS\n",
    "    )\n",
    "\n",
    "agent_total_train_steps = 0\n",
    "for delay in CURRICULUM:\n",
    "    agent.env.delay = delay\n",
    "    #agent.get_episodic_buffer().reset()\n",
    "    print(f\"\\n Training with delay of {delay}\")\n",
    "    \n",
    "    agent_total_train_steps += agent.learn(total_timesteps=total_timesteps(delay, 100000))\n",
    "\n",
    "    benchmark_config = dict(\n",
    "        delay=delay, \n",
    "        n_train_episodes=2000, \n",
    "        total_timesteps=1_000_000, \n",
    "        difficulty=0, \n",
    "        mode_name=\"EASY\", \n",
    "        verbose=0, \n",
    "        eval_base=True\n",
    "    )\n",
    "    \n",
    "    benchmark = AgentPerformanceBenchmark(benchmark_config)\n",
    "    e_r, e_s = benchmark.evaluate(agent,'motif')\n",
    "    \n",
    "    table = [[\"Avg reward\",e_r],[\"Std reward\",e_s]]\n",
    "    \n",
    "    print(tabulate(table, headers=[\"Evaluation\",\"\"], tablefmt=\"rounded_outline\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4796524b-7269-4843-a0d6-0a2a6a63c3fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class SimpleTransformerPolicy(nn.Module):\n",
    "    def __init__(self, obs_dim, action_dim=2, mem_dim=32, nhead=4, n_layers=1):\n",
    "        super().__init__()\n",
    "        self.embed = nn.Linear(obs_dim, mem_dim)\n",
    "        self.pos_embed = nn.Embedding(256, mem_dim)\n",
    "        encoder_layer = nn.TransformerEncoderLayer(d_model=mem_dim, nhead=nhead, batch_first=True)\n",
    "        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=n_layers)\n",
    "        self.policy_head = nn.Linear(mem_dim, action_dim)\n",
    "        self.value_head = nn.Linear(mem_dim, 1)\n",
    "\n",
    "    def forward(self, trajectory):\n",
    "        T = trajectory.shape[0]\n",
    "        x = self.embed(trajectory)\n",
    "        pos = torch.arange(T, device=trajectory.device)\n",
    "        x = x + self.pos_embed(pos)\n",
    "        x = x.unsqueeze(0)\n",
    "        x = self.transformer(x)\n",
    "        feat = x[0, -1]\n",
    "        logits = self.policy_head(feat)\n",
    "        value = self.value_head(feat)\n",
    "        return logits, value.squeeze(-1)\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.distributions import Categorical\n",
    "import numpy as np\n",
    "\n",
    "# -------------------------------\n",
    "# Simple Transformer Policy (no memory module)\n",
    "# -------------------------------\n",
    "class SimpleTransformerPolicy(nn.Module):\n",
    "    def __init__(self, obs_dim, action_dim=2, mem_dim=32, nhead=4, n_layers=1, **kwargs):\n",
    "        super().__init__()\n",
    "        self.mem_dim = mem_dim\n",
    "        self.embed = nn.Linear(obs_dim, mem_dim)\n",
    "        self.pos_embed = nn.Embedding(256, mem_dim)\n",
    "        encoder_layer = nn.TransformerEncoderLayer(d_model=mem_dim, nhead=nhead, batch_first=True)\n",
    "        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=n_layers)\n",
    "        self.policy_head = nn.Linear(mem_dim, action_dim)\n",
    "        self.value_head = nn.Linear(mem_dim, 1)\n",
    "\n",
    "    def forward(self, trajectory):\n",
    "        T = trajectory.shape[0]\n",
    "        x = self.embed(trajectory)\n",
    "        pos = torch.arange(T, device=trajectory.device)\n",
    "        x = x + self.pos_embed(pos)\n",
    "        x = x.unsqueeze(0)\n",
    "        x = self.transformer(x)\n",
    "        feat = x[0, -1]    # Last token\n",
    "        logits = self.policy_head(feat)\n",
    "        value = self.value_head(feat)\n",
    "        return logits, value.squeeze(-1)\n",
    "\n",
    "# -------------------------------\n",
    "# Baseline Transformer Agent\n",
    "# -------------------------------\n",
    "class TransformerAgent:\n",
    "    def __init__(\n",
    "        self,\n",
    "        env,\n",
    "        learning_rate=1e-3,\n",
    "        gamma=0.99,\n",
    "        lam=0.95,\n",
    "        ent_coef=0.01,\n",
    "        device=\"cpu\",\n",
    "        early_stop=True,\n",
    "        early_stop_n_samples=100,\n",
    "        early_stop_mean_threshold=0.95,\n",
    "        early_stop_std_threshold=0.05,\n",
    "        **kwargs\n",
    "    ):\n",
    "        self.env = env\n",
    "        self.device = torch.device(device)\n",
    "        self.gamma = gamma\n",
    "        self.lam = lam\n",
    "        self.ent_coef = ent_coef\n",
    "        self.early_stop = early_stop\n",
    "        self.early_stop_n_samples = early_stop_n_samples\n",
    "        self.early_stop_mean_threshold = early_stop_mean_threshold\n",
    "        self.early_stop_std_threshold = early_stop_std_threshold\n",
    "        obs_dim = env.observation_space.shape[0]\n",
    "        self.policy = SimpleTransformerPolicy(obs_dim=obs_dim, **kwargs).to(self.device)\n",
    "        self.optimizer = torch.optim.Adam(self.policy.parameters(), lr=learning_rate)\n",
    "        self.episode_rewards = []\n",
    "        self.episode_lengths = []\n",
    "\n",
    "    def run_episode(self):\n",
    "        obs, _ = self.env.reset()\n",
    "        done = False\n",
    "        trajectory, actions, rewards, log_probs, values = [], [], [], [], []\n",
    "        while not done:\n",
    "            obs_t = torch.tensor(obs, dtype=torch.float32, device=self.device)\n",
    "            trajectory.append(obs_t)\n",
    "            traj = torch.stack(trajectory)\n",
    "            logits, value = self.policy(traj)\n",
    "            dist = Categorical(logits=logits)\n",
    "            action = dist.sample()\n",
    "            log_prob = dist.log_prob(action)\n",
    "            obs, reward, done, _, _ = self.env.step(action.item())\n",
    "            actions.append(action)\n",
    "            log_probs.append(log_prob)\n",
    "            rewards.append(torch.tensor(reward, dtype=torch.float32, device=self.device))\n",
    "            values.append(value)\n",
    "        return trajectory, actions, rewards, log_probs, values\n",
    "\n",
    "    def learn(self, total_timesteps=2000, log_interval=100):\n",
    "        steps, episodes = 0, 0\n",
    "        start_time = time.time()\n",
    "        while steps < total_timesteps:\n",
    "            trajectory, actions, rewards, log_probs, values = self.run_episode()\n",
    "            T = len(rewards)\n",
    "            rewards_t = torch.stack(rewards)\n",
    "            values_t = torch.stack(values)\n",
    "            log_probs_t = torch.stack(log_probs)\n",
    "            last_value = 0.0\n",
    "            # Standard GAE computation (could import your compute_gae)\n",
    "            advantages = self.compute_gae(rewards_t, values_t, last_value)\n",
    "            returns = advantages + values_t.detach()\n",
    "\n",
    "            policy_loss = -(log_probs_t * advantages.detach()).sum()\n",
    "            value_loss = F.mse_loss(values_t, returns)\n",
    "            entropy = torch.stack([Categorical(logits=self.policy(torch.stack(trajectory[:i+1]))[0]).entropy()\n",
    "                                   for i in range(len(trajectory))]).mean()\n",
    "\n",
    "            loss = (\n",
    "                policy_loss\n",
    "                + 0.5 * value_loss\n",
    "                - self.ent_coef * entropy\n",
    "            )\n",
    "            self.optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "\n",
    "            total_reward = sum([r.item() for r in rewards])\n",
    "            self.episode_rewards.append(total_reward)\n",
    "            self.episode_lengths.append(T)\n",
    "            episodes += 1\n",
    "            steps += T\n",
    "\n",
    "            # Early stopping check\n",
    "            if (\n",
    "                self.early_stop and\n",
    "                len(self.episode_rewards) >= self.early_stop_n_samples\n",
    "            ):\n",
    "                mean_rew = np.mean(self.episode_rewards[-self.early_stop_n_samples:])\n",
    "                std_rew = np.std(self.episode_rewards[-self.early_stop_n_samples:])\n",
    "                if mean_rew > self.early_stop_mean_threshold and std_rew <= self.early_stop_std_threshold:\n",
    "                    elapsed = int(time.time() - start_time)\n",
    "                    print(f\"Early stopping: {episodes} episodes, mean reward {mean_rew:.2f}, std {std_rew:.2f}, time {elapsed}s\")\n",
    "                    return\n",
    "\n",
    "            if episodes % log_interval == 0:\n",
    "                elapsed = int(time.time() - start_time)\n",
    "                mean_rew = np.mean(self.episode_rewards[-log_interval:])\n",
    "                std_rew = np.std(self.episode_rewards[-log_interval:])\n",
    "                mean_len = np.mean(self.episode_lengths[-log_interval:])\n",
    "                print(f\"[{episodes} eps | {steps} steps] rew: {mean_rew:.2f} ± {std_rew:.2f} | len: {mean_len:.1f} | time: {elapsed}s\")\n",
    "\n",
    "    def compute_gae(self, rewards, values, last_value):\n",
    "        gamma, lam = self.gamma, self.lam\n",
    "        advantages = torch.zeros_like(rewards)\n",
    "        gae = 0\n",
    "        for t in reversed(range(len(rewards))):\n",
    "            next_value = values[t+1] if t + 1 < len(values) else torch.tensor(last_value, device=rewards.device)\n",
    "            delta = rewards[t] + gamma * next_value - values[t]\n",
    "            gae = delta + gamma * lam * gae\n",
    "            advantages[t] = gae\n",
    "        return advantages\n",
    "\n",
    "    def evaluate(self, n_episodes=10, deterministic=False, verbose=True):\n",
    "        returns = []\n",
    "        for _ in range(n_episodes):\n",
    "            obs, _ = self.env.reset()\n",
    "            done = False\n",
    "            trajectory = []\n",
    "            total_reward = 0.0\n",
    "            while not done:\n",
    "                obs_t = torch.tensor(obs, dtype=torch.float32, device=self.device)\n",
    "                trajectory.append(obs_t)\n",
    "                traj = torch.stack(trajectory)\n",
    "                logits, _ = self.policy(traj)\n",
    "                action = torch.argmax(logits).item() if deterministic else Categorical(logits=logits).sample().item()\n",
    "                obs, reward, done, _, _ = self.env.step(action)\n",
    "                total_reward += reward\n",
    "            returns.append(total_reward)\n",
    "        mean_return = np.mean(returns)\n",
    "        std_return = np.std(returns)\n",
    "        if verbose:\n",
    "            print(f\"TransformerAgent evaluation: mean return {mean_return:.2f} ± {std_return:.2f}\")\n",
    "        return mean_return, std_return\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05d9a291-4358-42e1-b082-a3f1cc2cae57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[100 eps | 1600 steps] rew: 0.12 ± 0.99 | len: 16.0 | time: 13s\n",
      "[200 eps | 3200 steps] rew: 0.08 ± 1.00 | len: 16.0 | time: 28s\n",
      "[300 eps | 4800 steps] rew: 0.14 ± 0.99 | len: 16.0 | time: 43s\n"
     ]
    }
   ],
   "source": [
    "env = MemoryTaskEnv(delay=delay, difficulty=0)\n",
    "strategic_agent = TraceRL(\n",
    "    policy_class=StrategicCombinedMemoryPolicy,\n",
    "    env=env,\n",
    "    memory=combined_memory,\n",
    "\n",
    "    # ...other kwargs\n",
    ")\n",
    "transformer_agent = TransformerAgent(env)  # No memory\n",
    "\n",
    "# Train\n",
    "#strategic_agent.learn(total_timesteps=5000)\n",
    "transformer_agent.learn(total_timesteps=agent_total_train_steps)\n",
    "\n",
    "# Evaluate\n",
    "#strategic_agent.evaluate(n_episodes=20)\n",
    "#transformer_agent.evaluate(n_episodes=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "551959ff-5ca4-4b08-af71-12f38a8bd544",
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer_agent.evaluate(n_episodes=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2671c90b-8bec-4bbb-8bee-9aeaf07d59a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.evaluate(n_episodes=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8f91c52-ed02-437b-85b7-2b25983fd970",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9434220b-b171-4450-a5d6-28539c9df0d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6820"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent_total_train_steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "700e36f3-2564-4cdb-8b89-d383e53dd9cc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:trading]",
   "language": "python",
   "name": "conda-env-trading-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
