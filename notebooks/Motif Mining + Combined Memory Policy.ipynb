{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "35273d81-4f3a-419d-9a40-7e63658818b4",
   "metadata": {},
   "source": [
    "# Motif Mining / Combined Memory Module\n",
    "\n",
    "### What is a Motif (in time series/finance context):\n",
    "A motif is a short, distinctive, recurring pattern in time series data (e.g., “three rising candles”, “hammer candlestick”).\n",
    "\n",
    "Motif mining is about discovering these patterns that frequently occur, possibly before some important event (like price spikes).\n",
    "\n",
    "Motifs can be fixed length (e.g., always 3 bars) or variable length, but they're usually not tied to any particular \"agent experience\" or reward—they're just patterns that are statistically common or relevant to outcomes.\n",
    "\n",
    "### How does this differ from current agent’s memory?\n",
    "Motifs could become part of memory if they prove useful, but in RL, memory entries are scored and selected by usefulness to the agent’s task, not just frequency.\n",
    "\n",
    "* **Motif:**\n",
    "  * **Purely pattern-based:** “What sequence shapes show up often in the market?”\n",
    "  * **Unsupervised:** Does not depend on what the agent did or the rewards/outcomes.\n",
    "  * Often discovered with algorithms like matrix profile, SAX, or clustering over subsequences.\n",
    "\n",
    "* **Strategic RL Memory:**\n",
    "  * Stores sequences of observations, actions, rewards from actual episodes, tied to what the agent did and what outcome it got.\n",
    "  * Is used for retrieval during decision-making, not just for pattern mining.\n",
    "  * Memory can be trained to only keep those episodes/patterns that are useful for policy improvement, not just frequent.\n",
    "\n",
    "* **Summary:**\n",
    "  * **Motif:**  Statistically recurring pattern in the world\n",
    "  * **Memory_** Agent’s own experienced or retained pattern, which it can choose to use, forget, or score for future use\n",
    "\n",
    "\n",
    "### Goal:\n",
    "\n",
    "* Get both kinds of retrieval in one single process.\n",
    "\n",
    "### Summary:\n",
    "* All memory retrieval (episodic and motif) is neural, attention-based, and trainable.\n",
    "\n",
    "* Motif memory can be used for either unsupervised mining (offline DTW) or end-to-end learned patterns.\n",
    "\n",
    "* Everything is differentiable and ready for RL + auxiliary losses.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3f941562-a714-4a19-9550-9585826a25a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "\n",
    "sys.path.append('../')\n",
    "from environments import MemoryTaskEnv\n",
    "from memory import StrategicMemoryBuffer, BaseMemoryBuffer,StrategicMemoryTransformerPolicy\n",
    "from agent import TraceRL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "247b3482-ff8c-4072-91a2-dde8dd748776",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MotifMemoryBank(BaseMemoryBuffer):\n",
    "    \"\"\"\n",
    "    Motif memory: learnable bank of pattern embeddings, attention-retrieved.\n",
    "\n",
    "    Features:\n",
    "        - Stores K motif embeddings, trainable.\n",
    "        - Neural encoder to encode subtrajectories as motifs.\n",
    "        - Attention over motifs given current context trajectory.\n",
    "    \"\"\"\n",
    "    def __init__(self, obs_dim, action_dim, mem_dim=32, n_motifs=32, motif_len=4, device='cpu'):\n",
    "        super().__init__()\n",
    "        self.obs_dim = obs_dim\n",
    "        self.action_dim = action_dim\n",
    "        self.mem_dim = mem_dim\n",
    "        self.n_motifs = n_motifs\n",
    "        self.motif_len = motif_len\n",
    "        self.device = device\n",
    "        self.last_attn = None\n",
    "        # Learnable motif memory bank\n",
    "        self.motif_embeds = nn.Parameter(torch.randn(n_motifs, mem_dim))\n",
    "        # Neural encoder for extracting motifs from subtrajectories\n",
    "        self.embedding_proj = nn.Linear(obs_dim + action_dim + 1, mem_dim)\n",
    "        self.encoder = nn.TransformerEncoder(\n",
    "            nn.TransformerEncoderLayer(d_model=mem_dim, nhead=2, batch_first=True),\n",
    "            num_layers=1\n",
    "        )\n",
    "\n",
    "    def retrieve(self, context_traj):\n",
    "        \"\"\"\n",
    "        Attends over motif bank using the latest motif_len steps of the context trajectory.\n",
    "        \"\"\"\n",
    "        if len(context_traj) < self.motif_len:\n",
    "            pad = [context_traj[0]] * (self.motif_len - len(context_traj))\n",
    "            motif_traj = pad + context_traj\n",
    "        else:\n",
    "            motif_traj = context_traj[-self.motif_len:]\n",
    "\n",
    "        motif_np = np.array([np.concatenate([obs, [a], [r]]) for obs, a, r in motif_traj], dtype=np.float32)\n",
    "        motif_input = torch.from_numpy(motif_np).unsqueeze(0).to(self.device)\n",
    "        motif_embed = self.encoder(self.embedding_proj(motif_input)).mean(dim=1).squeeze(0)  # [mem_dim]\n",
    "        attn_logits = torch.matmul(self.motif_embeds, motif_embed)\n",
    "        attn = torch.softmax(attn_logits, dim=0)\n",
    "        motif_readout = (attn.unsqueeze(1) * self.motif_embeds).sum(dim=0)\n",
    "        self.last_attn = attn.detach().cpu().numpy()\n",
    "        return motif_readout, attn\n",
    "\n",
    "    def motif_parameters(self):\n",
    "        return [self.motif_embeds]\n",
    "\n",
    "    def get_trainable_parameters(self):\n",
    "        return list(self.parameters()) + list(self.motif_parameters())\n",
    "\n",
    "    def get_last_attention(self):\n",
    "        return self.last_attn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0846a20f-0156-427d-b681-25f117318aa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CombinedMemoryModule(BaseMemoryBuffer):\n",
    "    def __init__(self, episodic_buffer, motif_bank):\n",
    "        super().__init__()\n",
    "        self.episodic_buffer = episodic_buffer\n",
    "        self.motif_bank = motif_bank\n",
    "        self.last_attn = None\n",
    "\n",
    "\n",
    "    def retrieve(self, context_trajectory):\n",
    "        epi_readout, epi_attn = self.episodic_buffer.retrieve(context_trajectory)\n",
    "        motif_readout, motif_attn = self.motif_bank.retrieve(context_trajectory)\n",
    "        combined = torch.cat([epi_readout, motif_readout], dim=-1)\n",
    "        self.last_attn = (epi_attn, motif_attn)\n",
    "        return combined, epi_attn, motif_attn\n",
    "\n",
    "    def add_entry(self, trajectory, outcome):\n",
    "        self.episodic_buffer.add_entry(trajectory, outcome)\n",
    "        # Motif bank may NOT need this, but later might optionally do motif mining here \n",
    "        # For now, only episodic buffer gets new entries\n",
    "        # If you want motifs to be updated with experience, call self.motif_bank.add_entry(trajectory, outcome) if you define it\n",
    "\n",
    "    def get_trainable_parameters(self):\n",
    "        params = []\n",
    "        if hasattr(self, \"episodic_buffer\"):\n",
    "            params += self.episodic_buffer.get_trainable_parameters()\n",
    "        if hasattr(self, \"motif_bank\"):\n",
    "            params += self.motif_bank.get_trainable_parameters()\n",
    "        return params\n",
    "\n",
    "    def get_last_attention(self):\n",
    "        return self.last_attn  # tuple: (episodic, motif)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1cfeb726-0c71-4142-9e12-307e3feb623b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class StrategicCombinedMemoryPolicy(nn.Module):\n",
    "    def __init__(self, obs_dim, mem_dim=32, nhead=4, memory=None, aux_modules=None, **kwargs):\n",
    "        super().__init__()\n",
    "        self.mem_dim = mem_dim\n",
    "        self.embed = nn.Linear(obs_dim, mem_dim)\n",
    "        self.pos_embed = nn.Embedding(256, mem_dim)\n",
    "        encoder_layer = nn.TransformerEncoderLayer(d_model=mem_dim, nhead=nhead, batch_first=True)\n",
    "        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=1)\n",
    "        self.policy_head = nn.Linear(mem_dim + 2 * mem_dim, 2)   # now +2mem_dim (episodic + motif)\n",
    "        self.value_head = nn.Linear(mem_dim + 2 * mem_dim, 1)\n",
    "        self.aux_modules = aux_modules if aux_modules is not None else []\n",
    "        self.memory = memory\n",
    "\n",
    "    def forward(self, trajectory, obs_t=None, actions=None, rewards=None):\n",
    "        T = trajectory.shape[0]\n",
    "        x = self.embed(trajectory)\n",
    "        pos = torch.arange(T, device=trajectory.device)\n",
    "        x = x + self.pos_embed(pos)\n",
    "        x = x.unsqueeze(0)\n",
    "        x = self.transformer(x)\n",
    "        feat = x[0, -1]\n",
    "\n",
    "        mem_feat = torch.zeros(2 * self.mem_dim, device=feat.device)\n",
    "        epi_attn, motif_attn = None, None\n",
    "        if self.memory is not None and actions is not None and rewards is not None:\n",
    "            actions_list = actions.tolist()\n",
    "            rewards_list = rewards.tolist()\n",
    "            if len(actions_list) < T:\n",
    "                actions_list = [0] * (T - len(actions_list)) + actions_list\n",
    "            if len(rewards_list) < T:\n",
    "                rewards_list = [0.0] * (T - len(rewards_list)) + rewards_list\n",
    "            context_traj = [\n",
    "                (trajectory[i].cpu().numpy(), actions_list[i], rewards_list[i]) for i in range(T)\n",
    "            ]\n",
    "            mem_feat, epi_attn, motif_attn = self.memory.retrieve(context_traj)\n",
    "        final_feat = torch.cat([feat, mem_feat], dim=-1)\n",
    "        logits = self.policy_head(final_feat)\n",
    "        value = self.value_head(final_feat)\n",
    "        aux_preds = {}\n",
    "        for aux in self.aux_modules:\n",
    "            aux_preds[aux.name] = aux.head(final_feat)\n",
    "        return logits, value.squeeze(-1), aux_preds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "61f79e99-6906-47db-bc28-f55f181db6eb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# ──────────────────────────────────────────────────────────────\n",
    "# Example training loop\n",
    "# ──────────────────────────────────────────────────────────────\n",
    "\n",
    "# SETUP ===================================\n",
    "DELAY = 4\n",
    "MEM_DIM = 32\n",
    "N_EPISODES = 2500\n",
    "N_MEMORIES = 32\n",
    "\n",
    "AGENT_KWARGS = dict(\n",
    "    device=\"cpu\",\n",
    "    verbose=0,\n",
    "    lam=0.95, \n",
    "    gamma=0.99, \n",
    "    ent_coef=0.01,\n",
    "    learning_rate=1e-3, \n",
    "    \n",
    ")\n",
    "MEMORY_AGENT_KWARGS=dict(\n",
    "    her=False,\n",
    "    reward_norm=False,\n",
    "    aux_modules=None,\n",
    "    \n",
    "    intrinsic_expl=False,\n",
    "    intrinsic_eta=0.01,\n",
    "    \n",
    "    use_rnd=False, \n",
    "    rnd_emb_dim=32, \n",
    "    rnd_lr=1e-3,\n",
    ")\n",
    "\n",
    "# HELPERS =================================\n",
    "def total_timesteps(delay,n_episodes):\n",
    "    return delay * n_episodes\n",
    "\n",
    "# ENVIRONMENT =============================\n",
    "env = MemoryTaskEnv(delay=DELAY, difficulty=0)\n",
    "\n",
    "# MEMORY BUFFER ===========================\n",
    "episodic_buffer = StrategicMemoryBuffer(\n",
    "    obs_dim=env.observation_space.shape[0],\n",
    "    action_dim=1,\n",
    "    mem_dim=MEM_DIM,\n",
    "    max_entries=N_MEMORIES,\n",
    "    device=\"cpu\"\n",
    ")\n",
    "motif_bank = MotifMemoryBank(\n",
    "    obs_dim=env.observation_space.shape[0],\n",
    "    action_dim=1,\n",
    "    mem_dim=MEM_DIM,\n",
    "    n_motifs=32,\n",
    "    motif_len=4,\n",
    "    device=\"cpu\"\n",
    ")\n",
    "combined_memory = CombinedMemoryModule(episodic_buffer, motif_bank)\n",
    "\n",
    "\n",
    "# POLICY NETWORK (use class) ==============\n",
    "policy = StrategicCombinedMemoryPolicy\n",
    "\n",
    "\n",
    "# AGENT SETUP =============================\n",
    "agent = TraceRL(\n",
    "    policy_class=policy,\n",
    "    env=env,\n",
    "    memory=combined_memory,\n",
    "    memory_learn_retention=True,    \n",
    "    memory_retention_coef=0.01,   \n",
    "    # aux_modules=aux_modules,  \n",
    "    device=\"cpu\",\n",
    "    verbose=1,\n",
    "    lam=0.95, \n",
    "    gamma=0.99, \n",
    "    ent_coef=0.01,\n",
    "    learning_rate=1e-3, \n",
    "    \n",
    "    **MEMORY_AGENT_KWARGS\n",
    ")\n",
    "\n",
    "# TRAIN THE AGENT =========================\n",
    "#agent.learn(\n",
    "#    total_timesteps=total_timesteps(DELAY, 1000),\n",
    "#    log_interval=50\n",
    "#)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7f882e22-7cea-482c-b4ba-2f7d3aa1776e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ENVIRONMENT =============================\n",
    "env = MemoryTaskEnv(delay=DELAY, difficulty=0)\n",
    "\n",
    "# MEMORY BUFFER ===========================\n",
    "memory = StrategicMemoryBuffer(\n",
    "    obs_dim=env.observation_space.shape[0],\n",
    "    action_dim=1,          # For Discrete(2)\n",
    "    mem_dim=MEM_DIM,\n",
    "    max_entries=N_MEMORIES,\n",
    "    device=\"cpu\"\n",
    ")\n",
    "\n",
    "# POLICY NETWORK (use class) ==============\n",
    "policy = StrategicMemoryTransformerPolicy\n",
    "\n",
    "# (optional) AUXILIARY MODULES ============\n",
    "\"\"\"\n",
    "aux_modules = [\n",
    "    CueAuxModule(feat_dim=MEM_DIM*2, n_classes=2),\n",
    "    ConfidenceAuxModule(feat_dim=MEM_DIM*2)\n",
    "]\n",
    "\"\"\"\n",
    "\n",
    "# AGENT SETUP =============================\n",
    "agent = TraceRL(\n",
    "    policy_class=policy,\n",
    "    env=env,\n",
    "    memory=memory,\n",
    "    memory_learn_retention=True,    \n",
    "    memory_retention_coef=0.01,   \n",
    "    # aux_modules=aux_modules,  \n",
    "    device=\"cpu\",\n",
    "    verbose=0,\n",
    "    lam=0.95, \n",
    "    gamma=0.99, \n",
    "    ent_coef=0.01,\n",
    "    learning_rate=1e-3, \n",
    "    \n",
    "    **MEMORY_AGENT_KWARGS\n",
    ")\n",
    "\n",
    "# TRAIN THE AGENT =========================\n",
    "#agent.learn(\n",
    "#    total_timesteps=total_timesteps(DELAY, 100),\n",
    "#    log_interval=50\n",
    "#)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0ca74c30-e6e8-4d4d-8496-75fcc36d2789",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "def predict_ep_duration(delay):\n",
    "    delays = np.array([2, 4, 8, 16, 32, 64, 128,258])\n",
    "    avg_durations = np.array([0.02, 0.03, 0.06, 0.12, 0.26, 0.61, 1.52,4.18])\n",
    "    coeff = np.polyfit(delays, avg_durations, 1)\n",
    "    # Use the fitted slope from your data\n",
    "    return coeff[0] * delay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ae40384a-021a-4a2c-b1b8-e8ccfc617ecb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8615542774982028"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_ep_duration(54)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b90e42e9-f342-41ce-b60b-c38c7168a02f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "055292e2-402b-4132-818b-4d9b73105628",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9d023b37-8f6a-4ced-961e-a66f23a1f377",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Training with delay=2 ---\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "not enough values to unpack (expected 3, got 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 38\u001b[0m\n\u001b[0;32m     36\u001b[0m \u001b[38;5;66;03m#agent.get_episodic_buffer().reset()\u001b[39;00m\n\u001b[0;32m     37\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m--- Training with delay=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdelay\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m ---\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 38\u001b[0m \u001b[43magent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlearn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtotal_timesteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtotal_timesteps\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdelay\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m100000\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlog_interval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m50\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     40\u001b[0m benchmark \u001b[38;5;241m=\u001b[39m AgentPerformanceBenchmark(\u001b[38;5;28mdict\u001b[39m(delay\u001b[38;5;241m=\u001b[39mdelay, n_train_episodes\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2000\u001b[39m, total_timesteps\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1_000_000\u001b[39m, difficulty\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m, mode_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEASY\u001b[39m\u001b[38;5;124m\"\u001b[39m, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m, eval_base\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m),)\n\u001b[0;32m     41\u001b[0m e_r, e_s \u001b[38;5;241m=\u001b[39m benchmark\u001b[38;5;241m.\u001b[39mevaluate(agent,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmotif\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32m~\\Dev\\strategic-memory-retrieval-reinforcement-agent\\notebooks\\..\\agent.py:252\u001b[0m, in \u001b[0;36mTraceRL.learn\u001b[1;34m(self, total_timesteps, log_interval)\u001b[0m\n\u001b[0;32m    247\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m steps \u001b[38;5;241m<\u001b[39m total_timesteps:\n\u001b[0;32m    248\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    249\u001b[0m         \u001b[38;5;66;03m#if hasattr(sys, 'last_traceback'):  # Quick hack: set by IPython on error/stop\u001b[39;00m\n\u001b[0;32m    250\u001b[0m         \u001b[38;5;66;03m#    print(\"Interrupted in Jupyter (sys.last_traceback). Exiting.\")\u001b[39;00m\n\u001b[0;32m    251\u001b[0m         \u001b[38;5;66;03m#    break\u001b[39;00m\n\u001b[1;32m--> 252\u001b[0m         episode \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_episode\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    253\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreward_norm:\n\u001b[0;32m    254\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreward_normalizer\u001b[38;5;241m.\u001b[39mupdate([r\u001b[38;5;241m.\u001b[39mitem() \u001b[38;5;28;01mfor\u001b[39;00m r \u001b[38;5;129;01min\u001b[39;00m episode[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrewards\u001b[39m\u001b[38;5;124m\"\u001b[39m]])\n",
      "File \u001b[1;32m~\\Dev\\strategic-memory-retrieval-reinforcement-agent\\notebooks\\..\\agent.py:146\u001b[0m, in \u001b[0;36mTraceRL.run_episode\u001b[1;34m(self, her_target)\u001b[0m\n\u001b[0;32m    143\u001b[0m reward_for_mem \u001b[38;5;241m=\u001b[39m rewards[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mitem() \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(rewards) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;241m0.0\u001b[39m\n\u001b[0;32m    144\u001b[0m context_traj\u001b[38;5;241m.\u001b[39mappend((obs_t\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy(), action_for_mem, reward_for_mem))\n\u001b[1;32m--> 146\u001b[0m logits, value, aux_preds \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpolicy\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    147\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtraj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mobs_t\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    148\u001b[0m \u001b[43m    \u001b[49m\u001b[43mactions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43ma\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43ma\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mactions\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mactions\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    149\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrewards\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mr\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrewards\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrewards\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\n\u001b[0;32m    150\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    151\u001b[0m dist \u001b[38;5;241m=\u001b[39m Categorical(logits\u001b[38;5;241m=\u001b[39mlogits)\n\u001b[0;32m    152\u001b[0m action \u001b[38;5;241m=\u001b[39m dist\u001b[38;5;241m.\u001b[39msample()\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\trading\\lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\trading\\lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[1;32mIn[4], line 35\u001b[0m, in \u001b[0;36mStrategicCombinedMemoryPolicy.forward\u001b[1;34m(self, trajectory, obs_t, actions, rewards)\u001b[0m\n\u001b[0;32m     31\u001b[0m         rewards_list \u001b[38;5;241m=\u001b[39m [\u001b[38;5;241m0.0\u001b[39m] \u001b[38;5;241m*\u001b[39m (T \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mlen\u001b[39m(rewards_list)) \u001b[38;5;241m+\u001b[39m rewards_list\n\u001b[0;32m     32\u001b[0m     context_traj \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m     33\u001b[0m         (trajectory[i]\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy(), actions_list[i], rewards_list[i]) \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(T)\n\u001b[0;32m     34\u001b[0m     ]\n\u001b[1;32m---> 35\u001b[0m     mem_feat, epi_attn, motif_attn \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmemory\u001b[38;5;241m.\u001b[39mretrieve(context_traj)\n\u001b[0;32m     36\u001b[0m final_feat \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat([feat, mem_feat], dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m     37\u001b[0m logits \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpolicy_head(final_feat)\n",
      "\u001b[1;31mValueError\u001b[0m: not enough values to unpack (expected 3, got 2)"
     ]
    }
   ],
   "source": [
    "from benchmark import AgentPerformanceBenchmark\n",
    "\n",
    "env = MemoryTaskEnv(delay=4, difficulty=0)\n",
    "\n",
    "# MEMORY BUFFER ===========================\n",
    "memory = StrategicMemoryBuffer(\n",
    "        obs_dim=env.observation_space.shape[0],\n",
    "        action_dim=1,          # For Discrete(2)\n",
    "        mem_dim=MEM_DIM,\n",
    "        max_entries=N_MEMORIES,\n",
    "        device=\"cpu\"\n",
    "    )\n",
    "    \n",
    "# POLICY NETWORK (use class) ==============\n",
    "#policy = StrategicMemoryTransformerPolicy\n",
    "policy = StrategicCombinedMemoryPolicy\n",
    "agent = TraceRL(\n",
    "        policy_class=policy,\n",
    "        env=env,\n",
    "        memory=memory,\n",
    "        memory_learn_retention=True,    \n",
    "        memory_retention_coef=0.01,   \n",
    "        # aux_modules=aux_modules,  \n",
    "        device=\"cpu\",\n",
    "        verbose=0,\n",
    "        lam=0.95, \n",
    "        gamma=0.99, \n",
    "        ent_coef=0.01,\n",
    "        learning_rate=1e-3, \n",
    "        \n",
    "        **MEMORY_AGENT_KWARGS\n",
    "    )\n",
    "curriculum = [2, 4, 8, 16,32,64,128,256]\n",
    "for delay in curriculum:\n",
    "    agent.env.delay = delay\n",
    "    #agent.get_episodic_buffer().reset()\n",
    "    print(f\"\\n--- Training with delay={delay} ---\")\n",
    "    agent.learn(total_timesteps=total_timesteps(delay, 100000), log_interval=50)\n",
    "    \n",
    "    benchmark = AgentPerformanceBenchmark(dict(delay=delay, n_train_episodes=2000, total_timesteps=1_000_000, difficulty=0, mode_name=\"EASY\", verbose=0, eval_base=True),)\n",
    "    e_r, e_s = benchmark.evaluate(agent,'motif')\n",
    "    table = [[\"Avg reward\",e_r],[\"Std reward\",e_s]]\n",
    "    print(tabulate(table, headers=[\"Evaluation\",\"\"], tablefmt=\"rounded_outline\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ba97dc9-5d8a-4b2e-9583-3aeaa3d829c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from benchmark import AgentPerformanceBenchmark\n",
    "\n",
    "env = MemoryTaskEnv(delay=delay, difficulty=0)\n",
    "\n",
    "# MEMORY BUFFER ===========================\n",
    "memory = StrategicMemoryBuffer(\n",
    "        obs_dim=env.observation_space.shape[0],\n",
    "        action_dim=1,          # For Discrete(2)\n",
    "        mem_dim=MEM_DIM,\n",
    "        max_entries=N_MEMORIES,\n",
    "        device=\"cpu\"\n",
    "    )\n",
    "    \n",
    "# POLICY NETWORK (use class) ==============\n",
    "#policy = StrategicMemoryTransformerPolicy\n",
    "policy = StrategicCombinedMemoryPolicy\n",
    "agent = TraceRL(\n",
    "        policy_class=policy,\n",
    "        env=env,\n",
    "        memory=memory,\n",
    "        memory_learn_retention=True,    \n",
    "        memory_retention_coef=0.01,   \n",
    "        # aux_modules=aux_modules,  \n",
    "        device=\"cpu\",\n",
    "        verbose=0,\n",
    "        lam=0.95, \n",
    "        gamma=0.99, \n",
    "        ent_coef=0.01,\n",
    "        learning_rate=1e-3, \n",
    "        \n",
    "        **MEMORY_AGENT_KWARGS\n",
    "    )\n",
    "curriculum = [2, 4, 8, 16,32,64,128,256]\n",
    "for delay in curriculum:\n",
    "    agent.env.delay = delay\n",
    "    #agent.get_episodic_buffer().reset()\n",
    "    print(f\"\\n--- Training with delay={delay} ---\")\n",
    "    agent.learn(total_timesteps=total_timesteps(delay, 100000), log_interval=50)\n",
    "    \n",
    "    benchmark = AgentPerformanceBenchmark(dict(delay=delay, n_train_episodes=2000, total_timesteps=1_000_000, difficulty=0, mode_name=\"EASY\", verbose=0, eval_base=True),)\n",
    "    e_r, e_s = benchmark.evaluate(agent,'motif')\n",
    "    table = [[\"Avg reward\",e_r],[\"Std reward\",e_s]]\n",
    "    print(tabulate(table, headers=[\"Evaluation\",\"\"], tablefmt=\"rounded_outline\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "129e91d8-da40-4faa-9b22-af9c3355837a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:trading]",
   "language": "python",
   "name": "conda-env-trading-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
