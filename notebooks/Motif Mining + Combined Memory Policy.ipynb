{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "35273d81-4f3a-419d-9a40-7e63658818b4",
   "metadata": {},
   "source": [
    "# Motif Mining / Combined Memory Module\n",
    "\n",
    "### What is a Motif (in time series/finance context):\n",
    "A motif is a short, distinctive, recurring pattern in time series data (e.g., “three rising candles”, “hammer candlestick”).\n",
    "\n",
    "Motif mining is about discovering these patterns that frequently occur, possibly before some important event (like price spikes).\n",
    "\n",
    "Motifs can be fixed length (e.g., always 3 bars) or variable length, but they're usually not tied to any particular \"agent experience\" or reward—they're just patterns that are statistically common or relevant to outcomes.\n",
    "\n",
    "### How does this differ from current agent’s memory?\n",
    "Motifs could become part of memory if they prove useful, but in RL, memory entries are scored and selected by usefulness to the agent’s task, not just frequency.\n",
    "\n",
    "* **Motif:**\n",
    "  * **Purely pattern-based:** “What sequence shapes show up often in the market?”\n",
    "  * **Unsupervised:** Does not depend on what the agent did or the rewards/outcomes.\n",
    "  * Often discovered with algorithms like matrix profile, SAX, or clustering over subsequences.\n",
    "\n",
    "* **Strategic RL Memory:**\n",
    "  * Stores sequences of observations, actions, rewards from actual episodes, tied to what the agent did and what outcome it got.\n",
    "  * Is used for retrieval during decision-making, not just for pattern mining.\n",
    "  * Memory can be trained to only keep those episodes/patterns that are useful for policy improvement, not just frequent.\n",
    "\n",
    "* **Summary:**\n",
    "  * **Motif:**  Statistically recurring pattern in the world\n",
    "  * **Memory_** Agent’s own experienced or retained pattern, which it can choose to use, forget, or score for future use\n",
    "\n",
    "\n",
    "### Goal:\n",
    "\n",
    "* Get both kinds of retrieval in one single process.\n",
    "\n",
    "### Summary:\n",
    "* All memory retrieval (episodic and motif) is neural, attention-based, and trainable.\n",
    "\n",
    "* Motif memory can be used for either unsupervised mining (offline DTW) or end-to-end learned patterns.\n",
    "\n",
    "* Everything is differentiable and ready for RL + auxiliary losses.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3f941562-a714-4a19-9550-9585826a25a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "\n",
    "sys.path.append('../')\n",
    "from environments import MemoryTaskEnv\n",
    "from memory import StrategicMemoryBuffer, BaseMemoryBuffer,StrategicMemoryTransformerPolicy\n",
    "from agent import StrategicMemoryAgent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "247b3482-ff8c-4072-91a2-dde8dd748776",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MotifMemoryBank(BaseMemoryBuffer):\n",
    "    \"\"\"\n",
    "    Motif memory: learnable bank of pattern embeddings, attention-retrieved.\n",
    "\n",
    "    Features:\n",
    "        - Stores K motif embeddings, trainable.\n",
    "        - Neural encoder to encode subtrajectories as motifs.\n",
    "        - Attention over motifs given current context trajectory.\n",
    "    \"\"\"\n",
    "    def __init__(self, obs_dim, action_dim, mem_dim=32, n_motifs=32, motif_len=4, device='cpu'):\n",
    "        super().__init__()\n",
    "        self.obs_dim = obs_dim\n",
    "        self.action_dim = action_dim\n",
    "        self.mem_dim = mem_dim\n",
    "        self.n_motifs = n_motifs\n",
    "        self.motif_len = motif_len\n",
    "        self.device = device\n",
    "        self.last_attn = None\n",
    "        # Learnable motif memory bank\n",
    "        self.motif_embeds = nn.Parameter(torch.randn(n_motifs, mem_dim))\n",
    "        # Neural encoder for extracting motifs from subtrajectories\n",
    "        self.embedding_proj = nn.Linear(obs_dim + action_dim + 1, mem_dim)\n",
    "        self.encoder = nn.TransformerEncoder(\n",
    "            nn.TransformerEncoderLayer(d_model=mem_dim, nhead=2, batch_first=True),\n",
    "            num_layers=1\n",
    "        )\n",
    "\n",
    "    def retrieve(self, context_traj):\n",
    "        \"\"\"\n",
    "        Attends over motif bank using the latest motif_len steps of the context trajectory.\n",
    "        \"\"\"\n",
    "        if len(context_traj) < self.motif_len:\n",
    "            pad = [context_traj[0]] * (self.motif_len - len(context_traj))\n",
    "            motif_traj = pad + context_traj\n",
    "        else:\n",
    "            motif_traj = context_traj[-self.motif_len:]\n",
    "\n",
    "        motif_np = np.array([np.concatenate([obs, [a], [r]]) for obs, a, r in motif_traj], dtype=np.float32)\n",
    "        motif_input = torch.from_numpy(motif_np).unsqueeze(0).to(self.device)\n",
    "        motif_embed = self.encoder(self.embedding_proj(motif_input)).mean(dim=1).squeeze(0)  # [mem_dim]\n",
    "        attn_logits = torch.matmul(self.motif_embeds, motif_embed)\n",
    "        attn = torch.softmax(attn_logits, dim=0)\n",
    "        motif_readout = (attn.unsqueeze(1) * self.motif_embeds).sum(dim=0)\n",
    "        self.last_attn = attn.detach().cpu().numpy()\n",
    "        return motif_readout, attn\n",
    "\n",
    "    def motif_parameters(self):\n",
    "        return [self.motif_embeds]\n",
    "\n",
    "    def get_trainable_parameters(self):\n",
    "        return list(self.parameters()) + list(self.motif_parameters())\n",
    "\n",
    "    def get_last_attention(self):\n",
    "        return self.last_attn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0846a20f-0156-427d-b681-25f117318aa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CombinedMemoryModule(BaseMemoryBuffer):\n",
    "    def __init__(self, episodic_buffer, motif_bank):\n",
    "        super().__init__()\n",
    "        self.episodic_buffer = episodic_buffer\n",
    "        self.motif_bank = motif_bank\n",
    "        self.last_attn = None\n",
    "\n",
    "\n",
    "    def retrieve(self, context_trajectory):\n",
    "        epi_readout, epi_attn = self.episodic_buffer.retrieve(context_trajectory)\n",
    "        motif_readout, motif_attn = self.motif_bank.retrieve(context_trajectory)\n",
    "        combined = torch.cat([epi_readout, motif_readout], dim=-1)\n",
    "        self.last_attn = (epi_attn, motif_attn)\n",
    "        return combined, epi_attn, motif_attn\n",
    "\n",
    "    def add_entry(self, trajectory, outcome):\n",
    "        self.episodic_buffer.add_entry(trajectory, outcome)\n",
    "        # Motif bank may NOT need this, but later might optionally do motif mining here \n",
    "        # For now, only episodic buffer gets new entries\n",
    "        # If you want motifs to be updated with experience, call self.motif_bank.add_entry(trajectory, outcome) if you define it\n",
    "\n",
    "    def get_trainable_parameters(self):\n",
    "        params = []\n",
    "        if hasattr(self, \"episodic_buffer\"):\n",
    "            params += self.episodic_buffer.get_trainable_parameters()\n",
    "        if hasattr(self, \"motif_bank\"):\n",
    "            params += self.motif_bank.get_trainable_parameters()\n",
    "        return params\n",
    "\n",
    "    def get_last_attention(self):\n",
    "        return self.last_attn  # tuple: (episodic, motif)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1cfeb726-0c71-4142-9e12-307e3feb623b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class StrategicCombinedMemoryPolicy(nn.Module):\n",
    "    def __init__(self, obs_dim, mem_dim=32, nhead=4, memory=None, aux_modules=None, **kwargs):\n",
    "        super().__init__()\n",
    "        self.mem_dim = mem_dim\n",
    "        self.embed = nn.Linear(obs_dim, mem_dim)\n",
    "        self.pos_embed = nn.Embedding(256, mem_dim)\n",
    "        encoder_layer = nn.TransformerEncoderLayer(d_model=mem_dim, nhead=nhead, batch_first=True)\n",
    "        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=1)\n",
    "        self.policy_head = nn.Linear(mem_dim + 2 * mem_dim, 2)   # now +2mem_dim (episodic + motif)\n",
    "        self.value_head = nn.Linear(mem_dim + 2 * mem_dim, 1)\n",
    "        self.aux_modules = aux_modules if aux_modules is not None else []\n",
    "        self.memory = memory\n",
    "\n",
    "    def forward(self, trajectory, obs_t=None, actions=None, rewards=None):\n",
    "        T = trajectory.shape[0]\n",
    "        x = self.embed(trajectory)\n",
    "        pos = torch.arange(T, device=trajectory.device)\n",
    "        x = x + self.pos_embed(pos)\n",
    "        x = x.unsqueeze(0)\n",
    "        x = self.transformer(x)\n",
    "        feat = x[0, -1]\n",
    "\n",
    "        mem_feat = torch.zeros(2 * self.mem_dim, device=feat.device)\n",
    "        epi_attn, motif_attn = None, None\n",
    "        if self.memory is not None and actions is not None and rewards is not None:\n",
    "            actions_list = actions.tolist()\n",
    "            rewards_list = rewards.tolist()\n",
    "            if len(actions_list) < T:\n",
    "                actions_list = [0] * (T - len(actions_list)) + actions_list\n",
    "            if len(rewards_list) < T:\n",
    "                rewards_list = [0.0] * (T - len(rewards_list)) + rewards_list\n",
    "            context_traj = [\n",
    "                (trajectory[i].cpu().numpy(), actions_list[i], rewards_list[i]) for i in range(T)\n",
    "            ]\n",
    "            mem_feat, epi_attn, motif_attn = self.memory.retrieve(context_traj)\n",
    "        final_feat = torch.cat([feat, mem_feat], dim=-1)\n",
    "        logits = self.policy_head(final_feat)\n",
    "        value = self.value_head(final_feat)\n",
    "        aux_preds = {}\n",
    "        for aux in self.aux_modules:\n",
    "            aux_preds[aux.name] = aux.head(final_feat)\n",
    "        return logits, value.squeeze(-1), aux_preds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "29212f88-5774-4e9f-bae6-f7364a2f342d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import time\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.distributions import Categorical\n",
    "\n",
    "\n",
    "from core_modules import RewardNormalizer, StateCounter, RNDModule\n",
    "from core_calculations import compute_gae, compute_explained_variance\n",
    "from callbacks import print_sb3_style_log_box\n",
    "\n",
    "\n",
    "class StrategicMemoryAgent:\n",
    "    \"\"\"\n",
    "    Proximal Policy Optimization (PPO) agent with integrated external memory retrieval.\n",
    "\n",
    "    Features:\n",
    "        - Supports auxiliary losses, HER, reward normalization, and RND-based exploration.\n",
    "        - Episodic or contextual memory (passed as `memory`) for strategic RL.\n",
    "        - Plug-and-play auxiliary modules (e.g., cue, event, confidence).\n",
    "        - Stable training with reward normalization and intrinsic/extrinsic reward mixing.\n",
    "\n",
    "    Args:\n",
    "        policy_class (nn.Module): Policy network class (should accept obs_dim, memory, aux_modules).\n",
    "        env (gym.Env): Gymnasium environment.\n",
    "        verbose (int): Logging verbosity (0 = silent, 1 = logs).\n",
    "        learning_rate (float): Adam optimizer learning rate.\n",
    "        gamma (float): Discount factor.\n",
    "        lam (float): GAE lambda.\n",
    "        device (str): Torch device.\n",
    "        her (bool): Enable Hindsight Experience Replay (if supported by env).\n",
    "        reward_norm (bool): Normalize reward with running stats.\n",
    "        intrinsic_expl (bool): Use count-based intrinsic reward.\n",
    "        intrinsic_eta (float): Scaling for intrinsic bonus.\n",
    "        ent_coef (float): Entropy coefficient.\n",
    "        memory: Memory module for contextual/episodic learning (optional).\n",
    "        aux_modules (list): List of auxiliary task modules (optional).\n",
    "        use_rnd (bool): Enable Random Network Distillation intrinsic reward.\n",
    "        rnd_emb_dim (int): Embedding dim for RND networks.\n",
    "        rnd_lr (float): Learning rate for RND predictor.\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    __version__ = \"1.4.0\"\n",
    "\n",
    "    def __init__(\n",
    "        self, \n",
    "        policy_class, \n",
    "        env, \n",
    "        verbose=0,\n",
    "        learning_rate=1e-3, \n",
    "        gamma=0.99, \n",
    "        lam=0.95, \n",
    "        ent_coef=0.01,\n",
    "        device=\"cpu\",\n",
    "        her=False,\n",
    "        reward_norm=False,\n",
    "        intrinsic_expl=True,\n",
    "        intrinsic_eta=0.01,\n",
    "        memory=None,\n",
    "        aux_modules=None,\n",
    "        use_rnd=False, \n",
    "        rnd_emb_dim=32, \n",
    "        rnd_lr=1e-3,\n",
    "        memory_learn_retention=False,      \n",
    "        memory_retention_coef=0.01        \n",
    "    ):\n",
    "        self.env = env\n",
    "        self.device = torch.device(device)\n",
    "        self.gamma = gamma\n",
    "        self.lam = lam\n",
    "        self.ent_coef = ent_coef\n",
    "        self.verbose = verbose\n",
    "        self.memory = memory\n",
    "        self.memory_learn_retention = memory_learn_retention\n",
    "        self.memory_retention_coef = memory_retention_coef\n",
    "        self.aux_modules = aux_modules if aux_modules is not None else []\n",
    "        self.aux = len(self.aux_modules) > 0\n",
    "        # Policy: must accept obs_dim, memory, aux_modules\n",
    "        self.policy = policy_class(\n",
    "            obs_dim=env.observation_space.shape[0], \n",
    "            memory=memory,\n",
    "            aux_modules=self.aux_modules\n",
    "        ).to(self.device)\n",
    "\n",
    "        # PATCH: include modular learning parameters to the optimizer \n",
    "\n",
    "        params = list(self.policy.parameters())\n",
    "        if self.memory_learn_retention and hasattr(self.memory, \"usefulness_parameters\"):\n",
    "            params += list(self.memory.usefulness_parameters())\n",
    "        params = list({id(p): p for p in params}.values())  # REMOVE DUPLICATES\n",
    "        self.optimizer = torch.optim.Adam(params, lr=learning_rate)\n",
    "\n",
    "        self.training_steps = 0\n",
    "        self.episode_rewards = []\n",
    "        self.episode_lengths = []\n",
    "        self.her = her\n",
    "        self.reward_norm = reward_norm\n",
    "        self.intrinsic_expl = intrinsic_expl\n",
    "        self.intrinsic_eta = intrinsic_eta\n",
    "        self.reward_normalizer = RewardNormalizer()\n",
    "        self.state_counter = StateCounter()\n",
    "        self.use_rnd = use_rnd\n",
    "        if self.use_rnd:\n",
    "            self.rnd = RNDModule(env.observation_space.shape[0], emb_dim=rnd_emb_dim).to(self.device)\n",
    "            self.rnd_optimizer = torch.optim.Adam(self.rnd.predictor.parameters(), lr=rnd_lr)\n",
    "        self.trajectory_buffer = []\n",
    "\n",
    "    def reset_trajectory(self):\n",
    "        self.trajectory_buffer = []\n",
    "\n",
    "    def run_episode(self, her_target=None):\n",
    "        obs, _ = self.env.reset()\n",
    "        if her_target is not None:\n",
    "            obs[0] = her_target\n",
    "\n",
    "        done = False\n",
    "        trajectory, actions, rewards, log_probs, values = [], [], [], [], []\n",
    "        entropies_ep, aux_preds_list = [], []\n",
    "        gate_history, memory_size_history = [], []\n",
    "        attn_weights = None\n",
    "        initial_cue = int(obs[0])\n",
    "        aux_targets_ep = {aux.name: [] for aux in self.aux_modules}\n",
    "        context_traj = []  # For memory module\n",
    "\n",
    "        while not done:\n",
    "            obs_t = torch.tensor(obs, dtype=torch.float32, device=self.device)\n",
    "            trajectory.append(obs_t)\n",
    "            traj = torch.stack(trajectory)\n",
    "            action_for_mem = actions[-1].item() if len(actions) > 0 else 0\n",
    "            reward_for_mem = rewards[-1].item() if len(rewards) > 0 else 0.0\n",
    "            context_traj.append((obs_t.cpu().numpy(), action_for_mem, reward_for_mem))\n",
    "\n",
    "            logits, value, aux_preds = self.policy(\n",
    "                traj, obs_t,\n",
    "                actions=torch.tensor([a.item() for a in actions], device=self.device) if actions else None,\n",
    "                rewards=torch.tensor([r.item() for r in rewards], device=self.device) if rewards else None\n",
    "            )\n",
    "            dist = Categorical(logits=logits)\n",
    "            action = dist.sample()\n",
    "            log_prob = dist.log_prob(action)\n",
    "            entropy = dist.entropy()\n",
    "            obs, reward, done, _, _ = self.env.step(action.item())\n",
    "\n",
    "            # Intrinsic reward: count-based and/or RND\n",
    "            if self.intrinsic_expl:\n",
    "                reward += self.intrinsic_eta * self.state_counter.intrinsic_reward(obs)\n",
    "            rnd_intrinsic = 0.0\n",
    "            if self.use_rnd:\n",
    "                with torch.no_grad():\n",
    "                    obs_rnd = obs_t.unsqueeze(0)\n",
    "                    rnd_intrinsic = self.rnd(obs_rnd).item()\n",
    "                    reward += self.intrinsic_eta * rnd_intrinsic\n",
    "\n",
    "            actions.append(action)\n",
    "            log_probs.append(log_prob)\n",
    "            rewards.append(torch.tensor(reward, dtype=torch.float32, device=self.device))\n",
    "            values.append(value)\n",
    "            entropies_ep.append(entropy)\n",
    "            aux_preds_list.append(aux_preds)\n",
    "\n",
    "            # Auxiliary targets (for supervised heads)\n",
    "            for aux in self.aux_modules:\n",
    "                if aux.name == \"cue\":\n",
    "                    aux_targets_ep[aux.name].append(initial_cue)\n",
    "                elif aux.name == \"next_obs\":\n",
    "                    aux_targets_ep[aux.name].append(torch.tensor(obs, dtype=torch.float32))\n",
    "                elif aux.name == \"confidence\":\n",
    "                    dist = Categorical(logits=logits)\n",
    "                    entropy = dist.entropy().item()\n",
    "                    confidence = 1.0 - entropy  # Heuristic; can be improved\n",
    "                    aux_targets_ep[aux.name].append(confidence)\n",
    "                elif aux.name == \"event\":\n",
    "                    event_flag = getattr(self.env, \"event_flag\", 0)\n",
    "                    aux_targets_ep[aux.name].append(event_flag)\n",
    "                elif aux.name == \"oracle_action\":\n",
    "                    oracle_action = getattr(self.env, \"oracle_action\", None)\n",
    "                    aux_targets_ep[aux.name].append(oracle_action)\n",
    "                else:\n",
    "                    aux_targets_ep[aux.name].append(0)\n",
    "\n",
    "        # Store full trajectory in memory module (episodic buffer)\n",
    "        if self.memory is not None:\n",
    "            outcome = sum([r.item() for r in rewards])\n",
    "            # Modular handling: always update episode buffer if available, else fallback\n",
    "            \n",
    "            if hasattr(self.memory, \"episodic_buffer\") and hasattr(self.memory.episodic_buffer, \"add_entry\"):\n",
    "                self.memory.episodic_buffer.add_entry(context_traj, outcome)\n",
    "            elif hasattr(self.memory, \"add_entry\"):\n",
    "                self.memory.add_entry(context_traj, outcome)\n",
    "            # Optionally: update motifs if needed (usually not online, but up to you)\n",
    "            # if hasattr(self.memory, \"motif_bank\") and hasattr(self.memory.motif_bank, \"add_entry\"):\n",
    "            #     self.memory.motif_bank.add_entry(context_traj, outcome)\n",
    "        if self.memory is not None and hasattr(self.memory, 'get_last_attention'):\n",
    "            attn_weights = self.memory.get_last_attention()\n",
    "\n",
    "        # RND predictor update (only predictor trained)\n",
    "        if self.use_rnd:\n",
    "            obs_batch = torch.stack([torch.tensor(np.array(o), dtype=torch.float32, device=self.device) for o in trajectory])\n",
    "            rnd_loss = self.rnd(obs_batch).mean()\n",
    "            self.rnd_optimizer.zero_grad()\n",
    "            rnd_loss.backward()\n",
    "            self.rnd_optimizer.step()\n",
    "\n",
    "        return {\n",
    "            \"trajectory\": trajectory,\n",
    "            \"actions\": actions,\n",
    "            \"rewards\": rewards,\n",
    "            \"log_probs\": log_probs,\n",
    "            \"values\": values,\n",
    "            \"entropies\": entropies_ep,\n",
    "            \"aux_preds\": aux_preds_list,\n",
    "            \"aux_targets\": aux_targets_ep,\n",
    "            \"initial_cue\": initial_cue,\n",
    "            \"gate_history\": gate_history,\n",
    "            \"memory_size_history\": memory_size_history,\n",
    "            \"attn_weights\": attn_weights\n",
    "        }\n",
    "\n",
    "    def get_episodic_buffer(self):\n",
    "        episodic_buffer = None\n",
    "        if self.memory :\n",
    "            episodic_buffer = self.memory.episodic_buffer if hasattr(self.memory,\"episodic_buffer\") else  self.memory\n",
    "        return episodic_buffer\n",
    "        \n",
    "    def learn(self, total_timesteps=2000, log_interval=100):\n",
    "        steps = 0\n",
    "        episodes = 0\n",
    "        all_returns = []\n",
    "        start_time = time.time()\n",
    "        aux_losses = []\n",
    "        while steps < total_timesteps:\n",
    "            try:\n",
    "                #if hasattr(sys, 'last_traceback'):  # Quick hack: set by IPython on error/stop\n",
    "                #    print(\"Interrupted in Jupyter (sys.last_traceback). Exiting.\")\n",
    "                #    break\n",
    "                episode = self.run_episode()\n",
    "                if self.reward_norm:\n",
    "                    self.reward_normalizer.update([r.item() for r in episode[\"rewards\"]])\n",
    "                    episode[\"rewards\"] = [\n",
    "                        torch.tensor(rn, dtype=torch.float32, device=self.device)\n",
    "                        for rn in self.reward_normalizer.normalize([r.item() for r in episode[\"rewards\"]])\n",
    "                    ]\n",
    "    \n",
    "                trajectory = episode[\"trajectory\"]\n",
    "                actions = episode[\"actions\"]\n",
    "                rewards = episode[\"rewards\"]\n",
    "                log_probs = episode[\"log_probs\"]\n",
    "                values = episode[\"values\"]\n",
    "                entropies_ep = episode[\"entropies\"]\n",
    "                aux_preds = episode[\"aux_preds\"]\n",
    "                aux_targets = episode[\"aux_targets\"]\n",
    "                T = len(rewards)\n",
    "                rewards_t = torch.stack(rewards)\n",
    "                values_t = torch.stack(values)\n",
    "                log_probs_t = torch.stack(log_probs)\n",
    "                actions_t = torch.stack(actions)\n",
    "                last_value = 0.0\n",
    "                advantages = compute_gae(rewards_t, values_t, gamma=self.gamma, lam=self.lam, last_value=last_value)\n",
    "                returns = advantages + values_t.detach()\n",
    "    \n",
    "                policy_loss = -(log_probs_t * advantages.detach()).sum()\n",
    "                value_loss = F.mse_loss(values_t, returns)\n",
    "                entropy_mean = torch.stack(entropies_ep).mean()\n",
    "                explained_var = compute_explained_variance(values_t, returns)\n",
    "    \n",
    "                # Auxiliary losses\n",
    "                aux_loss_total = torch.tensor(0.0, device=self.device)\n",
    "                aux_metrics_log = {}\n",
    "                if self.aux:\n",
    "                    for aux in self.aux_modules:\n",
    "                        preds = torch.stack([ap[aux.name] for ap in aux_preds])\n",
    "                        targets = torch.tensor(aux_targets[aux.name], device=self.device)\n",
    "                        if preds.dim() != targets.dim():\n",
    "                            targets = targets.squeeze(-1)\n",
    "                        loss = aux.aux_loss(preds, targets)\n",
    "                        aux_loss_total += loss\n",
    "                        metrics = aux.aux_metrics(preds, targets)\n",
    "                        aux_metrics_log[aux.name] = metrics\n",
    "                    aux_losses.append(aux_loss_total.item())\n",
    "    \n",
    "                # Memory usefullness (if enabled) =====\n",
    "                episodic_buffer = self.get_episodic_buffer()\n",
    "                if (\n",
    "                    self.memory_learn_retention\n",
    "                    and self.memory is not None\n",
    "                    and hasattr(episodic_buffer, 'get_last_attention')\n",
    "                    and episodic_buffer.last_attn is not None\n",
    "                    and len(episodic_buffer.usefulness_vec) == len(episodic_buffer.last_attn)\n",
    "                    and len(episodic_buffer.usefulness_vec) > 0\n",
    "                ):\n",
    "                    total_reward = sum([r.item() for r in rewards])\n",
    "                    if hasattr(self.memory,'episodic_buffer'):\n",
    "                        \n",
    "                        attn_tensor = torch.tensor(self.memory.episodic_buffer.last_attn, dtype=torch.float32, device=self.device)\n",
    "                        mem_loss = self.memory.episodic_buffer.usefulness_loss(attn_tensor, total_reward)\n",
    "                    else:\n",
    "                      \n",
    "                        attn_tensor = torch.tensor(self.memory.last_attn, dtype=torch.float32, device=self.device)\n",
    "                        mem_loss = self.memory.usefulness_loss(attn_tensor, total_reward)\n",
    "                else:\n",
    "                    mem_loss = torch.tensor(0.0, device=self.device)\n",
    "                    \n",
    "    \n",
    "                loss = (\n",
    "                    policy_loss \n",
    "                    + 0.5 * value_loss \n",
    "                    + 0.1 * aux_loss_total \n",
    "                    - self.ent_coef * entropy_mean\n",
    "                    + (self.memory_retention_coef * mem_loss if self.memory_learn_retention else 0.0)\n",
    "                )\n",
    "    \n",
    "                self.optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "    \n",
    "                total_reward = sum([r.item() for r in rewards])\n",
    "                self.episode_rewards.append(total_reward)\n",
    "                self.episode_lengths.append(T)\n",
    "                episodes += 1\n",
    "                steps += T\n",
    "    \n",
    "                # LOGGING (SB3-STYLE) =====================\n",
    "                if episodes % log_interval == 0 and self.verbose == 1:\n",
    "                    elapsed = int(time.time() - start_time)\n",
    "                    mean_rew = np.mean(self.episode_rewards[-log_interval:])\n",
    "                    std_rew = np.std(self.episode_rewards[-log_interval:])\n",
    "                    mean_len = np.mean(self.episode_lengths[-log_interval:])\n",
    "                \n",
    "                    fps = int(steps / (elapsed + 1e-8))\n",
    "                    adv_mean = advantages.mean().item()\n",
    "                    adv_std = advantages.std().item()\n",
    "                    mean_entropy = entropy_mean.item()\n",
    "                    mean_aux = np.mean(aux_losses[-log_interval:]) if aux_losses else 0.0\n",
    "                    stats = [{\n",
    "                        \"header\": \"rollout\",\n",
    "                        \"stats\": dict(\n",
    "                            ep_len_mean=mean_len,\n",
    "                            ep_rew_mean=mean_rew,\n",
    "                            ep_rew_std=std_rew,\n",
    "                            policy_entropy=mean_entropy,\n",
    "                            advantage_mean=adv_mean,\n",
    "                            advantage_std=adv_std,\n",
    "                            aux_loss_mean=mean_aux\n",
    "                        )}, {\n",
    "                        \"header\": \"time\",\n",
    "                        \"stats\": dict(\n",
    "                            fps=fps,\n",
    "                            episodes=episodes,\n",
    "                            time_elapsed=elapsed,\n",
    "                            total_timesteps=steps\n",
    "                        )}, {\n",
    "                        \"header\": \"train\",\n",
    "                        \"stats\": dict(\n",
    "                            loss=loss.item(),\n",
    "                            policy_loss=policy_loss.item(),\n",
    "                            value_loss=value_loss.item(),\n",
    "                            explained_variance=explained_var.item(),\n",
    "                            n_updates=episodes,\n",
    "                            progress=100 * steps / total_timesteps\n",
    "                        )}\n",
    "                    ]\n",
    "                    if len(aux_metrics_log.items()) > 0:\n",
    "                        aux_stats = {\n",
    "                            \"header\": \"aux_train\",\n",
    "                            \"stats\": {}\n",
    "                        }\n",
    "                        for aux_name, metrics in aux_metrics_log.items():\n",
    "                            for k, v in metrics.items():\n",
    "                                aux_stats[\"stats\"][f\"aux_{aux_name}_{k}\"] = v\n",
    "                        stats.append(aux_stats)\n",
    "                    if self.use_rnd:\n",
    "                        mean_rnd_bonus = np.mean([self.rnd(torch.tensor(np.array(o), dtype=torch.float32, device=self.device).unsqueeze(0)).item() for o in trajectory])\n",
    "                        stats.append({\n",
    "                            \"header\": \"rnd_net_dist\",\n",
    "                            \"stats\": {\"mean_rnd_bonus\": mean_rnd_bonus}\n",
    "                        })\n",
    "                    if self.memory_learn_retention:\n",
    "                        stats.append({\n",
    "                            \"header\": \"memory\",\n",
    "                            \"stats\": {\n",
    "                                \"usefulness_loss\": mem_loss.item()}\n",
    "                        })\n",
    "                    \n",
    "                    print_sb3_style_log_box(stats)\n",
    "                    \n",
    "            except KeyboardInterrupt:\n",
    "                print(\"\\n[Stopped by user] Gracefully exiting training loop...\")\n",
    "                return\n",
    "            \n",
    "        if self.verbose == 1:\n",
    "            print(f\"Training complete. Total episodes: {episodes}, total steps: {steps}\")\n",
    "\n",
    "    def predict(self, obs, deterministic=False, done=False, reward=0.0):\n",
    "        \"\"\"\n",
    "        Computes action for a given observation, with support for memory context.\n",
    "\n",
    "        Args:\n",
    "            obs (np.ndarray): Environment observation.\n",
    "            deterministic (bool): Use argmax instead of sampling.\n",
    "            done (bool): If episode ended, will reset trajectory buffer.\n",
    "            reward (float): Last received reward (for memory context).\n",
    "\n",
    "        Returns:\n",
    "            int: Action index.\n",
    "        \"\"\"\n",
    "        obs_t = torch.tensor(obs, dtype=torch.float32, device=self.device)\n",
    "        # Track full trajectory for memory\n",
    "        if not hasattr(self, \"trajectory_buffer\") or self.trajectory_buffer is None:\n",
    "            self.trajectory_buffer = []\n",
    "        if len(self.trajectory_buffer) == 0:\n",
    "            self.trajectory_buffer.append((obs_t.cpu().numpy(), 0, 0.0))\n",
    "        else:\n",
    "            last_action = self.last_action if hasattr(self, \"last_action\") else 0\n",
    "            last_reward = self.last_reward if hasattr(self, \"last_reward\") else 0.0\n",
    "            self.trajectory_buffer.append((obs_t.cpu().numpy(), last_action, last_reward))\n",
    "        context_traj = self.trajectory_buffer.copy()\n",
    "        actions_int = [a for _, a, _ in context_traj]\n",
    "        rewards_float = [r for _, _, r in context_traj]\n",
    "        obs_stack = torch.stack([torch.tensor(o, dtype=torch.float32, device=self.device) for o, _, _ in context_traj])\n",
    "        logits, _, _ = self.policy(\n",
    "            obs_stack, obs_t,\n",
    "            actions=torch.tensor(actions_int, device=self.device),\n",
    "            rewards=torch.tensor(rewards_float, device=self.device)\n",
    "        )\n",
    "        if deterministic:\n",
    "            action = torch.argmax(logits).item()\n",
    "        else:\n",
    "            dist = Categorical(logits=logits)\n",
    "            action = dist.sample().item()\n",
    "        self.last_action = action\n",
    "        self.last_reward = reward\n",
    "        if done:\n",
    "            self.trajectory_buffer = []\n",
    "        return action\n",
    "\n",
    "    def save(self, path=\"memoryppo.pt\"):\n",
    "        \"\"\"Save policy weights to file.\"\"\"\n",
    "        torch.save(self.policy.state_dict(), path)\n",
    "\n",
    "    def load(self, path=\"memoryppo.pt\"):\n",
    "        \"\"\"Load policy weights from file.\"\"\"\n",
    "        self.policy.load_state_dict(torch.load(path, map_location=self.device))\n",
    "\n",
    "    def evaluate(self, n_episodes=10, deterministic=False, verbose=True):\n",
    "        \"\"\"\n",
    "        Evaluates policy over several episodes, reporting mean/std return.\n",
    "\n",
    "        Args:\n",
    "            n_episodes (int): Number of test episodes.\n",
    "            deterministic (bool): Use argmax instead of sampling.\n",
    "            verbose (bool): Print results to console.\n",
    "\n",
    "        Returns:\n",
    "            mean_return (float): Average reward.\n",
    "            std_return (float): Std deviation of rewards.\n",
    "        \"\"\"\n",
    "        returns = []\n",
    "        for _ in range(n_episodes):\n",
    "            obs, _ = self.env.reset()\n",
    "            self.trajectory_buffer = []\n",
    "            done = False\n",
    "            total_reward = 0.0\n",
    "            last_reward = 0.0\n",
    "            while not done:\n",
    "                action = self.predict(obs, deterministic=deterministic, reward=last_reward)\n",
    "                obs, reward, done, _, _ = self.env.step(action)\n",
    "                total_reward += reward\n",
    "                last_reward = reward\n",
    "                if done:\n",
    "                    self.trajectory_buffer = []\n",
    "            returns.append(total_reward)\n",
    "        mean_return = np.mean(returns)\n",
    "        std_return = np.std(returns)\n",
    "        if verbose:\n",
    "            print(f\"Evaluation over {n_episodes} episodes: mean return {mean_return:.2f}, std {std_return:.2f}\")\n",
    "        return mean_return, std_return\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "61f79e99-6906-47db-bc28-f55f181db6eb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        |    4.000  |\n",
      "|    ep_rew_mean        |    0.200  |\n",
      "|    ep_rew_std         |    0.980  |\n",
      "|    policy_entropy     |    0.510  |\n",
      "|    advantage_mean     |    0.929  |\n",
      "|    advantage_std      |    0.052  |\n",
      "|    aux_loss_mean      |    0.000  |\n",
      "| time/                 |           |\n",
      "|    fps                |       33  |\n",
      "|    episodes           |       50  |\n",
      "|    time_elapsed       |        6  |\n",
      "|    total_timesteps    |      200  |\n",
      "| train/                |           |\n",
      "|    loss               |    1.434  |\n",
      "|    policy_loss        |    1.006  |\n",
      "|    value_loss         |    0.866  |\n",
      "|    explained_variance |    0.465  |\n",
      "|    n_updates          |       50  |\n",
      "|    progress           |    5.000  |\n",
      "| memory/               |           |\n",
      "|    usefulness_loss    |    0.003  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        |    4.000  |\n",
      "|    ep_rew_mean        |   -0.200  |\n",
      "|    ep_rew_std         |    0.980  |\n",
      "|    policy_entropy     |    0.670  |\n",
      "|    advantage_mean     |   -0.994  |\n",
      "|    advantage_std      |    0.190  |\n",
      "|    aux_loss_mean      |    0.000  |\n",
      "| time/                 |           |\n",
      "|    fps                |       33  |\n",
      "|    episodes           |      100  |\n",
      "|    time_elapsed       |       12  |\n",
      "|    total_timesteps    |      400  |\n",
      "| train/                |           |\n",
      "|    loss               |   -2.941  |\n",
      "|    policy_loss        |   -3.442  |\n",
      "|    value_loss         |    1.015  |\n",
      "|    explained_variance |   -4.365  |\n",
      "|    n_updates          |      100  |\n",
      "|    progress           |   10.000  |\n",
      "| memory/               |           |\n",
      "|    usefulness_loss    |    0.006  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        |    4.000  |\n",
      "|    ep_rew_mean        |   -0.200  |\n",
      "|    ep_rew_std         |    0.980  |\n",
      "|    policy_entropy     |    0.655  |\n",
      "|    advantage_mean     |    1.029  |\n",
      "|    advantage_std      |    0.077  |\n",
      "|    aux_loss_mean      |    0.000  |\n",
      "| time/                 |           |\n",
      "|    fps                |       33  |\n",
      "|    episodes           |      150  |\n",
      "|    time_elapsed       |       18  |\n",
      "|    total_timesteps    |      600  |\n",
      "| train/                |           |\n",
      "|    loss               |    3.491  |\n",
      "|    policy_loss        |    2.965  |\n",
      "|    value_loss         |    1.064  |\n",
      "|    explained_variance |    0.068  |\n",
      "|    n_updates          |      150  |\n",
      "|    progress           |   15.000  |\n",
      "| memory/               |           |\n",
      "|    usefulness_loss    |    0.006  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        |    4.000  |\n",
      "|    ep_rew_mean        |    0.000  |\n",
      "|    ep_rew_std         |    1.000  |\n",
      "|    policy_entropy     |    0.634  |\n",
      "|    advantage_mean     |    0.920  |\n",
      "|    advantage_std      |    0.071  |\n",
      "|    aux_loss_mean      |    0.000  |\n",
      "| time/                 |           |\n",
      "|    fps                |       33  |\n",
      "|    episodes           |      200  |\n",
      "|    time_elapsed       |       24  |\n",
      "|    total_timesteps    |      800  |\n",
      "| train/                |           |\n",
      "|    loss               |    2.938  |\n",
      "|    policy_loss        |    2.519  |\n",
      "|    value_loss         |    0.850  |\n",
      "|    explained_variance |    0.039  |\n",
      "|    n_updates          |      200  |\n",
      "|    progress           |   20.000  |\n",
      "| memory/               |           |\n",
      "|    usefulness_loss    |    0.003  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        |    4.000  |\n",
      "|    ep_rew_mean        |    0.600  |\n",
      "|    ep_rew_std         |    0.800  |\n",
      "|    policy_entropy     |    0.265  |\n",
      "|    advantage_mean     |   -1.116  |\n",
      "|    advantage_std      |    0.149  |\n",
      "|    aux_loss_mean      |    0.000  |\n",
      "| time/                 |           |\n",
      "|    fps                |       31  |\n",
      "|    episodes           |      250  |\n",
      "|    time_elapsed       |       32  |\n",
      "|    total_timesteps    |     1000  |\n",
      "| train/                |           |\n",
      "|    loss               |   -0.291  |\n",
      "|    policy_loss        |   -0.920  |\n",
      "|    value_loss         |    1.262  |\n",
      "|    explained_variance |   -2.617  |\n",
      "|    n_updates          |      250  |\n",
      "|    progress           |   25.000  |\n",
      "| memory/               |           |\n",
      "|    usefulness_loss    |    0.008  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        |    4.000  |\n",
      "|    ep_rew_mean        |   -0.120  |\n",
      "|    ep_rew_std         |    0.993  |\n",
      "|    policy_entropy     |    0.241  |\n",
      "|    advantage_mean     |   -0.746  |\n",
      "|    advantage_std      |    0.119  |\n",
      "|    aux_loss_mean      |    0.000  |\n",
      "| time/                 |           |\n",
      "|    fps                |       30  |\n",
      "|    episodes           |      300  |\n",
      "|    time_elapsed       |       39  |\n",
      "|    total_timesteps    |     1200  |\n",
      "| train/                |           |\n",
      "|    loss               |   -0.092  |\n",
      "|    policy_loss        |   -0.373  |\n",
      "|    value_loss         |    0.567  |\n",
      "|    explained_variance |   -3.143  |\n",
      "|    n_updates          |      300  |\n",
      "|    progress           |   30.000  |\n",
      "| memory/               |           |\n",
      "|    usefulness_loss    |    0.008  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        |    4.000  |\n",
      "|    ep_rew_mean        |   -0.280  |\n",
      "|    ep_rew_std         |    0.960  |\n",
      "|    policy_entropy     |    0.222  |\n",
      "|    advantage_mean     |    0.675  |\n",
      "|    advantage_std      |    0.156  |\n",
      "|    aux_loss_mean      |    0.000  |\n",
      "| time/                 |           |\n",
      "|    fps                |       30  |\n",
      "|    episodes           |      350  |\n",
      "|    time_elapsed       |       46  |\n",
      "|    total_timesteps    |     1400  |\n",
      "| train/                |           |\n",
      "|    loss               |    0.950  |\n",
      "|    policy_loss        |    0.715  |\n",
      "|    value_loss         |    0.474  |\n",
      "|    explained_variance |   -8.634  |\n",
      "|    n_updates          |      350  |\n",
      "|    progress           |   35.000  |\n",
      "| memory/               |           |\n",
      "|    usefulness_loss    |    0.015  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        |    4.000  |\n",
      "|    ep_rew_mean        |   -0.280  |\n",
      "|    ep_rew_std         |    0.960  |\n",
      "|    policy_entropy     |    0.186  |\n",
      "|    advantage_mean     |    0.100  |\n",
      "|    advantage_std      |    0.148  |\n",
      "|    aux_loss_mean      |    0.000  |\n",
      "| time/                 |           |\n",
      "|    fps                |       30  |\n",
      "|    episodes           |      400  |\n",
      "|    time_elapsed       |       53  |\n",
      "|    total_timesteps    |     1600  |\n",
      "| train/                |           |\n",
      "|    loss               |    0.126  |\n",
      "|    policy_loss        |    0.114  |\n",
      "|    value_loss         |    0.026  |\n",
      "|    explained_variance |  -91.039  |\n",
      "|    n_updates          |      400  |\n",
      "|    progress           |   40.000  |\n",
      "| memory/               |           |\n",
      "|    usefulness_loss    |    0.015  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        |    4.000  |\n",
      "|    ep_rew_mean        |    0.040  |\n",
      "|    ep_rew_std         |    0.999  |\n",
      "|    policy_entropy     |    0.410  |\n",
      "|    advantage_mean     |   -0.424  |\n",
      "|    advantage_std      |    0.302  |\n",
      "|    aux_loss_mean      |    0.000  |\n",
      "| time/                 |           |\n",
      "|    fps                |       30  |\n",
      "|    episodes           |      450  |\n",
      "|    time_elapsed       |       59  |\n",
      "|    total_timesteps    |     1800  |\n",
      "| train/                |           |\n",
      "|    loss               |   -0.839  |\n",
      "|    policy_loss        |   -0.959  |\n",
      "|    value_loss         |    0.248  |\n",
      "|    explained_variance |  -40.083  |\n",
      "|    n_updates          |      450  |\n",
      "|    progress           |   45.000  |\n",
      "| memory/               |           |\n",
      "|    usefulness_loss    |    0.015  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        |    4.000  |\n",
      "|    ep_rew_mean        |    0.800  |\n",
      "|    ep_rew_std         |    0.600  |\n",
      "|    policy_entropy     |    0.272  |\n",
      "|    advantage_mean     |   -2.063  |\n",
      "|    advantage_std      |    0.296  |\n",
      "|    aux_loss_mean      |    0.000  |\n",
      "| time/                 |           |\n",
      "|    fps                |       30  |\n",
      "|    episodes           |      500  |\n",
      "|    time_elapsed       |       65  |\n",
      "|    total_timesteps    |     2000  |\n",
      "| train/                |           |\n",
      "|    loss               |    1.333  |\n",
      "|    policy_loss        |   -0.824  |\n",
      "|    value_loss         |    4.320  |\n",
      "|    explained_variance |   -2.751  |\n",
      "|    n_updates          |      500  |\n",
      "|    progress           |   50.000  |\n",
      "| memory/               |           |\n",
      "|    usefulness_loss    |    0.013  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        |    4.000  |\n",
      "|    ep_rew_mean        |    0.280  |\n",
      "|    ep_rew_std         |    0.960  |\n",
      "|    policy_entropy     |    0.083  |\n",
      "|    advantage_mean     |    0.462  |\n",
      "|    advantage_std      |    0.165  |\n",
      "|    aux_loss_mean      |    0.000  |\n",
      "| time/                 |           |\n",
      "|    fps                |       30  |\n",
      "|    episodes           |      550  |\n",
      "|    time_elapsed       |       72  |\n",
      "|    total_timesteps    |     2200  |\n",
      "| train/                |           |\n",
      "|    loss               |    0.162  |\n",
      "|    policy_loss        |    0.046  |\n",
      "|    value_loss         |    0.234  |\n",
      "|    explained_variance |  -18.963  |\n",
      "|    n_updates          |      550  |\n",
      "|    progress           |   55.000  |\n",
      "| memory/               |           |\n",
      "|    usefulness_loss    |    0.007  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        |    4.000  |\n",
      "|    ep_rew_mean        |    1.000  |\n",
      "|    ep_rew_std         |    0.000  |\n",
      "|    policy_entropy     |    0.099  |\n",
      "|    advantage_mean     |    0.101  |\n",
      "|    advantage_std      |    0.081  |\n",
      "|    aux_loss_mean      |    0.000  |\n",
      "| time/                 |           |\n",
      "|    fps                |       30  |\n",
      "|    episodes           |      600  |\n",
      "|    time_elapsed       |       79  |\n",
      "|    total_timesteps    |     2400  |\n",
      "| train/                |           |\n",
      "|    loss               |    0.023  |\n",
      "|    policy_loss        |    0.017  |\n",
      "|    value_loss         |    0.015  |\n",
      "|    explained_variance |  -20.871  |\n",
      "|    n_updates          |      600  |\n",
      "|    progress           |   60.000  |\n",
      "| memory/               |           |\n",
      "|    usefulness_loss    |    0.018  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        |    4.000  |\n",
      "|    ep_rew_mean        |    0.960  |\n",
      "|    ep_rew_std         |    0.280  |\n",
      "|    policy_entropy     |    0.401  |\n",
      "|    advantage_mean     |    0.065  |\n",
      "|    advantage_std      |    0.162  |\n",
      "|    aux_loss_mean      |    0.000  |\n",
      "| time/                 |           |\n",
      "|    fps                |       30  |\n",
      "|    episodes           |      650  |\n",
      "|    time_elapsed       |       86  |\n",
      "|    total_timesteps    |     2600  |\n",
      "| train/                |           |\n",
      "|    loss               |    0.128  |\n",
      "|    policy_loss        |    0.120  |\n",
      "|    value_loss         |    0.024  |\n",
      "|    explained_variance | -167.524  |\n",
      "|    n_updates          |      650  |\n",
      "|    progress           |   65.000  |\n",
      "| memory/               |           |\n",
      "|    usefulness_loss    |    0.002  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        |    4.000  |\n",
      "|    ep_rew_mean        |    1.000  |\n",
      "|    ep_rew_std         |    0.000  |\n",
      "|    policy_entropy     |    0.403  |\n",
      "|    advantage_mean     |   -0.004  |\n",
      "|    advantage_std      |    0.029  |\n",
      "|    aux_loss_mean      |    0.000  |\n",
      "| time/                 |           |\n",
      "|    fps                |       30  |\n",
      "|    episodes           |      700  |\n",
      "|    time_elapsed       |       93  |\n",
      "|    total_timesteps    |     2800  |\n",
      "| train/                |           |\n",
      "|    loss               |   -0.008  |\n",
      "|    policy_loss        |   -0.005  |\n",
      "|    value_loss         |    0.001  |\n",
      "|    explained_variance |   -3.729  |\n",
      "|    n_updates          |      700  |\n",
      "|    progress           |   70.000  |\n",
      "| memory/               |           |\n",
      "|    usefulness_loss    |    0.001  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        |    4.000  |\n",
      "|    ep_rew_mean        |    0.960  |\n",
      "|    ep_rew_std         |    0.280  |\n",
      "|    policy_entropy     |    0.181  |\n",
      "|    advantage_mean     |   -0.002  |\n",
      "|    advantage_std      |    0.043  |\n",
      "|    aux_loss_mean      |    0.000  |\n",
      "| time/                 |           |\n",
      "|    fps                |       29  |\n",
      "|    episodes           |      750  |\n",
      "|    time_elapsed       |      100  |\n",
      "|    total_timesteps    |     3000  |\n",
      "| train/                |           |\n",
      "|    loss               |   -0.056  |\n",
      "|    policy_loss        |   -0.055  |\n",
      "|    value_loss         |    0.001  |\n",
      "|    explained_variance |   -8.652  |\n",
      "|    n_updates          |      750  |\n",
      "|    progress           |   75.000  |\n",
      "| memory/               |           |\n",
      "|    usefulness_loss    |    0.003  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        |    4.000  |\n",
      "|    ep_rew_mean        |    1.000  |\n",
      "|    ep_rew_std         |    0.000  |\n",
      "|    policy_entropy     |    0.451  |\n",
      "|    advantage_mean     |    0.034  |\n",
      "|    advantage_std      |    0.083  |\n",
      "|    aux_loss_mean      |    0.000  |\n",
      "| time/                 |           |\n",
      "|    fps                |       29  |\n",
      "|    episodes           |      800  |\n",
      "|    time_elapsed       |      107  |\n",
      "|    total_timesteps    |     3200  |\n",
      "| train/                |           |\n",
      "|    loss               |    0.082  |\n",
      "|    policy_loss        |    0.084  |\n",
      "|    value_loss         |    0.006  |\n",
      "|    explained_variance |  -39.427  |\n",
      "|    n_updates          |      800  |\n",
      "|    progress           |   80.000  |\n",
      "| memory/               |           |\n",
      "|    usefulness_loss    |    0.002  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        |    4.000  |\n",
      "|    ep_rew_mean        |    1.000  |\n",
      "|    ep_rew_std         |    0.000  |\n",
      "|    policy_entropy     |    0.189  |\n",
      "|    advantage_mean     |    0.039  |\n",
      "|    advantage_std      |    0.046  |\n",
      "|    aux_loss_mean      |    0.000  |\n",
      "| time/                 |           |\n",
      "|    fps                |       30  |\n",
      "|    episodes           |      850  |\n",
      "|    time_elapsed       |      112  |\n",
      "|    total_timesteps    |     3400  |\n",
      "| train/                |           |\n",
      "|    loss               |    0.001  |\n",
      "|    policy_loss        |    0.002  |\n",
      "|    value_loss         |    0.003  |\n",
      "|    explained_variance |   -7.252  |\n",
      "|    n_updates          |      850  |\n",
      "|    progress           |   85.000  |\n",
      "| memory/               |           |\n",
      "|    usefulness_loss    |    0.001  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        |    4.000  |\n",
      "|    ep_rew_mean        |    1.000  |\n",
      "|    ep_rew_std         |    0.000  |\n",
      "|    policy_entropy     |    0.474  |\n",
      "|    advantage_mean     |    0.025  |\n",
      "|    advantage_std      |    0.035  |\n",
      "|    aux_loss_mean      |    0.000  |\n",
      "| time/                 |           |\n",
      "|    fps                |       29  |\n",
      "|    episodes           |      900  |\n",
      "|    time_elapsed       |      121  |\n",
      "|    total_timesteps    |     3600  |\n",
      "| train/                |           |\n",
      "|    loss               |    0.011  |\n",
      "|    policy_loss        |    0.015  |\n",
      "|    value_loss         |    0.002  |\n",
      "|    explained_variance |   -4.902  |\n",
      "|    n_updates          |      900  |\n",
      "|    progress           |   90.000  |\n",
      "| memory/               |           |\n",
      "|    usefulness_loss    |    0.001  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        |    4.000  |\n",
      "|    ep_rew_mean        |    0.920  |\n",
      "|    ep_rew_std         |    0.392  |\n",
      "|    policy_entropy     |    0.113  |\n",
      "|    advantage_mean     |    0.070  |\n",
      "|    advantage_std      |    0.128  |\n",
      "|    aux_loss_mean      |    0.000  |\n",
      "| time/                 |           |\n",
      "|    fps                |       29  |\n",
      "|    episodes           |      950  |\n",
      "|    time_elapsed       |      127  |\n",
      "|    total_timesteps    |     3800  |\n",
      "| train/                |           |\n",
      "|    loss               |    0.025  |\n",
      "|    policy_loss        |    0.017  |\n",
      "|    value_loss         |    0.017  |\n",
      "|    explained_variance |  -58.169  |\n",
      "|    n_updates          |      950  |\n",
      "|    progress           |   95.000  |\n",
      "| memory/               |           |\n",
      "|    usefulness_loss    |    0.003  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        |    4.000  |\n",
      "|    ep_rew_mean        |    1.000  |\n",
      "|    ep_rew_std         |    0.000  |\n",
      "|    policy_entropy     |    0.122  |\n",
      "|    advantage_mean     |    0.004  |\n",
      "|    advantage_std      |    0.027  |\n",
      "|    aux_loss_mean      |    0.000  |\n",
      "| time/                 |           |\n",
      "|    fps                |       29  |\n",
      "|    episodes           |     1000  |\n",
      "|    time_elapsed       |      134  |\n",
      "|    total_timesteps    |     4000  |\n",
      "| train/                |           |\n",
      "|    loss               |    0.006  |\n",
      "|    policy_loss        |    0.006  |\n",
      "|    value_loss         |    0.001  |\n",
      "|    explained_variance |   -3.691  |\n",
      "|    n_updates          |     1000  |\n",
      "|    progress           |  100.000  |\n",
      "| memory/               |           |\n",
      "|    usefulness_loss    |    0.001  |\n",
      "-------------------------------------\n",
      "Training complete. Total episodes: 1000, total steps: 4000\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# ──────────────────────────────────────────────────────────────\n",
    "# Example training loop\n",
    "# ──────────────────────────────────────────────────────────────\n",
    "\n",
    "# SETUP ===================================\n",
    "DELAY = 4\n",
    "MEM_DIM = 32\n",
    "N_EPISODES = 2500\n",
    "N_MEMORIES = 32\n",
    "\n",
    "AGENT_KWARGS = dict(\n",
    "    device=\"cpu\",\n",
    "    verbose=0,\n",
    "    lam=0.95, \n",
    "    gamma=0.99, \n",
    "    ent_coef=0.01,\n",
    "    learning_rate=1e-3, \n",
    "    \n",
    ")\n",
    "MEMORY_AGENT_KWARGS=dict(\n",
    "    her=False,\n",
    "    reward_norm=False,\n",
    "    aux_modules=None,\n",
    "    \n",
    "    intrinsic_expl=False,\n",
    "    intrinsic_eta=0.01,\n",
    "    \n",
    "    use_rnd=False, \n",
    "    rnd_emb_dim=32, \n",
    "    rnd_lr=1e-3,\n",
    ")\n",
    "\n",
    "# HELPERS =================================\n",
    "def total_timesteps(delay,n_episodes):\n",
    "    return delay * n_episodes\n",
    "\n",
    "# ENVIRONMENT =============================\n",
    "env = MemoryTaskEnv(delay=DELAY, difficulty=0)\n",
    "\n",
    "# MEMORY BUFFER ===========================\n",
    "episodic_buffer = StrategicMemoryBuffer(\n",
    "    obs_dim=env.observation_space.shape[0],\n",
    "    action_dim=1,\n",
    "    mem_dim=MEM_DIM,\n",
    "    max_entries=N_MEMORIES,\n",
    "    device=\"cpu\"\n",
    ")\n",
    "motif_bank = MotifMemoryBank(\n",
    "    obs_dim=env.observation_space.shape[0],\n",
    "    action_dim=1,\n",
    "    mem_dim=MEM_DIM,\n",
    "    n_motifs=32,\n",
    "    motif_len=4,\n",
    "    device=\"cpu\"\n",
    ")\n",
    "combined_memory = CombinedMemoryModule(episodic_buffer, motif_bank)\n",
    "\n",
    "\n",
    "# POLICY NETWORK (use class) ==============\n",
    "policy = StrategicCombinedMemoryPolicy\n",
    "\n",
    "\n",
    "# AGENT SETUP =============================\n",
    "agent = StrategicMemoryAgent(\n",
    "    policy_class=policy,\n",
    "    env=env,\n",
    "    memory=combined_memory,\n",
    "    memory_learn_retention=True,    \n",
    "    memory_retention_coef=0.01,   \n",
    "    # aux_modules=aux_modules,  \n",
    "    device=\"cpu\",\n",
    "    verbose=1,\n",
    "    lam=0.95, \n",
    "    gamma=0.99, \n",
    "    ent_coef=0.01,\n",
    "    learning_rate=1e-3, \n",
    "    \n",
    "    **MEMORY_AGENT_KWARGS\n",
    ")\n",
    "\n",
    "# TRAIN THE AGENT =========================\n",
    "agent.learn(\n",
    "    total_timesteps=total_timesteps(DELAY, 100),\n",
    "    log_interval=50\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7e4cb2c5-55b3-4c70-b9df-2022b15f7cd9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "print(torch.cuda.is_available())  # True if at least one CUDA GPU is available\n",
    "print(torch.cuda.device_count())  # Number of GPUs detected\n",
    "if torch.cuda.device_count():\n",
    "    print(torch.cuda.current_device())  # The current device index (if using one)\n",
    "    print(torch.cuda.get_device_name(0))  # Name of GPU 0 (if available)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7f882e22-7cea-482c-b4ba-2f7d3aa1776e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        |    4.000  |\n",
      "|    ep_rew_mean        |    0.216  |\n",
      "|    ep_rew_std         |    0.976  |\n",
      "|    policy_entropy     |    0.413  |\n",
      "|    advantage_mean     |   -1.596  |\n",
      "|    advantage_std      |    0.138  |\n",
      "|    aux_loss_mean      |    0.000  |\n",
      "| time/                 |           |\n",
      "|    fps                |       38  |\n",
      "|    episodes           |      250  |\n",
      "|    time_elapsed       |       26  |\n",
      "|    total_timesteps    |     1000  |\n",
      "| train/                |           |\n",
      "|    loss               |   -3.936  |\n",
      "|    policy_loss        |   -5.213  |\n",
      "|    value_loss         |    2.563  |\n",
      "|    explained_variance |   -0.392  |\n",
      "|    n_updates          |      250  |\n",
      "|    progress           |    6.250  |\n",
      "| memory/               |           |\n",
      "|    usefulness_loss    |    0.008  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        |    4.000  |\n",
      "|    ep_rew_mean        |    0.896  |\n",
      "|    ep_rew_std         |    0.444  |\n",
      "|    policy_entropy     |    0.236  |\n",
      "|    advantage_mean     |   -0.050  |\n",
      "|    advantage_std      |    0.074  |\n",
      "|    aux_loss_mean      |    0.000  |\n",
      "| time/                 |           |\n",
      "|    fps                |       39  |\n",
      "|    episodes           |      500  |\n",
      "|    time_elapsed       |       51  |\n",
      "|    total_timesteps    |     2000  |\n",
      "| train/                |           |\n",
      "|    loss               |    0.026  |\n",
      "|    policy_loss        |    0.025  |\n",
      "|    value_loss         |    0.007  |\n",
      "|    explained_variance | -104.853  |\n",
      "|    n_updates          |      500  |\n",
      "|    progress           |   12.500  |\n",
      "| memory/               |           |\n",
      "|    usefulness_loss    |    0.002  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        |    4.000  |\n",
      "|    ep_rew_mean        |    0.944  |\n",
      "|    ep_rew_std         |    0.330  |\n",
      "|    policy_entropy     |    0.190  |\n",
      "|    advantage_mean     |    0.004  |\n",
      "|    advantage_std      |    0.050  |\n",
      "|    aux_loss_mean      |    0.000  |\n",
      "| time/                 |           |\n",
      "|    fps                |       41  |\n",
      "|    episodes           |      750  |\n",
      "|    time_elapsed       |       73  |\n",
      "|    total_timesteps    |     3000  |\n",
      "| train/                |           |\n",
      "|    loss               |    0.028  |\n",
      "|    policy_loss        |    0.029  |\n",
      "|    value_loss         |    0.002  |\n",
      "|    explained_variance |  -15.351  |\n",
      "|    n_updates          |      750  |\n",
      "|    progress           |   18.750  |\n",
      "| memory/               |           |\n",
      "|    usefulness_loss    |    0.001  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        |    4.000  |\n",
      "|    ep_rew_mean        |    1.000  |\n",
      "|    ep_rew_std         |    0.000  |\n",
      "|    policy_entropy     |    0.203  |\n",
      "|    advantage_mean     |   -0.024  |\n",
      "|    advantage_std      |    0.024  |\n",
      "|    aux_loss_mean      |    0.000  |\n",
      "| time/                 |           |\n",
      "|    fps                |       41  |\n",
      "|    episodes           |     1000  |\n",
      "|    time_elapsed       |       96  |\n",
      "|    total_timesteps    |     4000  |\n",
      "| train/                |           |\n",
      "|    loss               |   -0.022  |\n",
      "|    policy_loss        |   -0.020  |\n",
      "|    value_loss         |    0.001  |\n",
      "|    explained_variance |   -3.571  |\n",
      "|    n_updates          |     1000  |\n",
      "|    progress           |   25.000  |\n",
      "| memory/               |           |\n",
      "|    usefulness_loss    |    0.001  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        |    4.000  |\n",
      "|    ep_rew_mean        |    0.992  |\n",
      "|    ep_rew_std         |    0.126  |\n",
      "|    policy_entropy     |    0.188  |\n",
      "|    advantage_mean     |   -0.004  |\n",
      "|    advantage_std      |    0.018  |\n",
      "|    aux_loss_mean      |    0.000  |\n",
      "| time/                 |           |\n",
      "|    fps                |       42  |\n",
      "|    episodes           |     1250  |\n",
      "|    time_elapsed       |      119  |\n",
      "|    total_timesteps    |     5000  |\n",
      "| train/                |           |\n",
      "|    loss               |   -0.007  |\n",
      "|    policy_loss        |   -0.005  |\n",
      "|    value_loss         |    0.000  |\n",
      "|    explained_variance |   -0.997  |\n",
      "|    n_updates          |     1250  |\n",
      "|    progress           |   31.250  |\n",
      "| memory/               |           |\n",
      "|    usefulness_loss    |    0.001  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        |    4.000  |\n",
      "|    ep_rew_mean        |    0.992  |\n",
      "|    ep_rew_std         |    0.126  |\n",
      "|    policy_entropy     |    0.145  |\n",
      "|    advantage_mean     |   -0.000  |\n",
      "|    advantage_std      |    0.062  |\n",
      "|    aux_loss_mean      |    0.000  |\n",
      "| time/                 |           |\n",
      "|    fps                |       42  |\n",
      "|    episodes           |     1500  |\n",
      "|    time_elapsed       |      142  |\n",
      "|    total_timesteps    |     6000  |\n",
      "| train/                |           |\n",
      "|    loss               |    0.009  |\n",
      "|    policy_loss        |    0.009  |\n",
      "|    value_loss         |    0.003  |\n",
      "|    explained_variance |  -22.651  |\n",
      "|    n_updates          |     1500  |\n",
      "|    progress           |   37.500  |\n",
      "| memory/               |           |\n",
      "|    usefulness_loss    |    0.001  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        |    4.000  |\n",
      "|    ep_rew_mean        |    1.000  |\n",
      "|    ep_rew_std         |    0.000  |\n",
      "|    policy_entropy     |    0.157  |\n",
      "|    advantage_mean     |    0.016  |\n",
      "|    advantage_std      |    0.029  |\n",
      "|    aux_loss_mean      |    0.000  |\n",
      "| time/                 |           |\n",
      "|    fps                |       42  |\n",
      "|    episodes           |     1750  |\n",
      "|    time_elapsed       |      166  |\n",
      "|    total_timesteps    |     7000  |\n",
      "| train/                |           |\n",
      "|    loss               |    0.047  |\n",
      "|    policy_loss        |    0.048  |\n",
      "|    value_loss         |    0.001  |\n",
      "|    explained_variance |   -3.752  |\n",
      "|    n_updates          |     1750  |\n",
      "|    progress           |   43.750  |\n",
      "| memory/               |           |\n",
      "|    usefulness_loss    |    0.001  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        |    4.000  |\n",
      "|    ep_rew_mean        |    1.000  |\n",
      "|    ep_rew_std         |    0.000  |\n",
      "|    policy_entropy     |    0.166  |\n",
      "|    advantage_mean     |    0.012  |\n",
      "|    advantage_std      |    0.059  |\n",
      "|    aux_loss_mean      |    0.000  |\n",
      "| time/                 |           |\n",
      "|    fps                |       42  |\n",
      "|    episodes           |     2000  |\n",
      "|    time_elapsed       |      189  |\n",
      "|    total_timesteps    |     8000  |\n",
      "| train/                |           |\n",
      "|    loss               |    0.007  |\n",
      "|    policy_loss        |    0.008  |\n",
      "|    value_loss         |    0.003  |\n",
      "|    explained_variance |  -17.149  |\n",
      "|    n_updates          |     2000  |\n",
      "|    progress           |   50.000  |\n",
      "| memory/               |           |\n",
      "|    usefulness_loss    |    0.001  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        |    4.000  |\n",
      "|    ep_rew_mean        |    1.000  |\n",
      "|    ep_rew_std         |    0.000  |\n",
      "|    policy_entropy     |    0.167  |\n",
      "|    advantage_mean     |   -0.002  |\n",
      "|    advantage_std      |    0.024  |\n",
      "|    aux_loss_mean      |    0.000  |\n",
      "| time/                 |           |\n",
      "|    fps                |       42  |\n",
      "|    episodes           |     2250  |\n",
      "|    time_elapsed       |      213  |\n",
      "|    total_timesteps    |     9000  |\n",
      "| train/                |           |\n",
      "|    loss               |   -0.005  |\n",
      "|    policy_loss        |   -0.004  |\n",
      "|    value_loss         |    0.000  |\n",
      "|    explained_variance |   -2.716  |\n",
      "|    n_updates          |     2250  |\n",
      "|    progress           |   56.250  |\n",
      "| memory/               |           |\n",
      "|    usefulness_loss    |    0.001  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        |    4.000  |\n",
      "|    ep_rew_mean        |    1.000  |\n",
      "|    ep_rew_std         |    0.000  |\n",
      "|    policy_entropy     |    0.190  |\n",
      "|    advantage_mean     |   -0.019  |\n",
      "|    advantage_std      |    0.025  |\n",
      "|    aux_loss_mean      |    0.000  |\n",
      "| time/                 |           |\n",
      "|    fps                |       42  |\n",
      "|    episodes           |     2500  |\n",
      "|    time_elapsed       |      235  |\n",
      "|    total_timesteps    |    10000  |\n",
      "| train/                |           |\n",
      "|    loss               |   -0.017  |\n",
      "|    policy_loss        |   -0.016  |\n",
      "|    value_loss         |    0.001  |\n",
      "|    explained_variance |   -3.333  |\n",
      "|    n_updates          |     2500  |\n",
      "|    progress           |   62.500  |\n",
      "| memory/               |           |\n",
      "|    usefulness_loss    |    0.001  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        |    4.000  |\n",
      "|    ep_rew_mean        |    0.984  |\n",
      "|    ep_rew_std         |    0.178  |\n",
      "|    policy_entropy     |    0.175  |\n",
      "|    advantage_mean     |    0.018  |\n",
      "|    advantage_std      |    0.019  |\n",
      "|    aux_loss_mean      |    0.000  |\n",
      "| time/                 |           |\n",
      "|    fps                |       42  |\n",
      "|    episodes           |     2750  |\n",
      "|    time_elapsed       |      258  |\n",
      "|    total_timesteps    |    11000  |\n",
      "| train/                |           |\n",
      "|    loss               |   -0.006  |\n",
      "|    policy_loss        |   -0.005  |\n",
      "|    value_loss         |    0.001  |\n",
      "|    explained_variance |   -0.787  |\n",
      "|    n_updates          |     2750  |\n",
      "|    progress           |   68.750  |\n",
      "| memory/               |           |\n",
      "|    usefulness_loss    |    0.002  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        |    4.000  |\n",
      "|    ep_rew_mean        |    1.000  |\n",
      "|    ep_rew_std         |    0.000  |\n",
      "|    policy_entropy     |    0.177  |\n",
      "|    advantage_mean     |    0.006  |\n",
      "|    advantage_std      |    0.025  |\n",
      "|    aux_loss_mean      |    0.000  |\n",
      "| time/                 |           |\n",
      "|    fps                |       42  |\n",
      "|    episodes           |     3000  |\n",
      "|    time_elapsed       |      283  |\n",
      "|    total_timesteps    |    12000  |\n",
      "| train/                |           |\n",
      "|    loss               |   -0.006  |\n",
      "|    policy_loss        |   -0.005  |\n",
      "|    value_loss         |    0.000  |\n",
      "|    explained_variance |   -2.387  |\n",
      "|    n_updates          |     3000  |\n",
      "|    progress           |   75.000  |\n",
      "| memory/               |           |\n",
      "|    usefulness_loss    |    0.002  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        |    4.000  |\n",
      "|    ep_rew_mean        |    1.000  |\n",
      "|    ep_rew_std         |    0.000  |\n",
      "|    policy_entropy     |    0.176  |\n",
      "|    advantage_mean     |   -0.033  |\n",
      "|    advantage_std      |    0.021  |\n",
      "|    aux_loss_mean      |    0.000  |\n",
      "| time/                 |           |\n",
      "|    fps                |       42  |\n",
      "|    episodes           |     3250  |\n",
      "|    time_elapsed       |      307  |\n",
      "|    total_timesteps    |    13000  |\n",
      "| train/                |           |\n",
      "|    loss               |   -0.037  |\n",
      "|    policy_loss        |   -0.035  |\n",
      "|    value_loss         |    0.001  |\n",
      "|    explained_variance |   -2.583  |\n",
      "|    n_updates          |     3250  |\n",
      "|    progress           |   81.250  |\n",
      "| memory/               |           |\n",
      "|    usefulness_loss    |    0.001  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        |    4.000  |\n",
      "|    ep_rew_mean        |    1.000  |\n",
      "|    ep_rew_std         |    0.000  |\n",
      "|    policy_entropy     |    0.175  |\n",
      "|    advantage_mean     |    0.035  |\n",
      "|    advantage_std      |    0.061  |\n",
      "|    aux_loss_mean      |    0.000  |\n",
      "| time/                 |           |\n",
      "|    fps                |       42  |\n",
      "|    episodes           |     3500  |\n",
      "|    time_elapsed       |      331  |\n",
      "|    total_timesteps    |    14000  |\n",
      "| train/                |           |\n",
      "|    loss               |   -0.027  |\n",
      "|    policy_loss        |   -0.027  |\n",
      "|    value_loss         |    0.004  |\n",
      "|    explained_variance |  -13.078  |\n",
      "|    n_updates          |     3500  |\n",
      "|    progress           |   87.500  |\n",
      "| memory/               |           |\n",
      "|    usefulness_loss    |    0.001  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        |    4.000  |\n",
      "|    ep_rew_mean        |    1.000  |\n",
      "|    ep_rew_std         |    0.000  |\n",
      "|    policy_entropy     |    0.176  |\n",
      "|    advantage_mean     |    0.005  |\n",
      "|    advantage_std      |    0.032  |\n",
      "|    aux_loss_mean      |    0.000  |\n",
      "| time/                 |           |\n",
      "|    fps                |       42  |\n",
      "|    episodes           |     3750  |\n",
      "|    time_elapsed       |      354  |\n",
      "|    total_timesteps    |    15000  |\n",
      "| train/                |           |\n",
      "|    loss               |   -0.019  |\n",
      "|    policy_loss        |   -0.018  |\n",
      "|    value_loss         |    0.001  |\n",
      "|    explained_variance |   -4.425  |\n",
      "|    n_updates          |     3750  |\n",
      "|    progress           |   93.750  |\n",
      "| memory/               |           |\n",
      "|    usefulness_loss    |    0.001  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        |    4.000  |\n",
      "|    ep_rew_mean        |    1.000  |\n",
      "|    ep_rew_std         |    0.000  |\n",
      "|    policy_entropy     |    0.176  |\n",
      "|    advantage_mean     |    0.018  |\n",
      "|    advantage_std      |    0.039  |\n",
      "|    aux_loss_mean      |    0.000  |\n",
      "| time/                 |           |\n",
      "|    fps                |       42  |\n",
      "|    episodes           |     4000  |\n",
      "|    time_elapsed       |      376  |\n",
      "|    total_timesteps    |    16000  |\n",
      "| train/                |           |\n",
      "|    loss               |    0.044  |\n",
      "|    policy_loss        |    0.045  |\n",
      "|    value_loss         |    0.001  |\n",
      "|    explained_variance |   -7.929  |\n",
      "|    n_updates          |     4000  |\n",
      "|    progress           |  100.000  |\n",
      "| memory/               |           |\n",
      "|    usefulness_loss    |    0.001  |\n",
      "-------------------------------------\n",
      "Training complete. Total episodes: 4000, total steps: 16000\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# ENVIRONMENT =============================\n",
    "env = MemoryTaskEnv(delay=DELAY, difficulty=0)\n",
    "\n",
    "# MEMORY BUFFER ===========================\n",
    "memory = StrategicMemoryBuffer(\n",
    "    obs_dim=env.observation_space.shape[0],\n",
    "    action_dim=1,          # For Discrete(2)\n",
    "    mem_dim=MEM_DIM,\n",
    "    max_entries=N_MEMORIES,\n",
    "    device=\"cpu\"\n",
    ")\n",
    "\n",
    "# POLICY NETWORK (use class) ==============\n",
    "policy = StrategicMemoryTransformerPolicy\n",
    "\n",
    "# (optional) AUXILIARY MODULES ============\n",
    "\"\"\"\n",
    "aux_modules = [\n",
    "    CueAuxModule(feat_dim=MEM_DIM*2, n_classes=2),\n",
    "    ConfidenceAuxModule(feat_dim=MEM_DIM*2)\n",
    "]\n",
    "\"\"\"\n",
    "\n",
    "# AGENT SETUP =============================\n",
    "agent = StrategicMemoryAgent(\n",
    "    policy_class=policy,\n",
    "    env=env,\n",
    "    memory=memory,\n",
    "    memory_learn_retention=True,    \n",
    "    memory_retention_coef=0.01,   \n",
    "    # aux_modules=aux_modules,  \n",
    "    device=\"cpu\",\n",
    "    verbose=1,\n",
    "    lam=0.95, \n",
    "    gamma=0.99, \n",
    "    ent_coef=0.01,\n",
    "    learning_rate=1e-3, \n",
    "    \n",
    "    **MEMORY_AGENT_KWARGS\n",
    ")\n",
    "\n",
    "# TRAIN THE AGENT =========================\n",
    "agent.learn(\n",
    "    total_timesteps=total_timesteps(DELAY, 100),\n",
    "    log_interval=50\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2b05814b-37e0-4b95-9111-c9f29e99af11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.5.1\n",
      "None\n",
      "False\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.__version__)\n",
    "print(torch.version.cuda)\n",
    "print(torch.cuda.is_available())\n",
    "print(torch.cuda.device_count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2611c15c-771e-4305-9a14-42ffe5cfed7b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:trading]",
   "language": "python",
   "name": "conda-env-trading-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
