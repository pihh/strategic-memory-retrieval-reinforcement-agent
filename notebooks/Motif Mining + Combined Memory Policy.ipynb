{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "35273d81-4f3a-419d-9a40-7e63658818b4",
   "metadata": {},
   "source": [
    "# Motif Mining / Combined Memory Module\n",
    "\n",
    "### What is a Motif (in time series/finance context):\n",
    "A motif is a short, distinctive, recurring pattern in time series data (e.g., “three rising candles”, “hammer candlestick”).\n",
    "\n",
    "Motif mining is about discovering these patterns that frequently occur, possibly before some important event (like price spikes).\n",
    "\n",
    "Motifs can be fixed length (e.g., always 3 bars) or variable length, but they're usually not tied to any particular \"agent experience\" or reward—they're just patterns that are statistically common or relevant to outcomes.\n",
    "\n",
    "### How does this differ from current agent’s memory?\n",
    "Motifs could become part of memory if they prove useful, but in RL, memory entries are scored and selected by usefulness to the agent’s task, not just frequency.\n",
    "\n",
    "* **Motif:**\n",
    "  * **Purely pattern-based:** “What sequence shapes show up often in the market?”\n",
    "  * **Unsupervised:** Does not depend on what the agent did or the rewards/outcomes.\n",
    "  * Often discovered with algorithms like matrix profile, SAX, or clustering over subsequences.\n",
    "\n",
    "* **Strategic RL Memory:**\n",
    "  * Stores sequences of observations, actions, rewards from actual episodes, tied to what the agent did and what outcome it got.\n",
    "  * Is used for retrieval during decision-making, not just for pattern mining.\n",
    "  * Memory can be trained to only keep those episodes/patterns that are useful for policy improvement, not just frequent.\n",
    "\n",
    "* **Summary:**\n",
    "  * **Motif:**  Statistically recurring pattern in the world\n",
    "  * **Memory_** Agent’s own experienced or retained pattern, which it can choose to use, forget, or score for future use\n",
    "\n",
    "\n",
    "### Goal:\n",
    "\n",
    "* Get both kinds of retrieval in one single process.\n",
    "\n",
    "### Summary:\n",
    "* All memory retrieval (episodic and motif) is neural, attention-based, and trainable.\n",
    "\n",
    "* Motif memory can be used for either unsupervised mining (offline DTW) or end-to-end learned patterns.\n",
    "\n",
    "* Everything is differentiable and ready for RL + auxiliary losses.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3f941562-a714-4a19-9550-9585826a25a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "\n",
    "sys.path.append('../')\n",
    "from environments import MemoryTaskEnv\n",
    "from memory import StrategicMemoryBuffer, BaseMemoryBuffer,StrategicMemoryTransformerPolicy\n",
    "from agent import StrategicMemoryAgent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "247b3482-ff8c-4072-91a2-dde8dd748776",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MotifMemoryBank(BaseMemoryBuffer):\n",
    "    \"\"\"\n",
    "    Motif memory: learnable bank of pattern embeddings, attention-retrieved.\n",
    "\n",
    "    Features:\n",
    "        - Stores K motif embeddings, trainable.\n",
    "        - Neural encoder to encode subtrajectories as motifs.\n",
    "        - Attention over motifs given current context trajectory.\n",
    "    \"\"\"\n",
    "    def __init__(self, obs_dim, action_dim, mem_dim=32, n_motifs=32, motif_len=4, device='cpu'):\n",
    "        super().__init__()\n",
    "        self.obs_dim = obs_dim\n",
    "        self.action_dim = action_dim\n",
    "        self.mem_dim = mem_dim\n",
    "        self.n_motifs = n_motifs\n",
    "        self.motif_len = motif_len\n",
    "        self.device = device\n",
    "        self.last_attn = None\n",
    "        # Learnable motif memory bank\n",
    "        self.motif_embeds = nn.Parameter(torch.randn(n_motifs, mem_dim))\n",
    "        # Neural encoder for extracting motifs from subtrajectories\n",
    "        self.embedding_proj = nn.Linear(obs_dim + action_dim + 1, mem_dim)\n",
    "        self.encoder = nn.TransformerEncoder(\n",
    "            nn.TransformerEncoderLayer(d_model=mem_dim, nhead=2, batch_first=True),\n",
    "            num_layers=1\n",
    "        )\n",
    "\n",
    "    def retrieve(self, context_traj):\n",
    "        \"\"\"\n",
    "        Attends over motif bank using the latest motif_len steps of the context trajectory.\n",
    "        \"\"\"\n",
    "        if len(context_traj) < self.motif_len:\n",
    "            pad = [context_traj[0]] * (self.motif_len - len(context_traj))\n",
    "            motif_traj = pad + context_traj\n",
    "        else:\n",
    "            motif_traj = context_traj[-self.motif_len:]\n",
    "\n",
    "        motif_np = np.array([np.concatenate([obs, [a], [r]]) for obs, a, r in motif_traj], dtype=np.float32)\n",
    "        motif_input = torch.from_numpy(motif_np).unsqueeze(0).to(self.device)\n",
    "        motif_embed = self.encoder(self.embedding_proj(motif_input)).mean(dim=1).squeeze(0)  # [mem_dim]\n",
    "        attn_logits = torch.matmul(self.motif_embeds, motif_embed)\n",
    "        attn = torch.softmax(attn_logits, dim=0)\n",
    "        motif_readout = (attn.unsqueeze(1) * self.motif_embeds).sum(dim=0)\n",
    "        self.last_attn = attn.detach().cpu().numpy()\n",
    "        return motif_readout, attn\n",
    "\n",
    "    def motif_parameters(self):\n",
    "        return [self.motif_embeds]\n",
    "\n",
    "    def get_trainable_parameters(self):\n",
    "        return list(self.parameters()) + list(self.motif_parameters())\n",
    "\n",
    "    def get_last_attention(self):\n",
    "        return self.last_attn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0846a20f-0156-427d-b681-25f117318aa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CombinedMemoryModule(BaseMemoryBuffer):\n",
    "    def __init__(self, episodic_buffer, motif_bank):\n",
    "        super().__init__()\n",
    "        self.episodic_buffer = episodic_buffer\n",
    "        self.motif_bank = motif_bank\n",
    "        self.last_attn = None\n",
    "\n",
    "\n",
    "    def retrieve(self, context_trajectory):\n",
    "        epi_readout, epi_attn = self.episodic_buffer.retrieve(context_trajectory)\n",
    "        motif_readout, motif_attn = self.motif_bank.retrieve(context_trajectory)\n",
    "        combined = torch.cat([epi_readout, motif_readout], dim=-1)\n",
    "        self.last_attn = (epi_attn, motif_attn)\n",
    "        return combined, epi_attn, motif_attn\n",
    "\n",
    "    def add_entry(self, trajectory, outcome):\n",
    "        self.episodic_buffer.add_entry(trajectory, outcome)\n",
    "        # Motif bank may NOT need this, but later might optionally do motif mining here \n",
    "        # For now, only episodic buffer gets new entries\n",
    "        # If you want motifs to be updated with experience, call self.motif_bank.add_entry(trajectory, outcome) if you define it\n",
    "\n",
    "    def get_trainable_parameters(self):\n",
    "        params = []\n",
    "        if hasattr(self, \"episodic_buffer\"):\n",
    "            params += self.episodic_buffer.get_trainable_parameters()\n",
    "        if hasattr(self, \"motif_bank\"):\n",
    "            params += self.motif_bank.get_trainable_parameters()\n",
    "        return params\n",
    "\n",
    "    def get_last_attention(self):\n",
    "        return self.last_attn  # tuple: (episodic, motif)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1cfeb726-0c71-4142-9e12-307e3feb623b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class StrategicCombinedMemoryPolicy(nn.Module):\n",
    "    def __init__(self, obs_dim, mem_dim=32, nhead=4, memory=None, aux_modules=None, **kwargs):\n",
    "        super().__init__()\n",
    "        self.mem_dim = mem_dim\n",
    "        self.embed = nn.Linear(obs_dim, mem_dim)\n",
    "        self.pos_embed = nn.Embedding(256, mem_dim)\n",
    "        encoder_layer = nn.TransformerEncoderLayer(d_model=mem_dim, nhead=nhead, batch_first=True)\n",
    "        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=1)\n",
    "        self.policy_head = nn.Linear(mem_dim + 2 * mem_dim, 2)   # now +2mem_dim (episodic + motif)\n",
    "        self.value_head = nn.Linear(mem_dim + 2 * mem_dim, 1)\n",
    "        self.aux_modules = aux_modules if aux_modules is not None else []\n",
    "        self.memory = memory\n",
    "\n",
    "    def forward(self, trajectory, obs_t=None, actions=None, rewards=None):\n",
    "        T = trajectory.shape[0]\n",
    "        x = self.embed(trajectory)\n",
    "        pos = torch.arange(T, device=trajectory.device)\n",
    "        x = x + self.pos_embed(pos)\n",
    "        x = x.unsqueeze(0)\n",
    "        x = self.transformer(x)\n",
    "        feat = x[0, -1]\n",
    "\n",
    "        mem_feat = torch.zeros(2 * self.mem_dim, device=feat.device)\n",
    "        epi_attn, motif_attn = None, None\n",
    "        if self.memory is not None and actions is not None and rewards is not None:\n",
    "            actions_list = actions.tolist()\n",
    "            rewards_list = rewards.tolist()\n",
    "            if len(actions_list) < T:\n",
    "                actions_list = [0] * (T - len(actions_list)) + actions_list\n",
    "            if len(rewards_list) < T:\n",
    "                rewards_list = [0.0] * (T - len(rewards_list)) + rewards_list\n",
    "            context_traj = [\n",
    "                (trajectory[i].cpu().numpy(), actions_list[i], rewards_list[i]) for i in range(T)\n",
    "            ]\n",
    "            mem_feat, epi_attn, motif_attn = self.memory.retrieve(context_traj)\n",
    "        final_feat = torch.cat([feat, mem_feat], dim=-1)\n",
    "        logits = self.policy_head(final_feat)\n",
    "        value = self.value_head(final_feat)\n",
    "        aux_preds = {}\n",
    "        for aux in self.aux_modules:\n",
    "            aux_preds[aux.name] = aux.head(final_feat)\n",
    "        return logits, value.squeeze(-1), aux_preds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "61f79e99-6906-47db-bc28-f55f181db6eb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# ──────────────────────────────────────────────────────────────\n",
    "# Example training loop\n",
    "# ──────────────────────────────────────────────────────────────\n",
    "\n",
    "# SETUP ===================================\n",
    "DELAY = 4\n",
    "MEM_DIM = 32\n",
    "N_EPISODES = 2500\n",
    "N_MEMORIES = 32\n",
    "\n",
    "AGENT_KWARGS = dict(\n",
    "    device=\"cpu\",\n",
    "    verbose=0,\n",
    "    lam=0.95, \n",
    "    gamma=0.99, \n",
    "    ent_coef=0.01,\n",
    "    learning_rate=1e-3, \n",
    "    \n",
    ")\n",
    "MEMORY_AGENT_KWARGS=dict(\n",
    "    her=False,\n",
    "    reward_norm=False,\n",
    "    aux_modules=None,\n",
    "    \n",
    "    intrinsic_expl=False,\n",
    "    intrinsic_eta=0.01,\n",
    "    \n",
    "    use_rnd=False, \n",
    "    rnd_emb_dim=32, \n",
    "    rnd_lr=1e-3,\n",
    ")\n",
    "\n",
    "# HELPERS =================================\n",
    "def total_timesteps(delay,n_episodes):\n",
    "    return delay * n_episodes\n",
    "\n",
    "# ENVIRONMENT =============================\n",
    "env = MemoryTaskEnv(delay=DELAY, difficulty=0)\n",
    "\n",
    "# MEMORY BUFFER ===========================\n",
    "episodic_buffer = StrategicMemoryBuffer(\n",
    "    obs_dim=env.observation_space.shape[0],\n",
    "    action_dim=1,\n",
    "    mem_dim=MEM_DIM,\n",
    "    max_entries=N_MEMORIES,\n",
    "    device=\"cpu\"\n",
    ")\n",
    "motif_bank = MotifMemoryBank(\n",
    "    obs_dim=env.observation_space.shape[0],\n",
    "    action_dim=1,\n",
    "    mem_dim=MEM_DIM,\n",
    "    n_motifs=32,\n",
    "    motif_len=4,\n",
    "    device=\"cpu\"\n",
    ")\n",
    "combined_memory = CombinedMemoryModule(episodic_buffer, motif_bank)\n",
    "\n",
    "\n",
    "# POLICY NETWORK (use class) ==============\n",
    "policy = StrategicCombinedMemoryPolicy\n",
    "\n",
    "\n",
    "# AGENT SETUP =============================\n",
    "agent = StrategicMemoryAgent(\n",
    "    policy_class=policy,\n",
    "    env=env,\n",
    "    memory=combined_memory,\n",
    "    memory_learn_retention=True,    \n",
    "    memory_retention_coef=0.01,   \n",
    "    # aux_modules=aux_modules,  \n",
    "    device=\"cpu\",\n",
    "    verbose=1,\n",
    "    lam=0.95, \n",
    "    gamma=0.99, \n",
    "    ent_coef=0.01,\n",
    "    learning_rate=1e-3, \n",
    "    \n",
    "    **MEMORY_AGENT_KWARGS\n",
    ")\n",
    "\n",
    "# TRAIN THE AGENT =========================\n",
    "#agent.learn(\n",
    "#    total_timesteps=total_timesteps(DELAY, 1000),\n",
    "#    log_interval=50\n",
    "#)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "7f882e22-7cea-482c-b4ba-2f7d3aa1776e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ENVIRONMENT =============================\n",
    "env = MemoryTaskEnv(delay=DELAY, difficulty=0)\n",
    "\n",
    "# MEMORY BUFFER ===========================\n",
    "memory = StrategicMemoryBuffer(\n",
    "    obs_dim=env.observation_space.shape[0],\n",
    "    action_dim=1,          # For Discrete(2)\n",
    "    mem_dim=MEM_DIM,\n",
    "    max_entries=N_MEMORIES,\n",
    "    device=\"cpu\"\n",
    ")\n",
    "\n",
    "# POLICY NETWORK (use class) ==============\n",
    "policy = StrategicMemoryTransformerPolicy\n",
    "\n",
    "# (optional) AUXILIARY MODULES ============\n",
    "\"\"\"\n",
    "aux_modules = [\n",
    "    CueAuxModule(feat_dim=MEM_DIM*2, n_classes=2),\n",
    "    ConfidenceAuxModule(feat_dim=MEM_DIM*2)\n",
    "]\n",
    "\"\"\"\n",
    "\n",
    "# AGENT SETUP =============================\n",
    "agent = StrategicMemoryAgent(\n",
    "    policy_class=policy,\n",
    "    env=env,\n",
    "    memory=memory,\n",
    "    memory_learn_retention=True,    \n",
    "    memory_retention_coef=0.01,   \n",
    "    # aux_modules=aux_modules,  \n",
    "    device=\"cpu\",\n",
    "    verbose=0,\n",
    "    lam=0.95, \n",
    "    gamma=0.99, \n",
    "    ent_coef=0.01,\n",
    "    learning_rate=1e-3, \n",
    "    \n",
    "    **MEMORY_AGENT_KWARGS\n",
    ")\n",
    "\n",
    "# TRAIN THE AGENT =========================\n",
    "#agent.learn(\n",
    "#    total_timesteps=total_timesteps(DELAY, 100),\n",
    "#    log_interval=50\n",
    "#)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "0ca74c30-e6e8-4d4d-8496-75fcc36d2789",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "def predict_ep_duration(delay):\n",
    "    delays = np.array([2, 4, 8, 16, 32, 64, 128,258])\n",
    "    avg_durations = np.array([0.02, 0.03, 0.06, 0.12, 0.26, 0.61, 1.52,4.18])\n",
    "    coeff = np.polyfit(delays, avg_durations, 1)\n",
    "    # Use the fitted slope from your data\n",
    "    return coeff[0] * delay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "ae40384a-021a-4a2c-b1b8-e8ccfc617ecb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8615542774982028"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_ep_duration(54)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b90e42e9-f342-41ce-b60b-c38c7168a02f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "055292e2-402b-4132-818b-4d9b73105628",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d023b37-8f6a-4ced-961e-a66f23a1f377",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Training with delay=2 ---\n",
      "╭──────────────────────┬───────╮\n",
      "│ Early Stop           │       │\n",
      "├──────────────────────┼───────┤\n",
      "│ Train duration       │ 11s   │\n",
      "│ Avg episode duration │ 0.02s │\n",
      "│ Rolling ep rew mean  │ 1.00  │\n",
      "│ Rolling ep rew std   │ 0.00  │\n",
      "│ Rolling ep length    │ 2.00  │\n",
      "│ N updates            │ 619   │\n",
      "╰──────────────────────┴───────╯\n",
      "╭──────────────┬────╮\n",
      "│ Evaluation   │    │\n",
      "├──────────────┼────┤\n",
      "│ Avg reward   │  1 │\n",
      "│ Std reward   │  0 │\n",
      "╰──────────────┴────╯\n",
      "\n",
      "--- Training with delay=4 ---\n",
      "╭──────────────────────┬───────╮\n",
      "│ Early Stop           │       │\n",
      "├──────────────────────┼───────┤\n",
      "│ Train duration       │ 3s    │\n",
      "│ Avg episode duration │ 0.03s │\n",
      "│ Rolling ep rew mean  │ 1.00  │\n",
      "│ Rolling ep rew std   │ 0.00  │\n",
      "│ Rolling ep length    │ 4.00  │\n",
      "│ N updates            │ 100   │\n",
      "╰──────────────────────┴───────╯\n",
      "╭──────────────┬────╮\n",
      "│ Evaluation   │    │\n",
      "├──────────────┼────┤\n",
      "│ Avg reward   │  1 │\n",
      "│ Std reward   │  0 │\n",
      "╰──────────────┴────╯\n",
      "\n",
      "--- Training with delay=8 ---\n",
      "╭──────────────────────┬───────╮\n",
      "│ Early Stop           │       │\n",
      "├──────────────────────┼───────┤\n",
      "│ Train duration       │ 7s    │\n",
      "│ Avg episode duration │ 0.05s │\n",
      "│ Rolling ep rew mean  │ 1.00  │\n",
      "│ Rolling ep rew std   │ 0.00  │\n",
      "│ Rolling ep length    │ 8.00  │\n",
      "│ N updates            │ 128   │\n",
      "╰──────────────────────┴───────╯\n",
      "╭──────────────┬────╮\n",
      "│ Evaluation   │    │\n",
      "├──────────────┼────┤\n",
      "│ Avg reward   │  1 │\n",
      "│ Std reward   │  0 │\n",
      "╰──────────────┴────╯\n",
      "\n",
      "--- Training with delay=16 ---\n",
      "╭──────────────────────┬───────╮\n",
      "│ Early Stop           │       │\n",
      "├──────────────────────┼───────┤\n",
      "│ Train duration       │ 12s   │\n",
      "│ Avg episode duration │ 0.12s │\n",
      "│ Rolling ep rew mean  │ 1.00  │\n",
      "│ Rolling ep rew std   │ 0.00  │\n",
      "│ Rolling ep length    │ 16.00 │\n",
      "│ N updates            │ 100   │\n",
      "╰──────────────────────┴───────╯\n",
      "╭──────────────┬────╮\n",
      "│ Evaluation   │    │\n",
      "├──────────────┼────┤\n",
      "│ Avg reward   │  1 │\n",
      "│ Std reward   │  0 │\n",
      "╰──────────────┴────╯\n",
      "\n",
      "--- Training with delay=32 ---\n",
      "╭──────────────────────┬───────╮\n",
      "│ Early Stop           │       │\n",
      "├──────────────────────┼───────┤\n",
      "│ Train duration       │ 65s   │\n",
      "│ Avg episode duration │ 0.28s │\n",
      "│ Rolling ep rew mean  │ 1.00  │\n",
      "│ Rolling ep rew std   │ 0.00  │\n",
      "│ Rolling ep length    │ 32.00 │\n",
      "│ N updates            │ 235   │\n",
      "╰──────────────────────┴───────╯\n",
      "╭──────────────┬────╮\n",
      "│ Evaluation   │    │\n",
      "├──────────────┼────┤\n",
      "│ Avg reward   │  1 │\n",
      "│ Std reward   │  0 │\n",
      "╰──────────────┴────╯\n",
      "\n",
      "--- Training with delay=64 ---\n",
      "╭──────────────────────┬───────╮\n",
      "│ Early Stop           │       │\n",
      "├──────────────────────┼───────┤\n",
      "│ Train duration       │ 63s   │\n",
      "│ Avg episode duration │ 0.63s │\n",
      "│ Rolling ep rew mean  │ 1.00  │\n",
      "│ Rolling ep rew std   │ 0.00  │\n",
      "│ Rolling ep length    │ 64.00 │\n",
      "│ N updates            │ 100   │\n",
      "╰──────────────────────┴───────╯\n",
      "╭──────────────┬────╮\n",
      "│ Evaluation   │    │\n",
      "├──────────────┼────┤\n",
      "│ Avg reward   │  1 │\n",
      "│ Std reward   │  0 │\n",
      "╰──────────────┴────╯\n",
      "\n",
      "--- Training with delay=128 ---\n"
     ]
    }
   ],
   "source": [
    "from benchmark import AgentPerformanceBenchmark\n",
    "\n",
    "env = MemoryTaskEnv(delay=delay, difficulty=0)\n",
    "\n",
    "# MEMORY BUFFER ===========================\n",
    "memory = StrategicMemoryBuffer(\n",
    "        obs_dim=env.observation_space.shape[0],\n",
    "        action_dim=1,          # For Discrete(2)\n",
    "        mem_dim=MEM_DIM,\n",
    "        max_entries=N_MEMORIES,\n",
    "        device=\"cpu\"\n",
    "    )\n",
    "    \n",
    "# POLICY NETWORK (use class) ==============\n",
    "policy = StrategicMemoryTransformerPolicy\n",
    "\n",
    "agent = StrategicMemoryAgent(\n",
    "        policy_class=policy,\n",
    "        env=env,\n",
    "        memory=memory,\n",
    "        memory_learn_retention=True,    \n",
    "        memory_retention_coef=0.01,   \n",
    "        # aux_modules=aux_modules,  \n",
    "        device=\"cpu\",\n",
    "        verbose=0,\n",
    "        lam=0.95, \n",
    "        gamma=0.99, \n",
    "        ent_coef=0.01,\n",
    "        learning_rate=1e-3, \n",
    "        \n",
    "        **MEMORY_AGENT_KWARGS\n",
    "    )\n",
    "curriculum = [2, 4, 8, 16,32,64,128,256]\n",
    "for delay in curriculum:\n",
    "    agent.env.delay = delay\n",
    "    #agent.get_episodic_buffer().reset()\n",
    "    print(f\"\\n--- Training with delay={delay} ---\")\n",
    "    agent.learn(total_timesteps=total_timesteps(delay, 100000), log_interval=50)\n",
    "    \n",
    "    benchmark = AgentPerformanceBenchmark(dict(delay=delay, n_train_episodes=2000, total_timesteps=1_000_000, difficulty=0, mode_name=\"EASY\", verbose=0, eval_base=True),)\n",
    "    e_r, e_s = benchmark.evaluate(agent,'motif')\n",
    "    table = [[\"Avg reward\",e_r],[\"Std reward\",e_s]]\n",
    "    print(tabulate(table, headers=[\"Evaluation\",\"\"], tablefmt=\"rounded_outline\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ba97dc9-5d8a-4b2e-9583-3aeaa3d829c4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
