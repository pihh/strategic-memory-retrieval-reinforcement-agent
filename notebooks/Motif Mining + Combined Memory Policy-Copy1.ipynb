{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "35273d81-4f3a-419d-9a40-7e63658818b4",
   "metadata": {},
   "source": [
    "# Motif Mining / Combined Memory Module\n",
    "\n",
    "### What is a Motif (in time series/finance context):\n",
    "A motif is a short, distinctive, recurring pattern in time series data (e.g., “three rising candles”, “hammer candlestick”).\n",
    "\n",
    "Motif mining is about discovering these patterns that frequently occur, possibly before some important event (like price spikes).\n",
    "\n",
    "Motifs can be fixed length (e.g., always 3 bars) or variable length, but they're usually not tied to any particular \"agent experience\" or reward—they're just patterns that are statistically common or relevant to outcomes.\n",
    "\n",
    "### How does this differ from current agent’s memory?\n",
    "Motifs could become part of memory if they prove useful, but in RL, memory entries are scored and selected by usefulness to the agent’s task, not just frequency.\n",
    "\n",
    "* **Motif:**\n",
    "  * **Purely pattern-based:** “What sequence shapes show up often in the market?”\n",
    "  * **Unsupervised:** Does not depend on what the agent did or the rewards/outcomes.\n",
    "  * Often discovered with algorithms like matrix profile, SAX, or clustering over subsequences.\n",
    "\n",
    "* **Strategic RL Memory:**\n",
    "  * Stores sequences of observations, actions, rewards from actual episodes, tied to what the agent did and what outcome it got.\n",
    "  * Is used for retrieval during decision-making, not just for pattern mining.\n",
    "  * Memory can be trained to only keep those episodes/patterns that are useful for policy improvement, not just frequent.\n",
    "\n",
    "* **Summary:**\n",
    "  * **Motif:**  Statistically recurring pattern in the world\n",
    "  * **Memory_** Agent’s own experienced or retained pattern, which it can choose to use, forget, or score for future use\n",
    "\n",
    "\n",
    "### Goal:\n",
    "\n",
    "* Get both kinds of retrieval in one single process.\n",
    "\n",
    "### Summary:\n",
    "* All memory retrieval (episodic and motif) is neural, attention-based, and trainable.\n",
    "\n",
    "* Motif memory can be used for either unsupervised mining (offline DTW) or end-to-end learned patterns.\n",
    "\n",
    "* Everything is differentiable and ready for RL + auxiliary losses.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3f941562-a714-4a19-9550-9585826a25a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "\n",
    "sys.path.append('../')\n",
    "from environments import MemoryTaskEnv\n",
    "from memory import StrategicMemoryBuffer, BaseMemoryBuffer,StrategicMemoryTransformerPolicy\n",
    "from agent import TraceRL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6e5283d8-7c34-455d-a70f-e561a1c2ead9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "\n",
    "class BaseMemoryBuffer(nn.Module):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__()\n",
    "\n",
    "    def get_last_attention(self):\n",
    "        \"\"\"Return the latest attention weights, if available.\"\"\"\n",
    "        raise NotImplementedError(\"Memory module must implement get_last_attention()\")\n",
    "\n",
    "class StrategicMemoryBuffer(BaseMemoryBuffer):\n",
    "    \"\"\"\n",
    "    Episodic memory buffer for RL agents using neural trajectory encoding and\n",
    "    attention-based retrieval, with learnable usefulness/retention scores.\n",
    "\n",
    "    Features:\n",
    "        - Stores episode trajectories and outcomes.\n",
    "        - Each trajectory is encoded as a vector with a Transformer.\n",
    "        - Each memory entry gets a trainable usefulness parameter.\n",
    "        - When full, discards the least-useful entry.\n",
    "        - Returns soft-attended memory readout given a context trajectory.\n",
    "\n",
    "    Args:\n",
    "        obs_dim (int): Observation dimension.\n",
    "        action_dim (int): Action dimension (scalar=1).\n",
    "        mem_dim (int): Embedding size for memory entries.\n",
    "        max_entries (int): Max entries to keep (FIFO with learning).\n",
    "        device (str): Device for tensors (\"cpu\" or \"cuda\").\n",
    "    \"\"\"\n",
    "\n",
    "    __version__     = \"1.3.0\"\n",
    "    __description__ = \"Memory usefullness is now trainable and retention is based on it\"\n",
    "\n",
    "\n",
    "    def __init__(self, obs_dim, action_dim, mem_dim=32, max_entries=100, device=DEFAULT_DEVICE):\n",
    "        super().__init__()\n",
    "      \n",
    "        self.obs_dim = obs_dim\n",
    "        self.action_dim = action_dim\n",
    "        self.mem_dim = mem_dim\n",
    "        self.max_entries = max_entries\n",
    "        self.device = device\n",
    "\n",
    "        self.embedding_proj = nn.Linear(obs_dim + action_dim + 1, mem_dim)\n",
    "        self.encoder = nn.TransformerEncoder(\n",
    "            nn.TransformerEncoderLayer(d_model=mem_dim, nhead=2, batch_first=True),\n",
    "            num_layers=1\n",
    "        )\n",
    "\n",
    "        # Fixed usefulness per slot (size max_entries)\n",
    "        self.usefulness_vec = nn.Parameter(torch.zeros(max_entries, device=self.device), requires_grad=True)\n",
    "        self.entries = []\n",
    "        self._entry_indices = []  # Maps entries to slot indices\n",
    "\n",
    "    def reset(self):\n",
    "        \"\"\"Clears all memory entries (keeps usefulness param, but marks buffer empty).\"\"\"\n",
    "        self.entries = []\n",
    "        self._entry_indices = []\n",
    "\n",
    "    def add_entry(self, trajectory, outcome):\n",
    "        \"\"\"\n",
    "        Stores a new trajectory/outcome. If full, replaces the least useful slot.\n",
    "        \"\"\"\n",
    "        traj_np = np.array([np.concatenate([obs, [action], [reward]]) for obs, action, reward in trajectory], dtype=np.float32)\n",
    "        traj = torch.from_numpy(traj_np).to(self.device)\n",
    "        traj_proj = self.embedding_proj(traj)\n",
    "        mem_embed = self.encoder(traj_proj.unsqueeze(0)).mean(dim=1).squeeze(0)\n",
    "        entry = {\n",
    "            'trajectory': trajectory,\n",
    "            'outcome': outcome,\n",
    "            'embedding': mem_embed.detach()\n",
    "        }\n",
    "        if len(self.entries) < self.max_entries:\n",
    "            # Use next available slot (by index)\n",
    "            slot_idx = len(self.entries)\n",
    "            self.entries.append(entry)\n",
    "            self._entry_indices.append(slot_idx)\n",
    "        else:\n",
    "            # Overwrite least-useful slot\n",
    "            usefulness_scores = self.usefulness_vec.detach().cpu()\n",
    "            idx_remove = usefulness_scores.argmin().item()\n",
    "            self.entries[idx_remove] = entry\n",
    "            self._entry_indices[idx_remove] = idx_remove\n",
    "\n",
    "    def retrieve(self, context_trajectory):\n",
    "        if len(self.entries) == 0:\n",
    "            self.last_attn = None\n",
    "            return torch.zeros(self.mem_dim, device=self.device), None, None\n",
    "        traj_np = np.array([np.concatenate([obs, [action], [reward]]) for obs, action, reward in context_trajectory], dtype=np.float32)\n",
    "        traj = torch.from_numpy(traj_np).to(self.device)\n",
    "        traj_proj = self.embedding_proj(traj)\n",
    "        context_embed = self.encoder(traj_proj.unsqueeze(0)).mean(dim=1).squeeze(0)\n",
    "        mem_embeddings = torch.stack([e['embedding'] for e in self.entries])  # [N, mem_dim]\n",
    "        attn_logits = torch.matmul(mem_embeddings, context_embed)\n",
    "        attn = torch.softmax(attn_logits, dim=0)\n",
    "        mem_readout = (attn.unsqueeze(1) * mem_embeddings).sum(dim=0)\n",
    "        self.last_attn = attn.detach().cpu().numpy()\n",
    "        return mem_readout, attn, None  # PATCH: add None for motif_attn\n",
    "\n",
    "    def usefulness_loss(self, attn, reward):\n",
    "        \"\"\"\n",
    "        Compute usefulness loss: encourages usefulness score to match (attn * reward).\n",
    "        Only updates the active slots (those with entries).\n",
    "        \"\"\"\n",
    "        # attn: [N] attention weights used for the memory readout\n",
    "        # reward: scalar, or [N]\n",
    "        N = len(self.entries)\n",
    "        if N == 0:\n",
    "            return torch.tensor(0.0, device=self.device)\n",
    "        idxs = self._entry_indices[:N]\n",
    "        usefulness_vec = self.usefulness_vec[idxs]  # Only the in-use slots\n",
    "        if isinstance(reward, (float, int)):\n",
    "            reward = torch.tensor(reward, dtype=torch.float32, device=self.device)\n",
    "        targets = attn.detach() * reward  # [N]\n",
    "        loss = ((usefulness_vec - targets) ** 2).mean()\n",
    "        return loss\n",
    "\n",
    "    def usefulness_parameters(self):\n",
    "        return [self.usefulness_vec]\n",
    "\n",
    "    def get_trainable_parameters(self):\n",
    "        # Everything that should be trained here\n",
    "        return list(self.parameters()) + list(self.usefulness_parameters())\n",
    "\n",
    "    def get_last_attention(self):\n",
    "        return getattr(self, \"last_attn\", None)\n",
    "        \n",
    "    def retrieve_with_custom_query(self, custom_query):\n",
    "        \"\"\"\n",
    "        Attend over memory entries using a custom query vector (not trajectory context).\n",
    "        \"\"\"\n",
    "        if len(self.entries) == 0:\n",
    "            self.last_attn = None\n",
    "            return torch.zeros(self.mem_dim, device=self.device), None\n",
    "        mem_embeddings = torch.stack([e['embedding'] for e in self.entries])  # [N, mem_dim]\n",
    "        attn_logits = torch.matmul(mem_embeddings, custom_query)\n",
    "        attn = torch.softmax(attn_logits, dim=0)\n",
    "        mem_readout = (attn.unsqueeze(1) * mem_embeddings).sum(dim=0)\n",
    "        self.last_attn = attn.detach().cpu().numpy()\n",
    "        return mem_readout, attn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "247b3482-ff8c-4072-91a2-dde8dd748776",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MotifMemoryBank(BaseMemoryBuffer):\n",
    "    \"\"\"\n",
    "    Motif memory: learnable bank of pattern embeddings, attention-retrieved.\n",
    "\n",
    "    Features:\n",
    "        - Stores K motif embeddings, trainable.\n",
    "        - Neural encoder to encode subtrajectories as motifs.\n",
    "        - Attention over motifs given current context trajectory.\n",
    "    \"\"\"\n",
    "    def __init__(self, obs_dim, action_dim, mem_dim=32, n_motifs=32, motif_len=4, device='cpu'):\n",
    "        super().__init__()\n",
    "        self.obs_dim = obs_dim\n",
    "        self.action_dim = action_dim\n",
    "        self.mem_dim = mem_dim\n",
    "        self.n_motifs = n_motifs\n",
    "        self.motif_len = motif_len\n",
    "        self.device = device\n",
    "        self.last_attn = None\n",
    "        # Learnable motif memory bank\n",
    "        self.motif_embeds = nn.Parameter(torch.randn(n_motifs, mem_dim))\n",
    "        # Neural encoder for extracting motifs from subtrajectories\n",
    "        self.embedding_proj = nn.Linear(obs_dim + action_dim + 1, mem_dim)\n",
    "        self.encoder = nn.TransformerEncoder(\n",
    "            nn.TransformerEncoderLayer(d_model=mem_dim, nhead=2, batch_first=True),\n",
    "            num_layers=1\n",
    "        )\n",
    "\n",
    "    def retrieve(self, context_traj):\n",
    "        if len(context_traj) < self.motif_len:\n",
    "            pad = [context_traj[0]] * (self.motif_len - len(context_traj))\n",
    "            motif_traj = pad + context_traj\n",
    "        else:\n",
    "            motif_traj = context_traj[-self.motif_len:]\n",
    "\n",
    "        motif_np = np.array([np.concatenate([obs, [a], [r]]) for obs, a, r in motif_traj], dtype=np.float32)\n",
    "        motif_input = torch.from_numpy(motif_np).unsqueeze(0).to(self.device)\n",
    "        motif_embed = self.encoder(self.embedding_proj(motif_input)).mean(dim=1).squeeze(0)  # [mem_dim]\n",
    "        attn_logits = torch.matmul(self.motif_embeds, motif_embed)\n",
    "        attn = torch.softmax(attn_logits, dim=0)\n",
    "        motif_readout = (attn.unsqueeze(1) * self.motif_embeds).sum(dim=0)\n",
    "        self.last_attn = attn.detach().cpu().numpy()\n",
    "        return motif_readout, None, attn  # PATCH: None for epi_attn, attn for motif_attn\n",
    "\n",
    "    def motif_parameters(self):\n",
    "        return [self.motif_embeds]\n",
    "\n",
    "    def get_trainable_parameters(self):\n",
    "        return list(self.parameters()) + list(self.motif_parameters())\n",
    "\n",
    "    def get_last_attention(self):\n",
    "        return self.last_attn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0846a20f-0156-427d-b681-25f117318aa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CombinedMemoryModule(BaseMemoryBuffer):\n",
    "    def __init__(self, episodic_buffer, motif_bank):\n",
    "        super().__init__()\n",
    "        self.episodic_buffer = episodic_buffer\n",
    "        self.motif_bank = motif_bank\n",
    "        self.last_attn = None\n",
    "\n",
    "\n",
    "    def retrieve(self, context_trajectory):\n",
    "        motif_readout, motif_attn, _ = self.motif_bank.retrieve(context_trajectory)\n",
    "        epi_readout, epi_attn, _ = self.episodic_buffer.retrieve_with_custom_query(motif_readout)\n",
    "        combined = torch.cat([epi_readout, motif_readout], dim=-1)\n",
    "        self.last_attn = (epi_attn, motif_attn)\n",
    "        return combined, epi_attn, motif_attn\n",
    "\n",
    "    def add_entry(self, trajectory, outcome):\n",
    "        self.episodic_buffer.add_entry(trajectory, outcome)\n",
    "        # Motif bank may NOT need this, but later might optionally do motif mining here \n",
    "        # For now, only episodic buffer gets new entries\n",
    "        # If you want motifs to be updated with experience, call self.motif_bank.add_entry(trajectory, outcome) if you define it\n",
    "\n",
    "    def get_trainable_parameters(self):\n",
    "        params = []\n",
    "        if hasattr(self, \"episodic_buffer\"):\n",
    "            params += self.episodic_buffer.get_trainable_parameters()\n",
    "        if hasattr(self, \"motif_bank\"):\n",
    "            params += self.motif_bank.get_trainable_parameters()\n",
    "        return params\n",
    "\n",
    "    def get_last_attention(self):\n",
    "        return self.last_attn  # tuple: (episodic, motif)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1cfeb726-0c71-4142-9e12-307e3feb623b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class StrategicCombinedMemoryPolicy(nn.Module):\n",
    "    def __init__(self, obs_dim, mem_dim=32, nhead=4, memory=None, aux_modules=None, **kwargs):\n",
    "        super().__init__()\n",
    "        self.mem_dim = mem_dim\n",
    "        self.embed = nn.Linear(obs_dim, mem_dim)\n",
    "        self.pos_embed = nn.Embedding(256, mem_dim)\n",
    "        encoder_layer = nn.TransformerEncoderLayer(d_model=mem_dim, nhead=nhead, batch_first=True)\n",
    "        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=1)\n",
    "        self.policy_head = nn.Linear(mem_dim + 2 * mem_dim, 2)   # now +2mem_dim (episodic + motif)\n",
    "        self.value_head = nn.Linear(mem_dim + 2 * mem_dim, 1)\n",
    "        self.aux_modules = aux_modules if aux_modules is not None else []\n",
    "        self.memory = memory\n",
    "\n",
    "    def forward(self, trajectory, obs_t=None, actions=None, rewards=None):\n",
    "        T = trajectory.shape[0]\n",
    "        x = self.embed(trajectory)\n",
    "        pos = torch.arange(T, device=trajectory.device)\n",
    "        x = x + self.pos_embed(pos)\n",
    "        x = x.unsqueeze(0)\n",
    "        x = self.transformer(x)\n",
    "        feat = x[0, -1]\n",
    "\n",
    "        mem_feat = torch.zeros(2 * self.mem_dim, device=feat.device)\n",
    "        epi_attn, motif_attn = None, None\n",
    "        if self.memory is not None and actions is not None and rewards is not None:\n",
    "            actions_list = actions.tolist()\n",
    "            rewards_list = rewards.tolist()\n",
    "            if len(actions_list) < T:\n",
    "                actions_list = [0] * (T - len(actions_list)) + actions_list\n",
    "            if len(rewards_list) < T:\n",
    "                rewards_list = [0.0] * (T - len(rewards_list)) + rewards_list\n",
    "            context_traj = [\n",
    "                (trajectory[i].cpu().numpy(), actions_list[i], rewards_list[i]) for i in range(T)\n",
    "            ]\n",
    "            mem_raw, epi_attn, motif_attn = self.memory.retrieve(context_traj)\n",
    "            # Robust: shape of mem_raw\n",
    "            if mem_raw.shape[0] == 2 * self.mem_dim:\n",
    "                mem_feat = mem_raw\n",
    "            elif mem_raw.shape[0] == self.mem_dim:\n",
    "                # Pad: put episodic first, motif second (or vice versa, as you wish)\n",
    "                # Here: [episodic, motif], pad motif with zeros if only episodic is present\n",
    "                if epi_attn is not None and motif_attn is None:\n",
    "                    mem_feat = torch.cat([mem_raw, torch.zeros(self.mem_dim, device=mem_raw.device)], dim=0)\n",
    "                elif motif_attn is not None and epi_attn is None:\n",
    "                    mem_feat = torch.cat([torch.zeros(self.mem_dim, device=mem_raw.device), mem_raw], dim=0)\n",
    "                else:\n",
    "                    # fallback: both None, just zeros\n",
    "                    mem_feat = torch.zeros(2 * self.mem_dim, device=mem_raw.device)\n",
    "        final_feat = torch.cat([feat, mem_feat], dim=-1)\n",
    "        logits = self.policy_head(final_feat)\n",
    "        value = self.value_head(final_feat)\n",
    "        aux_preds = {}\n",
    "        for aux in self.aux_modules:\n",
    "            aux_preds[aux.name] = aux.head(final_feat)\n",
    "        return logits, value.squeeze(-1), aux_preds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3fb36fd3-cd9c-425c-aab4-69f39e1c21ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "def mine_motifs_from_buffer(episodic_buffer, motif_bank, motif_len=4, n_motifs=32, min_windows=100):\n",
    "    \"\"\"\n",
    "    Mine most frequent motifs from episodic buffer and refresh the motif bank.\n",
    "    - motif_bank: instance of MotifMemoryBank\n",
    "    - episodic_buffer: should have .entries (each has 'trajectory')\n",
    "    \"\"\"\n",
    "    subtraj_embeds = []\n",
    "    for entry in episodic_buffer.entries:\n",
    "        traj = entry['trajectory']\n",
    "        if len(traj) >= motif_len:\n",
    "            for i in range(len(traj) - motif_len + 1):\n",
    "                window = traj[i:i+motif_len]\n",
    "                # Convert window to tensor and embed\n",
    "                window_np = np.array([np.concatenate([obs, [a], [r]]) for obs, a, r in window], dtype=np.float32)\n",
    "                window_tensor = torch.from_numpy(window_np).unsqueeze(0).to(motif_bank.device)\n",
    "                with torch.no_grad():\n",
    "                    embed = motif_bank.encoder(motif_bank.embedding_proj(window_tensor)).mean(dim=1).squeeze(0).cpu().numpy()\n",
    "                subtraj_embeds.append(embed)\n",
    "    if len(subtraj_embeds) < max(n_motifs, min_windows):\n",
    "        print(f\"Motif mining: not enough motif windows ({len(subtraj_embeds)}) for {n_motifs} motifs.\")\n",
    "        return\n",
    "    X = np.stack(subtraj_embeds)\n",
    "    kmeans = KMeans(n_clusters=n_motifs, random_state=0, n_init=\"auto\").fit(X)\n",
    "    centroids = torch.tensor(kmeans.cluster_centers_, dtype=torch.float32, device=motif_bank.device)\n",
    "    with torch.no_grad():\n",
    "        motif_bank.motif_embeds.copy_(centroids)\n",
    "    print(f\"[Motif mining] Updated motif bank with {n_motifs} clusters from buffer.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "534ee9f2-ce60-4c0b-a1d0-669c2fad50f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import time\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.distributions import Categorical\n",
    "from tabulate import tabulate\n",
    "\n",
    "from core_modules import RewardNormalizer, StateCounter, RNDModule\n",
    "from core_calculations import compute_gae, compute_explained_variance\n",
    "from callbacks import print_sb3_style_log_box\n",
    "MINING_INTERVAL = 100 \n",
    "\n",
    "class TraceRL:\n",
    "    \"\"\"\n",
    "    Proximal Policy Optimization (PPO) agent with integrated external memory retrieval.\n",
    "\n",
    "    Features:\n",
    "        - Supports auxiliary losses, HER, reward normalization, and RND-based exploration.\n",
    "        - Episodic or contextual memory (passed as `memory`) for strategic RL.\n",
    "        - Plug-and-play auxiliary modules (e.g., cue, event, confidence).\n",
    "        - Stable training with reward normalization and intrinsic/extrinsic reward mixing.\n",
    "\n",
    "    Args:\n",
    "        policy_class (nn.Module): Policy network class (should accept obs_dim, memory, aux_modules).\n",
    "        env (gym.Env): Gymnasium environment.\n",
    "        verbose (int): Logging verbosity (0 = silent, 1 = logs).\n",
    "        learning_rate (float): Adam optimizer learning rate.\n",
    "        gamma (float): Discount factor.\n",
    "        lam (float): GAE lambda.\n",
    "        device (str): Torch device.\n",
    "        her (bool): Enable Hindsight Experience Replay (if supported by env).\n",
    "        reward_norm (bool): Normalize reward with running stats.\n",
    "        intrinsic_expl (bool): Use count-based intrinsic reward.\n",
    "        intrinsic_eta (float): Scaling for intrinsic bonus.\n",
    "        ent_coef (float): Entropy coefficient.\n",
    "        memory: Memory module for contextual/episodic learning (optional).\n",
    "        aux_modules (list): List of auxiliary task modules (optional).\n",
    "        use_rnd (bool): Enable Random Network Distillation intrinsic reward.\n",
    "        rnd_emb_dim (int): Embedding dim for RND networks.\n",
    "        rnd_lr (float): Learning rate for RND predictor.\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    __version__ = \"1.4.0\"\n",
    "\n",
    "    def __init__(\n",
    "        self, \n",
    "        policy_class, \n",
    "        env, \n",
    "        verbose=0,\n",
    "        learning_rate=1e-3, \n",
    "        gamma=0.99, \n",
    "        lam=0.95, \n",
    "        ent_coef=0.01,\n",
    "        device=\"cpu\",\n",
    "        her=False,\n",
    "        reward_norm=False,\n",
    "        intrinsic_expl=True,\n",
    "        intrinsic_eta=0.01,\n",
    "        memory=None,\n",
    "        aux_modules=None,\n",
    "        use_rnd=False, \n",
    "        rnd_emb_dim=32, \n",
    "        rnd_lr=1e-3,\n",
    "        memory_learn_retention=False,      \n",
    "        memory_retention_coef=0.01,\n",
    "        early_stop=True,\n",
    "        early_stop_n_samples=100,\n",
    "        early_stop_mean_threshold=0.95,\n",
    "        early_stop_std_threshold=0.05,\n",
    "    ):\n",
    "        self.env = env\n",
    "        self.device = torch.device(device)\n",
    "        self.gamma = gamma\n",
    "        self.lam = lam\n",
    "        self.ent_coef = ent_coef\n",
    "        self.verbose = verbose\n",
    "        self.memory = memory\n",
    "        self.memory_learn_retention = memory_learn_retention\n",
    "        self.memory_retention_coef = memory_retention_coef\n",
    "        self.aux_modules = aux_modules if aux_modules is not None else []\n",
    "        self.aux = len(self.aux_modules) > 0\n",
    "        self.early_stop= early_stop\n",
    "        self.early_stop_n_samples=early_stop_n_samples\n",
    "        self.early_stop_mean_threshold= early_stop_mean_threshold\n",
    "        self.early_stop_std_threshold= early_stop_std_threshold\n",
    "        # Policy: must accept obs_dim, memory, aux_modules\n",
    "        self.policy = policy_class(\n",
    "            obs_dim=env.observation_space.shape[0], \n",
    "            memory=memory,\n",
    "            aux_modules=self.aux_modules\n",
    "        ).to(self.device)\n",
    "\n",
    "        # PATCH: include modular learning parameters to the optimizer \n",
    "\n",
    "        params = list(self.policy.parameters())\n",
    "        if self.memory_learn_retention and hasattr(self.memory, \"usefulness_parameters\"):\n",
    "            params += list(self.memory.usefulness_parameters())\n",
    "        params = list({id(p): p for p in params}.values())  # REMOVE DUPLICATES\n",
    "        self.optimizer = torch.optim.Adam(params, lr=learning_rate)\n",
    "\n",
    "        self.training_steps = 0\n",
    "        self.episode_rewards = []\n",
    "        self.episode_lengths = []\n",
    "        self.her = her\n",
    "        self.reward_norm = reward_norm\n",
    "        self.intrinsic_expl = intrinsic_expl\n",
    "        self.intrinsic_eta = intrinsic_eta\n",
    "        self.reward_normalizer = RewardNormalizer()\n",
    "        self.state_counter = StateCounter()\n",
    "        self.use_rnd = use_rnd\n",
    "        if self.use_rnd:\n",
    "            self.rnd = RNDModule(env.observation_space.shape[0], emb_dim=rnd_emb_dim).to(self.device)\n",
    "            self.rnd_optimizer = torch.optim.Adam(self.rnd.predictor.parameters(), lr=rnd_lr)\n",
    "        self.trajectory_buffer = []\n",
    "\n",
    "    def reset_trajectory(self):\n",
    "        self.trajectory_buffer = []\n",
    "\n",
    "    def run_episode(self, her_target=None):\n",
    "        obs, _ = self.env.reset()\n",
    "        if her_target is not None:\n",
    "            obs[0] = her_target\n",
    "\n",
    "        done = False\n",
    "        trajectory, actions, rewards, log_probs, values = [], [], [], [], []\n",
    "        entropies_ep, aux_preds_list = [], []\n",
    "        gate_history, memory_size_history = [], []\n",
    "        attn_weights = None\n",
    "        initial_cue = int(obs[0])\n",
    "        aux_targets_ep = {aux.name: [] for aux in self.aux_modules}\n",
    "        context_traj = []  # For memory module\n",
    "\n",
    "        while not done:\n",
    "            obs_t = torch.tensor(obs, dtype=torch.float32, device=self.device)\n",
    "            trajectory.append(obs_t)\n",
    "            traj = torch.stack(trajectory)\n",
    "            action_for_mem = actions[-1].item() if len(actions) > 0 else 0\n",
    "            reward_for_mem = rewards[-1].item() if len(rewards) > 0 else 0.0\n",
    "            context_traj.append((obs_t.cpu().numpy(), action_for_mem, reward_for_mem))\n",
    "\n",
    "            logits, value, aux_preds = self.policy(\n",
    "                traj, obs_t,\n",
    "                actions=torch.tensor([a.item() for a in actions], device=self.device) if actions else None,\n",
    "                rewards=torch.tensor([r.item() for r in rewards], device=self.device) if rewards else None\n",
    "            )\n",
    "            dist = Categorical(logits=logits)\n",
    "            action = dist.sample()\n",
    "            log_prob = dist.log_prob(action)\n",
    "            entropy = dist.entropy()\n",
    "            obs, reward, done, _, _ = self.env.step(action.item())\n",
    "\n",
    "            # Intrinsic reward: count-based and/or RND\n",
    "            if self.intrinsic_expl:\n",
    "                reward += self.intrinsic_eta * self.state_counter.intrinsic_reward(obs)\n",
    "            rnd_intrinsic = 0.0\n",
    "            if self.use_rnd:\n",
    "                with torch.no_grad():\n",
    "                    obs_rnd = obs_t.unsqueeze(0)\n",
    "                    rnd_intrinsic = self.rnd(obs_rnd).item()\n",
    "                    reward += self.intrinsic_eta * rnd_intrinsic\n",
    "\n",
    "            actions.append(action)\n",
    "            log_probs.append(log_prob)\n",
    "            rewards.append(torch.tensor(reward, dtype=torch.float32, device=self.device))\n",
    "            values.append(value)\n",
    "            entropies_ep.append(entropy)\n",
    "            aux_preds_list.append(aux_preds)\n",
    "\n",
    "            # Auxiliary targets (for supervised heads)\n",
    "            for aux in self.aux_modules:\n",
    "                if aux.name == \"cue\":\n",
    "                    aux_targets_ep[aux.name].append(initial_cue)\n",
    "                elif aux.name == \"next_obs\":\n",
    "                    aux_targets_ep[aux.name].append(torch.tensor(obs, dtype=torch.float32))\n",
    "                elif aux.name == \"confidence\":\n",
    "                    dist = Categorical(logits=logits)\n",
    "                    entropy = dist.entropy().item()\n",
    "                    confidence = 1.0 - entropy  # Heuristic; can be improved\n",
    "                    aux_targets_ep[aux.name].append(confidence)\n",
    "                elif aux.name == \"event\":\n",
    "                    event_flag = getattr(self.env, \"event_flag\", 0)\n",
    "                    aux_targets_ep[aux.name].append(event_flag)\n",
    "                elif aux.name == \"oracle_action\":\n",
    "                    oracle_action = getattr(self.env, \"oracle_action\", None)\n",
    "                    aux_targets_ep[aux.name].append(oracle_action)\n",
    "                else:\n",
    "                    aux_targets_ep[aux.name].append(0)\n",
    "\n",
    "        # Store full trajectory in memory module (episodic buffer)\n",
    "        if self.memory is not None:\n",
    "            outcome = sum([r.item() for r in rewards])\n",
    "            # Modular handling: always update episode buffer if available, else fallback\n",
    "            \n",
    "            if hasattr(self.memory, \"episodic_buffer\") and hasattr(self.memory.episodic_buffer, \"add_entry\"):\n",
    "                self.memory.episodic_buffer.add_entry(context_traj, outcome)\n",
    "            elif hasattr(self.memory, \"add_entry\"):\n",
    "                self.memory.add_entry(context_traj, outcome)\n",
    "            # Optionally: update motifs if needed (usually not online, but up to you)\n",
    "            # if hasattr(self.memory, \"motif_bank\") and hasattr(self.memory.motif_bank, \"add_entry\"):\n",
    "            #     self.memory.motif_bank.add_entry(context_traj, outcome)\n",
    "        if self.memory is not None and hasattr(self.memory, 'get_last_attention'):\n",
    "            attn_weights = self.memory.get_last_attention()\n",
    "\n",
    "        # RND predictor update (only predictor trained)\n",
    "        if self.use_rnd:\n",
    "            obs_batch = torch.stack([torch.tensor(np.array(o), dtype=torch.float32, device=self.device) for o in trajectory])\n",
    "            rnd_loss = self.rnd(obs_batch).mean()\n",
    "            self.rnd_optimizer.zero_grad()\n",
    "            rnd_loss.backward()\n",
    "            self.rnd_optimizer.step()\n",
    "\n",
    "        return {\n",
    "            \"trajectory\": trajectory,\n",
    "            \"actions\": actions,\n",
    "            \"rewards\": rewards,\n",
    "            \"log_probs\": log_probs,\n",
    "            \"values\": values,\n",
    "            \"entropies\": entropies_ep,\n",
    "            \"aux_preds\": aux_preds_list,\n",
    "            \"aux_targets\": aux_targets_ep,\n",
    "            \"initial_cue\": initial_cue,\n",
    "            \"gate_history\": gate_history,\n",
    "            \"memory_size_history\": memory_size_history,\n",
    "            \"attn_weights\": attn_weights\n",
    "        }\n",
    "\n",
    "    def get_episodic_buffer(self):\n",
    "        episodic_buffer = None\n",
    "        if self.memory :\n",
    "            episodic_buffer = self.memory.episodic_buffer if hasattr(self.memory,\"episodic_buffer\") else  self.memory\n",
    "        return episodic_buffer\n",
    "\n",
    "        \n",
    "        \n",
    "    def learn(self, total_timesteps=2000, log_interval=100):\n",
    "        steps = 0\n",
    "        episodes = 0\n",
    "        all_returns = []\n",
    "        start_time = time.time()\n",
    "        aux_losses = []\n",
    "        unlock_early_stopping = len(self.episode_rewards)+self.early_stop_n_samples\n",
    "        while steps < total_timesteps:\n",
    "            try:\n",
    "                #if hasattr(sys, 'last_traceback'):  # Quick hack: set by IPython on error/stop\n",
    "                #    print(\"Interrupted in Jupyter (sys.last_traceback). Exiting.\")\n",
    "                #    break\n",
    "                episode = self.run_episode()\n",
    "                if self.reward_norm:\n",
    "                    self.reward_normalizer.update([r.item() for r in episode[\"rewards\"]])\n",
    "                    episode[\"rewards\"] = [\n",
    "                        torch.tensor(rn, dtype=torch.float32, device=self.device)\n",
    "                        for rn in self.reward_normalizer.normalize([r.item() for r in episode[\"rewards\"]])\n",
    "                    ]\n",
    "    \n",
    "                trajectory = episode[\"trajectory\"]\n",
    "                actions = episode[\"actions\"]\n",
    "                rewards = episode[\"rewards\"]\n",
    "                log_probs = episode[\"log_probs\"]\n",
    "                values = episode[\"values\"]\n",
    "                entropies_ep = episode[\"entropies\"]\n",
    "                aux_preds = episode[\"aux_preds\"]\n",
    "                aux_targets = episode[\"aux_targets\"]\n",
    "                T = len(rewards)\n",
    "                rewards_t = torch.stack(rewards)\n",
    "                values_t = torch.stack(values)\n",
    "                log_probs_t = torch.stack(log_probs)\n",
    "                actions_t = torch.stack(actions)\n",
    "                last_value = 0.0\n",
    "                advantages = compute_gae(rewards_t, values_t, gamma=self.gamma, lam=self.lam, last_value=last_value)\n",
    "                returns = advantages + values_t.detach()\n",
    "    \n",
    "                policy_loss = -(log_probs_t * advantages.detach()).sum()\n",
    "                value_loss = F.mse_loss(values_t, returns)\n",
    "                entropy_mean = torch.stack(entropies_ep).mean()\n",
    "                explained_var = compute_explained_variance(values_t, returns)\n",
    "    \n",
    "                # Auxiliary losses\n",
    "                aux_loss_total = torch.tensor(0.0, device=self.device)\n",
    "                aux_metrics_log = {}\n",
    "                if self.aux:\n",
    "                    for aux in self.aux_modules:\n",
    "                        preds = torch.stack([ap[aux.name] for ap in aux_preds])\n",
    "                        targets = torch.tensor(aux_targets[aux.name], device=self.device)\n",
    "                        if preds.dim() != targets.dim():\n",
    "                            targets = targets.squeeze(-1)\n",
    "                        loss = aux.aux_loss(preds, targets)\n",
    "                        aux_loss_total += loss\n",
    "                        metrics = aux.aux_metrics(preds, targets)\n",
    "                        aux_metrics_log[aux.name] = metrics\n",
    "                    aux_losses.append(aux_loss_total.item())\n",
    "    \n",
    "                # Memory usefullness (if enabled) =====\n",
    "                episodic_buffer = self.get_episodic_buffer()\n",
    "                if (\n",
    "                    self.memory_learn_retention\n",
    "                    and self.memory is not None\n",
    "                    and hasattr(episodic_buffer, 'get_last_attention')\n",
    "                    and episodic_buffer.last_attn is not None\n",
    "                    and len(episodic_buffer.usefulness_vec) == len(episodic_buffer.last_attn)\n",
    "                    and len(episodic_buffer.usefulness_vec) > 0\n",
    "                ):\n",
    "                    total_reward = sum([r.item() for r in rewards])\n",
    "                    if hasattr(self.memory,'episodic_buffer'):\n",
    "                        \n",
    "                        attn_tensor = torch.tensor(self.memory.episodic_buffer.last_attn, dtype=torch.float32, device=self.device)\n",
    "                        mem_loss = self.memory.episodic_buffer.usefulness_loss(attn_tensor, total_reward)\n",
    "                    else:\n",
    "                      \n",
    "                        attn_tensor = torch.tensor(self.memory.last_attn, dtype=torch.float32, device=self.device)\n",
    "                        mem_loss = self.memory.usefulness_loss(attn_tensor, total_reward)\n",
    "                else:\n",
    "                    mem_loss = torch.tensor(0.0, device=self.device)\n",
    "                    \n",
    "    \n",
    "                loss = (\n",
    "                    policy_loss \n",
    "                    + 0.5 * value_loss \n",
    "                    + 0.1 * aux_loss_total \n",
    "                    - self.ent_coef * entropy_mean\n",
    "                    + (self.memory_retention_coef * mem_loss if self.memory_learn_retention else 0.0)\n",
    "                )\n",
    "    \n",
    "                self.optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "    \n",
    "                total_reward = sum([r.item() for r in rewards])\n",
    "                self.episode_rewards.append(total_reward)\n",
    "                self.episode_lengths.append(T)\n",
    "                episodes += 1\n",
    "                steps += T\n",
    "\n",
    "                if self.early_stop and len(self.episode_rewards) >= unlock_early_stopping:\n",
    "                    mean_rew = np.mean(self.episode_rewards[-self.early_stop_n_samples:])\n",
    "                    std_rew = np.std(self.episode_rewards[-self.early_stop_n_samples:])\n",
    "                    if mean_rew >self.early_stop_mean_threshold and std_rew <= self.early_stop_std_threshold:\n",
    "                        mean_len = np.mean(self.episode_lengths[-log_interval:])\n",
    "                        elapsed = int(time.time() - start_time)\n",
    "                        ep_duration = elapsed/episodes\n",
    "                        table = [\n",
    "                                [\"Train duration\",f\"{elapsed}s\"],\n",
    "                                [\"Avg episode duration\",f\"{ep_duration:.2f}s\"],\n",
    "                                [\"Rolling ep rew mean\", f\"{mean_rew:.2f}\"],\n",
    "                                [\"Rolling ep rew std\",f\"{std_rew:.2f}\"],\n",
    "                                [\"Rolling ep length\",f\"{mean_len:.2f}\"],\n",
    "                                [\"N updates\", episodes]]\n",
    "                        \n",
    "                        print(tabulate(table ,tablefmt=\"rounded_outline\" , headers=[\"Early Stop\",\"\"]))\n",
    "                        return\n",
    "\n",
    "                if hasattr(self.memory, 'motif_bank') and hasattr(self, 'get_episodic_buffer'):\n",
    "                    if episodes > 0 and episodes % MINING_INTERVAL == 0:\n",
    "                        mine_motifs_from_buffer(\n",
    "                            self.get_episodic_buffer(),\n",
    "                            self.memory.motif_bank,\n",
    "                            motif_len=self.memory.motif_bank.motif_len,\n",
    "                            n_motifs=self.memory.motif_bank.n_motifs\n",
    "                        )\n",
    "                                # LOGGING (SB3-STYLE) =====================\n",
    "                if episodes % log_interval == 0 and self.verbose == 1:\n",
    "                    elapsed = int(time.time() - start_time)\n",
    "                    mean_rew = np.mean(self.episode_rewards[-log_interval:])\n",
    "                    std_rew = np.std(self.episode_rewards[-log_interval:])\n",
    "                    mean_len = np.mean(self.episode_lengths[-log_interval:])\n",
    "                \n",
    "                    fps = int(steps / (elapsed + 1e-8))\n",
    "                    adv_mean = advantages.mean().item()\n",
    "                    adv_std = advantages.std().item()\n",
    "                    mean_entropy = entropy_mean.item()\n",
    "                    mean_aux = np.mean(aux_losses[-log_interval:]) if aux_losses else 0.0\n",
    "                    stats = [{\n",
    "                        \"header\": \"rollout\",\n",
    "                        \"stats\": dict(\n",
    "                            ep_len_mean=mean_len,\n",
    "                            ep_rew_mean=mean_rew,\n",
    "                            ep_rew_std=std_rew,\n",
    "                            policy_entropy=mean_entropy,\n",
    "                            advantage_mean=adv_mean,\n",
    "                            advantage_std=adv_std,\n",
    "                            aux_loss_mean=mean_aux\n",
    "                        )}, {\n",
    "                        \"header\": \"time\",\n",
    "                        \"stats\": dict(\n",
    "                            fps=fps,\n",
    "                            episodes=episodes,\n",
    "                            time_elapsed=elapsed,\n",
    "                            total_timesteps=steps\n",
    "                        )}, {\n",
    "                        \"header\": \"train\",\n",
    "                        \"stats\": dict(\n",
    "                            loss=loss.item(),\n",
    "                            policy_loss=policy_loss.item(),\n",
    "                            value_loss=value_loss.item(),\n",
    "                            explained_variance=explained_var.item(),\n",
    "                            n_updates=episodes,\n",
    "                            progress=100 * steps / total_timesteps\n",
    "                        )}\n",
    "                    ]\n",
    "                    if len(aux_metrics_log.items()) > 0:\n",
    "                        aux_stats = {\n",
    "                            \"header\": \"aux_train\",\n",
    "                            \"stats\": {}\n",
    "                        }\n",
    "                        for aux_name, metrics in aux_metrics_log.items():\n",
    "                            for k, v in metrics.items():\n",
    "                                aux_stats[\"stats\"][f\"aux_{aux_name}_{k}\"] = v\n",
    "                        stats.append(aux_stats)\n",
    "                    if self.use_rnd:\n",
    "                        mean_rnd_bonus = np.mean([self.rnd(torch.tensor(np.array(o), dtype=torch.float32, device=self.device).unsqueeze(0)).item() for o in trajectory])\n",
    "                        stats.append({\n",
    "                            \"header\": \"rnd_net_dist\",\n",
    "                            \"stats\": {\"mean_rnd_bonus\": mean_rnd_bonus}\n",
    "                        })\n",
    "                    if self.memory_learn_retention:\n",
    "                        stats.append({\n",
    "                            \"header\": \"memory\",\n",
    "                            \"stats\": {\n",
    "                                \"usefulness_loss\": mem_loss.item()}\n",
    "                        })\n",
    "                    \n",
    "                    print_sb3_style_log_box(stats)\n",
    "                    \n",
    "            except KeyboardInterrupt:\n",
    "                print(\"\\n[Stopped by user] Gracefully exiting training loop...\")\n",
    "                return\n",
    "            \n",
    "        if self.verbose == 1:\n",
    "            print(f\"Training complete. Total episodes: {episodes}, total steps: {steps}\")\n",
    "\n",
    "    def predict(self, obs, deterministic=False, done=False, reward=0.0):\n",
    "        \"\"\"\n",
    "        Computes action for a given observation, with support for memory context.\n",
    "\n",
    "        Args:\n",
    "            obs (np.ndarray): Environment observation.\n",
    "            deterministic (bool): Use argmax instead of sampling.\n",
    "            done (bool): If episode ended, will reset trajectory buffer.\n",
    "            reward (float): Last received reward (for memory context).\n",
    "\n",
    "        Returns:\n",
    "            int: Action index.\n",
    "        \"\"\"\n",
    "        obs_t = torch.tensor(obs, dtype=torch.float32, device=self.device)\n",
    "        # Track full trajectory for memory\n",
    "        if not hasattr(self, \"trajectory_buffer\") or self.trajectory_buffer is None:\n",
    "            self.trajectory_buffer = []\n",
    "        if len(self.trajectory_buffer) == 0:\n",
    "            self.trajectory_buffer.append((obs_t.cpu().numpy(), 0, 0.0))\n",
    "        else:\n",
    "            last_action = self.last_action if hasattr(self, \"last_action\") else 0\n",
    "            last_reward = self.last_reward if hasattr(self, \"last_reward\") else 0.0\n",
    "            self.trajectory_buffer.append((obs_t.cpu().numpy(), last_action, last_reward))\n",
    "        context_traj = self.trajectory_buffer.copy()\n",
    "        actions_int = [a for _, a, _ in context_traj]\n",
    "        rewards_float = [r for _, _, r in context_traj]\n",
    "        obs_stack = torch.stack([torch.tensor(o, dtype=torch.float32, device=self.device) for o, _, _ in context_traj])\n",
    "        logits, _, _ = self.policy(\n",
    "            obs_stack, obs_t,\n",
    "            actions=torch.tensor(actions_int, device=self.device),\n",
    "            rewards=torch.tensor(rewards_float, device=self.device)\n",
    "        )\n",
    "        if deterministic:\n",
    "            action = torch.argmax(logits).item()\n",
    "        else:\n",
    "            dist = Categorical(logits=logits)\n",
    "            action = dist.sample().item()\n",
    "        self.last_action = action\n",
    "        self.last_reward = reward\n",
    "        if done:\n",
    "            self.trajectory_buffer = []\n",
    "        return action\n",
    "\n",
    "    def save(self, path=\"memoryppo.pt\"):\n",
    "        \"\"\"Save policy weights to file.\"\"\"\n",
    "        torch.save(self.policy.state_dict(), path)\n",
    "\n",
    "    def load(self, path=\"memoryppo.pt\"):\n",
    "        \"\"\"Load policy weights from file.\"\"\"\n",
    "        self.policy.load_state_dict(torch.load(path, map_location=self.device))\n",
    "\n",
    "    def evaluate(self, n_episodes=10, deterministic=False, verbose=True):\n",
    "        \"\"\"\n",
    "        Evaluates policy over several episodes, reporting mean/std return.\n",
    "\n",
    "        Args:\n",
    "            n_episodes (int): Number of test episodes.\n",
    "            deterministic (bool): Use argmax instead of sampling.\n",
    "            verbose (bool): Print results to console.\n",
    "\n",
    "        Returns:\n",
    "            mean_return (float): Average reward.\n",
    "            std_return (float): Std deviation of rewards.\n",
    "        \"\"\"\n",
    "        returns = []\n",
    "        for _ in range(n_episodes):\n",
    "            obs, _ = self.env.reset()\n",
    "            self.trajectory_buffer = []\n",
    "            done = False\n",
    "            total_reward = 0.0\n",
    "            last_reward = 0.0\n",
    "            while not done:\n",
    "                action = self.predict(obs, deterministic=deterministic, reward=last_reward)\n",
    "                obs, reward, done, _, _ = self.env.step(action)\n",
    "                total_reward += reward\n",
    "                last_reward = reward\n",
    "                if done:\n",
    "                    self.trajectory_buffer = []\n",
    "            returns.append(total_reward)\n",
    "        mean_return = np.mean(returns)\n",
    "        std_return = np.std(returns)\n",
    "        if verbose:\n",
    "            print(f\"Evaluation over {n_episodes} episodes: mean return {mean_return:.2f}, std {std_return:.2f}\")\n",
    "        return mean_return, std_return\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "61f79e99-6906-47db-bc28-f55f181db6eb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# ──────────────────────────────────────────────────────────────\n",
    "# Example training loop\n",
    "# ──────────────────────────────────────────────────────────────\n",
    "\n",
    "# SETUP ===================================\n",
    "DELAY = 4\n",
    "MEM_DIM = 32\n",
    "N_EPISODES = 2500\n",
    "N_MEMORIES = 32\n",
    "\n",
    "AGENT_KWARGS = dict(\n",
    "    device=\"cpu\",\n",
    "    verbose=0,\n",
    "    lam=0.95, \n",
    "    gamma=0.99, \n",
    "    ent_coef=0.01,\n",
    "    learning_rate=1e-3, \n",
    "    \n",
    ")\n",
    "MEMORY_AGENT_KWARGS=dict(\n",
    "    her=False,\n",
    "    reward_norm=False,\n",
    "    aux_modules=None,\n",
    "    \n",
    "    intrinsic_expl=False,\n",
    "    intrinsic_eta=0.01,\n",
    "    \n",
    "    use_rnd=False, \n",
    "    rnd_emb_dim=32, \n",
    "    rnd_lr=1e-3,\n",
    ")\n",
    "\n",
    "# HELPERS =================================\n",
    "def total_timesteps(delay,n_episodes):\n",
    "    return delay * n_episodes\n",
    "\n",
    "# ENVIRONMENT =============================\n",
    "env = MemoryTaskEnv(delay=DELAY, difficulty=0)\n",
    "\n",
    "# MEMORY BUFFER ===========================\n",
    "episodic_buffer = StrategicMemoryBuffer(\n",
    "    obs_dim=env.observation_space.shape[0],\n",
    "    action_dim=1,\n",
    "    mem_dim=MEM_DIM,\n",
    "    max_entries=N_MEMORIES,\n",
    "    device=\"cpu\"\n",
    ")\n",
    "motif_bank = MotifMemoryBank(\n",
    "    obs_dim=env.observation_space.shape[0],\n",
    "    action_dim=1,\n",
    "    mem_dim=MEM_DIM,\n",
    "    n_motifs=32,\n",
    "    motif_len=4,\n",
    "    device=\"cpu\"\n",
    ")\n",
    "combined_memory = CombinedMemoryModule(episodic_buffer, motif_bank)\n",
    "\n",
    "\n",
    "# POLICY NETWORK (use class) ==============\n",
    "policy = StrategicCombinedMemoryPolicy\n",
    "\n",
    "\n",
    "# AGENT SETUP =============================\n",
    "agent = TraceRL(\n",
    "    policy_class=policy,\n",
    "    env=env,\n",
    "    memory=combined_memory,\n",
    "    memory_learn_retention=True,    \n",
    "    memory_retention_coef=0.01,   \n",
    "    # aux_modules=aux_modules,  \n",
    "    device=\"cpu\",\n",
    "    verbose=1,\n",
    "    lam=0.95, \n",
    "    gamma=0.99, \n",
    "    ent_coef=0.01,\n",
    "    learning_rate=1e-3, \n",
    "    \n",
    "    **MEMORY_AGENT_KWARGS\n",
    ")\n",
    "\n",
    "# TRAIN THE AGENT =========================\n",
    "#agent.learn(\n",
    "#    total_timesteps=total_timesteps(DELAY, 1000),\n",
    "#    log_interval=50\n",
    "#)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b90e42e9-f342-41ce-b60b-c38c7168a02f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "055292e2-402b-4132-818b-4d9b73105628",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d023b37-8f6a-4ced-961e-a66f23a1f377",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Training with delay=2 ---\n",
      "╭──────────────────────┬───────╮\n",
      "│ Early Stop           │       │\n",
      "├──────────────────────┼───────┤\n",
      "│ Train duration       │ 7s    │\n",
      "│ Avg episode duration │ 0.01s │\n",
      "│ Rolling ep rew mean  │ 1.00  │\n",
      "│ Rolling ep rew std   │ 0.00  │\n",
      "│ Rolling ep length    │ 2.00  │\n",
      "│ N updates            │ 553   │\n",
      "╰──────────────────────┴───────╯\n",
      "╭──────────────┬────╮\n",
      "│ Evaluation   │    │\n",
      "├──────────────┼────┤\n",
      "│ Avg reward   │  1 │\n",
      "│ Std reward   │  0 │\n",
      "╰──────────────┴────╯\n",
      "\n",
      "--- Training with delay=4 ---\n",
      "╭──────────────────────┬───────╮\n",
      "│ Early Stop           │       │\n",
      "├──────────────────────┼───────┤\n",
      "│ Train duration       │ 13s   │\n",
      "│ Avg episode duration │ 0.02s │\n",
      "│ Rolling ep rew mean  │ 1.00  │\n",
      "│ Rolling ep rew std   │ 0.00  │\n",
      "│ Rolling ep length    │ 4.00  │\n",
      "│ N updates            │ 530   │\n",
      "╰──────────────────────┴───────╯\n",
      "╭──────────────┬────╮\n",
      "│ Evaluation   │    │\n",
      "├──────────────┼────┤\n",
      "│ Avg reward   │  1 │\n",
      "│ Std reward   │  0 │\n",
      "╰──────────────┴────╯\n",
      "\n",
      "--- Training with delay=8 ---\n",
      "╭──────────────────────┬───────╮\n",
      "│ Early Stop           │       │\n",
      "├──────────────────────┼───────┤\n",
      "│ Train duration       │ 4s    │\n",
      "│ Avg episode duration │ 0.04s │\n",
      "│ Rolling ep rew mean  │ 1.00  │\n",
      "│ Rolling ep rew std   │ 0.00  │\n",
      "│ Rolling ep length    │ 8.00  │\n",
      "│ N updates            │ 100   │\n",
      "╰──────────────────────┴───────╯\n",
      "╭──────────────┬────╮\n",
      "│ Evaluation   │    │\n",
      "├──────────────┼────┤\n",
      "│ Avg reward   │  1 │\n",
      "│ Std reward   │  0 │\n",
      "╰──────────────┴────╯\n",
      "\n",
      "--- Training with delay=16 ---\n"
     ]
    }
   ],
   "source": [
    "from benchmark import AgentPerformanceBenchmark\n",
    "from tabulate import tabulate\n",
    "env = MemoryTaskEnv(delay=4, difficulty=0)\n",
    "\n",
    "# MEMORY BUFFER ===========================\n",
    "memory = StrategicMemoryBuffer(\n",
    "        obs_dim=env.observation_space.shape[0],\n",
    "        action_dim=1,          # For Discrete(2)\n",
    "        mem_dim=MEM_DIM,\n",
    "        max_entries=N_MEMORIES,\n",
    "        device=\"cpu\"\n",
    "    )\n",
    "    \n",
    "# POLICY NETWORK (use class) ==============\n",
    "#policy = StrategicMemoryTransformerPolicy\n",
    "policy = StrategicCombinedMemoryPolicy\n",
    "agent = TraceRL(\n",
    "        policy_class=policy,\n",
    "        env=env,\n",
    "        memory=memory,\n",
    "        memory_learn_retention=True,    \n",
    "        memory_retention_coef=0.01,   \n",
    "        # aux_modules=aux_modules,  \n",
    "        device=\"cpu\",\n",
    "        verbose=0,\n",
    "        lam=0.95, \n",
    "        gamma=0.99, \n",
    "        ent_coef=0.01,\n",
    "        learning_rate=1e-3, \n",
    "        \n",
    "        **MEMORY_AGENT_KWARGS\n",
    "    )\n",
    "curriculum = [2, 4, 8, 16,32,64,128,256]\n",
    "for delay in curriculum:\n",
    "    agent.env.delay = delay\n",
    "    #agent.get_episodic_buffer().reset()\n",
    "    print(f\"\\n--- Training with delay={delay} ---\")\n",
    "    agent.learn(total_timesteps=total_timesteps(delay, 100000), log_interval=50)\n",
    "    \n",
    "    benchmark = AgentPerformanceBenchmark(dict(delay=delay, n_train_episodes=2000, total_timesteps=1_000_000, difficulty=0, mode_name=\"EASY\", verbose=0, eval_base=True),)\n",
    "    e_r, e_s = benchmark.evaluate(agent,'motif')\n",
    "    table = [[\"Avg reward\",e_r],[\"Std reward\",e_s]]\n",
    "    print(tabulate(table, headers=[\"Evaluation\",\"\"], tablefmt=\"rounded_outline\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ba97dc9-5d8a-4b2e-9583-3aeaa3d829c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from benchmark import AgentPerformanceBenchmark\n",
    "\n",
    "env = MemoryTaskEnv(delay=delay, difficulty=0)\n",
    "\n",
    "# MEMORY BUFFER ===========================\n",
    "memory = StrategicMemoryBuffer(\n",
    "        obs_dim=env.observation_space.shape[0],\n",
    "        action_dim=1,          # For Discrete(2)\n",
    "        mem_dim=MEM_DIM,\n",
    "        max_entries=N_MEMORIES,\n",
    "        device=\"cpu\"\n",
    "    )\n",
    "    \n",
    "# POLICY NETWORK (use class) ==============\n",
    "#policy = StrategicMemoryTransformerPolicy\n",
    "policy = StrategicCombinedMemoryPolicy\n",
    "agent = TraceRL(\n",
    "        policy_class=policy,\n",
    "        env=env,\n",
    "        memory=memory,\n",
    "        memory_learn_retention=True,    \n",
    "        memory_retention_coef=0.01,   \n",
    "        # aux_modules=aux_modules,  \n",
    "        device=\"cpu\",\n",
    "        verbose=0,\n",
    "        lam=0.95, \n",
    "        gamma=0.99, \n",
    "        ent_coef=0.01,\n",
    "        learning_rate=1e-3, \n",
    "        \n",
    "        **MEMORY_AGENT_KWARGS\n",
    "    )\n",
    "curriculum = [2, 4, 8, 16,32,64,128,256]\n",
    "for delay in curriculum:\n",
    "    agent.env.delay = delay\n",
    "    #agent.get_episodic_buffer().reset()\n",
    "    print(f\"\\n--- Training with delay={delay} ---\")\n",
    "    agent.learn(total_timesteps=total_timesteps(delay, 100000), log_interval=50)\n",
    "    \n",
    "    benchmark = AgentPerformanceBenchmark(dict(delay=delay, n_train_episodes=2000, total_timesteps=1_000_000, difficulty=0, mode_name=\"EASY\", verbose=0, eval_base=True),)\n",
    "    e_r, e_s = benchmark.evaluate(agent,'motif')\n",
    "    table = [[\"Avg reward\",e_r],[\"Std reward\",e_s]]\n",
    "    print(tabulate(table, headers=[\"Evaluation\",\"\"], tablefmt=\"rounded_outline\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "129e91d8-da40-4faa-9b22-af9c3355837a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:trading]",
   "language": "python",
   "name": "conda-env-trading-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
