{
 "cells": [
  {
   "cell_type": "raw",
   "id": "f6bdabb3-80ba-4a08-a3ae-b74a6be82347",
   "metadata": {},
   "source": [
    "\"\"\"\n",
    "Memory-Augmented PPO Benchmark\n",
    "Author: Pihh \n",
    "Description:\n",
    "    Benchmarks a Memory Transformer PPO agent vs a Stateless PPO and a LSTM PPOagent \n",
    "    on a synthetic memory task with configurable delay.\n",
    "\n",
    "Structure:\n",
    "- Custom Gym Environment: MemoryTaskEnv\n",
    "- Agents: \n",
    "    - MemoryTransformerAgent (sequence-aware)\n",
    "    - RecurrentPPO (baseline/sequence-aware)\n",
    "    - PPO (baseline/stateless)\n",
    "- Training & Evaluation loops\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e685052c-04e3-4fb0-81d5-7c8384a5e826",
   "metadata": {},
   "source": [
    "# StrategicMemoryBuffer with Trainable Retention\n",
    "\n",
    "Key ideas:\n",
    "* Each memory entry now stores a learned “usefulness” score (a scalar).\n",
    "\n",
    "* When the buffer is full, discard the entry with the lowest usefulness, not just the oldest.\n",
    "\n",
    "* The usefulness score is updated as the agent interacts:\n",
    "\n",
    "* For now, let’s make it a simple trainable parameter for each entry, but you can make it a neural network that depends on the context, attention frequency, outcome, or reward.\n",
    "\n",
    "* Optionally, I can update usefulness every time the memory is attended to (e.g., with a moving average or a learned auxiliary head)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c993b529-5977-4cf0-ad70-3f1dc29bbaac",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from environments import MemoryTaskEnv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ee6acb67-f80f-4395-b9d5-1df31beeccd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.distributions import Categorical\n",
    "import torch.nn.functional as F\n",
    "import time\n",
    "from collections import defaultdict\n",
    "\n",
    "# ──────────────────────────────────────────────────────────────\n",
    "# 2. Reward Normalizer\n",
    "# ──────────────────────────────────────────────────────────────\n",
    "\n",
    "class RewardNormalizer:\n",
    "    \"\"\"\n",
    "    Running mean/variance normalizer for rewards.\n",
    "    Stabilizes learning by normalizing returns online.\n",
    "    \"\"\"\n",
    "    def __init__(self, epsilon=1e-8):\n",
    "        self.mean = 0.0\n",
    "        self.var = 1.0\n",
    "        self.count = 1e-4  # prevents division by zero\n",
    "        self.epsilon = epsilon\n",
    "\n",
    "    def update(self, rewards):\n",
    "        \"\"\"\n",
    "        Updates running statistics given a batch of rewards.\n",
    "        Args:\n",
    "            rewards (array-like): list or np.ndarray of rewards\n",
    "        \"\"\"\n",
    "        rewards = np.array(rewards)\n",
    "        batch_mean = rewards.mean()\n",
    "        batch_var = rewards.var()\n",
    "        batch_count = len(rewards)\n",
    "        # Online update of mean/var\n",
    "        self.mean = (self.mean * self.count + batch_mean * batch_count) / (self.count + batch_count)\n",
    "        self.var = (self.var * self.count + batch_var * batch_count) / (self.count + batch_count)\n",
    "        self.count += batch_count\n",
    "\n",
    "    def normalize(self, rewards):\n",
    "        \"\"\"\n",
    "        Normalizes a batch of rewards based on running statistics.\n",
    "        Args:\n",
    "            rewards (array-like): list or np.ndarray of rewards\n",
    "        Returns:\n",
    "            List of normalized rewards\n",
    "        \"\"\"\n",
    "        rewards = np.array(rewards)\n",
    "        return ((rewards - self.mean) / (np.sqrt(self.var) + self.epsilon)).tolist()\n",
    "\n",
    "# ──────────────────────────────────────────────────────────────\n",
    "# 3. State Counter for Intrinsic Reward (Exploration Bonus)\n",
    "# ──────────────────────────────────────────────────────────────\n",
    "\n",
    "class StateCounter:\n",
    "    \"\"\"\n",
    "    Counts state visitations for intrinsic motivation (exploration).\n",
    "    Implements simple count-based exploration bonus.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self.counts = defaultdict(int)\n",
    "\n",
    "    def count(self, obs):\n",
    "        \"\"\"\n",
    "        Increments and returns the visit count for a discretized observation.\n",
    "        Args:\n",
    "            obs (np.ndarray): observation\n",
    "        Returns:\n",
    "            int: visit count\n",
    "        \"\"\"\n",
    "        key = tuple(np.round(obs, 2))  # discretize for generalization\n",
    "        self.counts[key] += 1\n",
    "        return self.counts[key]\n",
    "\n",
    "    def intrinsic_reward(self, obs):\n",
    "        \"\"\"\n",
    "        Returns intrinsic reward: inversely proportional to sqrt of count.\n",
    "        Args:\n",
    "            obs (np.ndarray): observation\n",
    "        Returns:\n",
    "            float: exploration bonus\n",
    "        \"\"\"\n",
    "        c = self.count(obs)\n",
    "        return 1.0 / np.sqrt(c)\n",
    "\n",
    "# ──────────────────────────────────────────────────────────────\n",
    "# 4. Generalized Advantage Estimation (GAE) and Explained Variance\n",
    "# ──────────────────────────────────────────────────────────────\n",
    "\n",
    "def compute_explained_variance(y_pred, y_true):\n",
    "    \"\"\"\n",
    "    Computes explained variance between prediction and ground-truth.\n",
    "    Used for value function diagnostics in RL.\n",
    "    \"\"\"\n",
    "    var_y = torch.var(y_true)\n",
    "    if var_y == 0:\n",
    "        return torch.tensor(0.0)\n",
    "    return 1 - torch.var(y_true - y_pred) / (var_y + 1e-8)\n",
    "\n",
    "def compute_gae(rewards, values, gamma=0.99, lam=0.95, last_value=0.0):\n",
    "    \"\"\"\n",
    "    Compute Generalized Advantage Estimation (GAE) for a trajectory.\n",
    "    Args:\n",
    "        rewards (torch.Tensor): reward sequence [T]\n",
    "        values (torch.Tensor): value sequence [T]\n",
    "        gamma (float): discount factor\n",
    "        lam (float): GAE lambda\n",
    "        last_value (float): bootstrap value after final state\n",
    "    Returns:\n",
    "        torch.Tensor: advantage sequence [T]\n",
    "    \"\"\"\n",
    "    T = len(rewards)\n",
    "    advantages = torch.zeros(T, dtype=torch.float32, device=values.device)\n",
    "    gae = 0.0\n",
    "    # concatenate last value for bootstrap\n",
    "    values_ext = torch.cat([values, torch.tensor([last_value], dtype=torch.float32, device=values.device)])\n",
    "    for t in reversed(range(T)):\n",
    "        delta = rewards[t] + gamma * values_ext[t + 1] - values_ext[t]\n",
    "        gae = delta + gamma * lam * gae\n",
    "        advantages[t] = gae\n",
    "    return advantages\n",
    "\n",
    "# ──────────────────────────────────────────────────────────────\n",
    "# 5. Memory Transformer Policy with Auxiliary Head\n",
    "# ──────────────────────────────────────────────────────────────\n",
    "\n",
    "class MemoryTransformerPolicy(nn.Module):\n",
    "    \"\"\"\n",
    "    MemoryTransformerPolicy\n",
    "    ----------------------\n",
    "    Transformer-based policy for sequence decision tasks.\n",
    "    - Processes full trajectory as input (not just current state)\n",
    "    - Supports auxiliary head for supervised tasks (e.g., cue recall)\n",
    "\n",
    "    Args:\n",
    "        obs_dim (int): Dimension of observation vector\n",
    "        mem_dim (int): Transformer embedding size\n",
    "        nhead (int): Number of attention heads\n",
    "    \"\"\"\n",
    "    def __init__(self, obs_dim, mem_dim=32, nhead=4):\n",
    "        super().__init__()\n",
    "        self.mem_dim = mem_dim\n",
    "        self.embed = nn.Linear(obs_dim, mem_dim)\n",
    "        self.pos_embed = nn.Embedding(256, mem_dim)\n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=mem_dim, nhead=nhead, batch_first=True)\n",
    "        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=1)\n",
    "        self.policy_head = nn.Linear(mem_dim, 2)   # Action logits\n",
    "        self.value_head = nn.Linear(mem_dim, 1)    # Value prediction\n",
    "        self.aux_head = nn.Linear(mem_dim, 2)      # Auxiliary: predict initial cue\n",
    "\n",
    "    def forward(self, trajectory):\n",
    "        \"\"\"\n",
    "        Forward pass: processes trajectory and outputs policy, value, and aux predictions.\n",
    "        Args:\n",
    "            trajectory (torch.Tensor): [T, obs_dim] trajectory\n",
    "        Returns:\n",
    "            logits (torch.Tensor): [2,] action logits\n",
    "            value (torch.Tensor): [1,] state value\n",
    "            aux_pred (torch.Tensor): [2,] auxiliary logits (for cue prediction)\n",
    "        \"\"\"\n",
    "        T = trajectory.shape[0]\n",
    "        x = self.embed(trajectory)\n",
    "        pos = torch.arange(T, device=trajectory.device)\n",
    "        x = x + self.pos_embed(pos)\n",
    "        x = x.unsqueeze(0)  # [1, T, mem_dim]\n",
    "        x = self.transformer(x)\n",
    "        feat = x[0, -1]  # use final state embedding for decision\n",
    "        logits = self.policy_head(feat)\n",
    "        value = self.value_head(feat)\n",
    "        aux_pred = self.aux_head(feat)\n",
    "        return logits, value.squeeze(-1), aux_pred\n",
    "\n",
    "# ──────────────────────────────────────────────────────────────\n",
    "# 6. HER-enabled MemoryPPO Trainer\n",
    "# ──────────────────────────────────────────────────────────────\n",
    "\n",
    "class MemoryPPO:\n",
    "    \"\"\"\n",
    "    MemoryPPO\n",
    "    ---------\n",
    "    PPO trainer for memory-based RL tasks.\n",
    "    - Supports Hindsight Experience Replay (HER)\n",
    "    - Reward normalization, auxiliary losses, intrinsic/exploration bonuses\n",
    "    - Sequence-based trajectory modeling\n",
    "\n",
    "    Args:\n",
    "        policy_class: policy network class\n",
    "        env: Gymnasium environment\n",
    "        device: PyTorch device\n",
    "        (other RL hyperparameters)\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self, \n",
    "        policy_class, \n",
    "        env, \n",
    "        verbose=0,\n",
    "        learning_rate=1e-3, \n",
    "        gamma=0.99, \n",
    "        lam=0.95, \n",
    "        device=\"cpu\",\n",
    "        her=True,                 # Enable HER\n",
    "        reward_norm=False,         # Enable reward normalization\n",
    "        aux=True,                 # Enable auxiliary loss\n",
    "        intrinsic_expl=True,      # Enable intrinsic exploration\n",
    "        intrinsic_eta=0.05,        # Intrinsic bonus multiplier\n",
    "        ent_coef=0.01\n",
    "    ):\n",
    "        self.env = env\n",
    "        self.device = torch.device(device)\n",
    "        self.gamma = gamma\n",
    "        self.lam = lam\n",
    "        self.ent_coef = ent_coef\n",
    "        self.verbose = verbose\n",
    "        self.policy = policy_class(obs_dim=env.observation_space.shape[0]).to(self.device)\n",
    "        self.policy = torch.jit.script(self.policy)\n",
    "        self.optimizer = optim.Adam(self.policy.parameters(), lr=learning_rate)\n",
    "        self.training_steps = 0\n",
    "        self.episode_rewards = []\n",
    "        self.episode_lengths = []\n",
    "        self.her = her\n",
    "        self.reward_norm = reward_norm\n",
    "        self.aux = aux\n",
    "        self.intrinsic_expl = intrinsic_expl\n",
    "        self.intrinsic_eta = intrinsic_eta\n",
    "        self.reward_normalizer = RewardNormalizer()\n",
    "        self.state_counter = StateCounter()\n",
    "        self.trajectory = []\n",
    "    \n",
    "    def reset_trajectory(self):\n",
    "        \"\"\"\n",
    "        Resets internal trajectory buffer after each episode or reset.\n",
    "        \"\"\"\n",
    "        self.trajectory = []\n",
    "        \n",
    "    def run_episode(self, her_target=None):\n",
    "        \"\"\"\n",
    "        Executes a full episode in the environment.\n",
    "        Supports HER by relabelling the initial cue if provided.\n",
    "        Returns trajectory data for training.\n",
    "        \"\"\"\n",
    "        obs, _ = self.env.reset()\n",
    "        if her_target is not None:\n",
    "            obs[0] = her_target  # Relabel initial cue with HER goal\n",
    "\n",
    "        done = False\n",
    "        trajectory = []\n",
    "        rewards = []\n",
    "        actions = []\n",
    "        log_probs = []\n",
    "        values = []\n",
    "        entropies_ep = []\n",
    "        aux_preds = []\n",
    "        t = 0\n",
    "\n",
    "        initial_cue = int(obs[0])  # Ground-truth cue for auxiliary prediction\n",
    "\n",
    "        while not done:\n",
    "            obs_t = torch.tensor(obs, dtype=torch.float32, device=self.device)\n",
    "            trajectory.append(obs_t)\n",
    "            traj = torch.stack(trajectory)\n",
    "            logits, value, aux_pred = self.policy(traj)\n",
    "            dist = Categorical(logits=logits)\n",
    "            action = dist.sample()\n",
    "            log_prob = dist.log_prob(action)\n",
    "            entropy = dist.entropy()\n",
    "            obs, reward, done, _, _ = self.env.step(action.item())\n",
    "            # Intrinsic (exploration) reward\n",
    "            if self.intrinsic_expl:\n",
    "                reward += self.intrinsic_eta * self.state_counter.intrinsic_reward(obs)\n",
    "            actions.append(action)\n",
    "            log_probs.append(log_prob)\n",
    "            rewards.append(torch.tensor(reward, dtype=torch.float32, device=self.device))\n",
    "            values.append(value)\n",
    "            entropies_ep.append(entropy)\n",
    "            aux_preds.append(aux_pred)\n",
    "            t += 1\n",
    "        return {\n",
    "            \"trajectory\": trajectory,\n",
    "            \"actions\": actions,\n",
    "            \"rewards\": rewards,\n",
    "            \"log_probs\": log_probs,\n",
    "            \"values\": values,\n",
    "            \"entropies\": entropies_ep,\n",
    "            \"aux_preds\": aux_preds,\n",
    "            \"initial_cue\": initial_cue\n",
    "        }\n",
    "\n",
    "    def learn(self, total_timesteps=2000, log_interval=100):\n",
    "        \"\"\"\n",
    "        Main training loop for PPO.\n",
    "        Collects rollouts, computes losses, performs optimization, and logs metrics.\n",
    "        Supports HER, reward normalization, auxiliary loss, and intrinsic bonuses.\n",
    "        \"\"\"\n",
    "        steps = 0\n",
    "        episodes = 0\n",
    "        all_returns = []\n",
    "        start_time = time.time()\n",
    "        aux_losses = []\n",
    "\n",
    "        while steps < total_timesteps:\n",
    "            # ---- Run normal episode ----\n",
    "            episode = self.run_episode()\n",
    "            # ---- Reward normalization ----\n",
    "            if self.reward_norm:\n",
    "                self.reward_normalizer.update([r.item() for r in episode[\"rewards\"]])\n",
    "                episode[\"rewards\"] = [torch.tensor(rn, dtype=torch.float32, device=self.device)\n",
    "                                      for rn in self.reward_normalizer.normalize([r.item() for r in episode[\"rewards\"]])]\n",
    "\n",
    "            # ---- HER relabelling ----\n",
    "            if self.her:\n",
    "                her_target = int(episode[\"actions\"][-1].item())\n",
    "                her_episode = self.run_episode(her_target=her_target)\n",
    "                for k in [\"trajectory\", \"actions\", \"rewards\", \"log_probs\", \"values\", \"entropies\", \"aux_preds\"]:\n",
    "                    episode[k] += her_episode[k]\n",
    "                initial_cue = [episode[\"initial_cue\"], her_target]\n",
    "            else:\n",
    "                initial_cue = [episode[\"initial_cue\"]]\n",
    "\n",
    "            # ---- Batchify for loss ----\n",
    "            trajectory = episode[\"trajectory\"]\n",
    "            actions = episode[\"actions\"]\n",
    "            rewards = episode[\"rewards\"]\n",
    "            log_probs = episode[\"log_probs\"]\n",
    "            values = episode[\"values\"]\n",
    "            entropies_ep = episode[\"entropies\"]\n",
    "            aux_preds = episode[\"aux_preds\"]\n",
    "            T = len(rewards)\n",
    "\n",
    "            rewards_t = torch.stack(rewards)\n",
    "            values_t = torch.stack(values)\n",
    "            log_probs_t = torch.stack(log_probs)\n",
    "            actions_t = torch.stack(actions)\n",
    "            aux_preds_t = torch.stack(aux_preds)\n",
    "            last_value = 0.0\n",
    "            advantages = compute_gae(rewards_t, values_t, gamma=self.gamma, lam=self.lam, last_value=last_value)\n",
    "            returns = advantages + values_t.detach()\n",
    "\n",
    "            # ---- Losses ----\n",
    "            policy_loss = -(log_probs_t * advantages.detach()).sum()\n",
    "            value_loss = F.mse_loss(values_t, returns)\n",
    "            entropy_mean = torch.stack(entropies_ep).mean()\n",
    "            explained_var = compute_explained_variance(values_t, returns)\n",
    "\n",
    "            # Auxiliary (supervised) loss for cue recall\n",
    "            aux_loss = torch.tensor(0.0, device=self.device)\n",
    "            if self.aux:\n",
    "                if self.her:\n",
    "                    cues = torch.tensor(initial_cue, dtype=torch.long, device=self.device)       # [2]\n",
    "                    aux_preds_to_use = torch.stack([aux_preds_t[0], aux_preds_t[T // 2]])        # [2,2]\n",
    "                else:\n",
    "                    cues = torch.tensor([initial_cue[0]], dtype=torch.long, device=self.device)  # [1]\n",
    "                    aux_preds_to_use = aux_preds_t[0].unsqueeze(0)                               # [1,2]\n",
    "                assert aux_preds_to_use.shape[0] == cues.shape[0], \\\n",
    "                    f\"Shape mismatch: preds {aux_preds_to_use.shape}, cues {cues.shape}\"\n",
    "                aux_loss = F.cross_entropy(aux_preds_to_use, cues)\n",
    "                aux_losses.append(aux_loss.item())\n",
    "\n",
    "            # ---- Total loss ----\n",
    "            loss = policy_loss + 0.5 * value_loss + 0.1 * aux_loss - self.ent_coef * entropy_mean\n",
    "\n",
    "            self.optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "\n",
    "            total_reward = sum([r.item() for r in rewards])\n",
    "            self.episode_rewards.append(total_reward)\n",
    "            self.episode_lengths.append(T)\n",
    "            episodes += 1\n",
    "            steps += T\n",
    "\n",
    "            # SB3-style logging\n",
    "            if episodes % log_interval == 0 and self.verbose == 1:\n",
    "                elapsed = time.time() - start_time\n",
    "                mean_rew = np.mean(self.episode_rewards[-log_interval:])\n",
    "                mean_len = np.mean(self.episode_lengths[-log_interval:])\n",
    "                fps = int(steps / (elapsed + 1e-8))\n",
    "                adv_mean = advantages.mean().item()\n",
    "                adv_std = advantages.std().item()\n",
    "                mean_entropy = entropy_mean.item()\n",
    "                mean_aux = np.mean(aux_losses[-log_interval:]) if aux_losses else 0.0\n",
    "                print(\"-\" * 40)\n",
    "                print(f\"| rollout/               |\")\n",
    "                print(f\"|    ep_len_mean         | {mean_len:8.2f}\")\n",
    "                print(f\"|    ep_rew_mean         | {mean_rew:8.2f}\")\n",
    "                print(f\"|    policy_entropy      | {mean_entropy:8.3f}\")\n",
    "                print(f\"|    advantage_mean      | {adv_mean:8.3f}\")\n",
    "                print(f\"|    advantage_std       | {adv_std:8.3f}\")\n",
    "                print(f\"|    aux_loss_mean       | {mean_aux:8.3f}\")\n",
    "                print(f\"| time/                  |\")\n",
    "                print(f\"|    fps                 | {fps:8d}\")\n",
    "                print(f\"|    episodes            | {episodes:8d}\")\n",
    "                print(f\"|    time_elapsed        | {elapsed:8.1f}\")\n",
    "                print(f\"|    total_timesteps     | {steps:8d}\")\n",
    "                print(f\"| train/                 |\")\n",
    "                print(f\"|    loss                | {loss.item():8.3f}\")\n",
    "                print(f\"|    policy_loss         | {policy_loss.item():8.3f}\")\n",
    "                print(f\"|    value_loss          | {value_loss.item():8.3f}\")\n",
    "                print(f\"|    explained_variance  | {explained_var.item():8.3f}\")\n",
    "                print(f\"|    n_updates           | {episodes:8d}\")\n",
    "                print(\"-\" * 40)\n",
    "        \n",
    "        if self.verbose == 1:\n",
    "            print(f\"Training complete. Total episodes: {episodes}, total steps: {steps}\")\n",
    "    \n",
    "    def predict(self, obs, deterministic=False, done=False):\n",
    "        \"\"\"\n",
    "        Predicts action given observation using the full trajectory.\n",
    "        Optionally resets buffer if episode done.\n",
    "        \"\"\"\n",
    "        obs_t = torch.tensor(obs, dtype=torch.float32, device=self.device)\n",
    "        self.trajectory.append(obs_t)\n",
    "        traj = torch.stack(self.trajectory)\n",
    "        self.policy.eval()\n",
    "        with torch.no_grad():\n",
    "            logits, _, _ = self.policy(traj)\n",
    "            if deterministic:\n",
    "                action = torch.argmax(logits).item()\n",
    "            else:\n",
    "                dist = Categorical(logits=logits)\n",
    "                action = dist.sample().item()\n",
    "        self.policy.train()\n",
    "        if done:\n",
    "            self.reset_trajectory()\n",
    "        return action\n",
    "\n",
    "    def save(self, path=\"memoryppo.pt\"):\n",
    "        \"\"\"\n",
    "        Saves model parameters to file.\n",
    "        \"\"\"\n",
    "        torch.save(self.policy.state_dict(), path)\n",
    "\n",
    "    def load(self, path=\"memoryppo.pt\"):\n",
    "        \"\"\"\n",
    "        Loads model parameters from file.\n",
    "        \"\"\"\n",
    "        self.policy.load_state_dict(torch.load(path, map_location=self.device))\n",
    "\n",
    "    def evaluate(self, n_episodes=10, deterministic=False, verbose=True):\n",
    "        \"\"\"\n",
    "        Runs evaluation episodes to estimate mean and std of return.\n",
    "        \"\"\"\n",
    "        returns = []\n",
    "        for _ in range(n_episodes):\n",
    "            obs, _ = self.env.reset()\n",
    "            self.reset_trajectory()\n",
    "            done = False\n",
    "            total_reward = 0.0\n",
    "            while not done:\n",
    "                action = self.predict(obs, deterministic=deterministic)\n",
    "                obs, reward, done, _, _ = self.env.step(action)\n",
    "                total_reward += reward\n",
    "                if done:\n",
    "                    self.reset_trajectory()\n",
    "            returns.append(total_reward)\n",
    "        mean_return = np.mean(returns)\n",
    "        std_return = np.std(returns)\n",
    "        if verbose:\n",
    "            print(f\"Evaluation over {n_episodes} episodes: mean return {mean_return:.2f}, std {std_return:.2f}\")\n",
    "        return mean_return, std_return\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e70ea1ca-f1f1-4d67-9888-7ba5508f1a43",
   "metadata": {},
   "source": [
    "# New modules\n",
    "\n",
    "## Differentiable Episodic Memory Buffer\n",
    "A trainable external memory, with:\n",
    "\n",
    "* Key/Value: Store every step as (key = obs, value = obs, or richer tuple).\n",
    "\n",
    "* Gating: Agent learns when to write (memory filter).\n",
    "\n",
    "* Query: At each step, agent emits a query vector; we compute soft attention (weights) over all stored keys.\n",
    "\n",
    "* Retrieval: Output is a weighted sum over stored values.\n",
    "\n",
    "## Policy update:\n",
    "Policy now receives:\n",
    "\n",
    "* The full trajectory (as before)\n",
    "\n",
    "* The current obs\n",
    "\n",
    "* The memory readout (retrieved with a query emitted by the policy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7d2cf308-746e-461f-8ffe-cc56ad35515b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DifferentiableEpisodicMemory(nn.Module):\n",
    "    \"\"\"\n",
    "    Differentiable episodic memory: soft attention over stored keys/values.\n",
    "    - Agent decides what to store (learnable gate).\n",
    "    - Agent emits query vector; gets soft attention-weighted sum of stored values.\n",
    "    \"\"\"\n",
    "    def __init__(self, obs_dim, mem_dim=32, max_size=32):\n",
    "        super().__init__()\n",
    "        self.last_attn = None \n",
    "        self.obs_dim = obs_dim\n",
    "        self.mem_dim = mem_dim\n",
    "        self.max_size = max_size\n",
    "        self.key_proj = nn.Linear(obs_dim, mem_dim)\n",
    "        self.val_proj = nn.Linear(obs_dim, mem_dim)\n",
    "        self.gate = nn.Sequential(\n",
    "            nn.Linear(obs_dim, 1), nn.Sigmoid()\n",
    "        )\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        # Clear all memory at episode start\n",
    "        self.keys = []\n",
    "        self.values = []\n",
    "\n",
    "    def write(self, obs):\n",
    "        # Agent chooses whether to store (learned gate)\n",
    "        obs = obs.detach() if isinstance(obs, torch.Tensor) else torch.tensor(obs, dtype=torch.float32)\n",
    "        key = self.key_proj(obs)\n",
    "        val = self.val_proj(obs)\n",
    "        prob = self.gate(obs)\n",
    "        if prob.item() > np.random.rand():  # stochastic gate\n",
    "            self.keys.append(key)\n",
    "            self.values.append(val)\n",
    "        # Cap memory size\n",
    "        if len(self.keys) > self.max_size:\n",
    "            self.keys = self.keys[-self.max_size:]\n",
    "            self.values = self.values[-self.max_size:]\n",
    "\n",
    "    def query(self, query_vec):\n",
    "        if not self.keys:\n",
    "            self.last_attn = None\n",
    "            return torch.zeros(self.mem_dim, device=query_vec.device)\n",
    "        keys = torch.stack(self.keys)\n",
    "        values = torch.stack(self.values)\n",
    "        attn_logits = torch.matmul(keys, query_vec)\n",
    "        attn = torch.softmax(attn_logits, dim=0)\n",
    "        self.last_attn = attn.detach().cpu().numpy()   # <--- Store for logging\n",
    "        mem_read = (attn.unsqueeze(-1) * values).sum(0)\n",
    "        return mem_read\n",
    "\n",
    "class ExternalMemoryTransformerPolicy(nn.Module):\n",
    "    def __init__(self, obs_dim, mem_dim=32, nhead=4, memory=None, aux_modules=None):\n",
    "        super().__init__()\n",
    "        self.mem_dim = mem_dim\n",
    "        self.memory = memory\n",
    "        self.embed = nn.Linear(obs_dim, mem_dim)\n",
    "        self.pos_embed = nn.Embedding(256, mem_dim)\n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=mem_dim, nhead=nhead, batch_first=True)\n",
    "        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=1)\n",
    "        self.policy_head = nn.Linear(mem_dim * 2, 2)   # Action logits\n",
    "        self.value_head = nn.Linear(mem_dim * 2, 1)    # Value prediction\n",
    "        self.aux_modules = aux_modules if aux_modules is not None else []\n",
    "\n",
    "    def forward(self, trajectory, curr_obs):\n",
    "        T = trajectory.shape[0]\n",
    "        x = self.embed(trajectory)\n",
    "        pos = torch.arange(T, device=trajectory.device)\n",
    "        x = x + self.pos_embed(pos)\n",
    "        x = x.unsqueeze(0)\n",
    "        x = self.transformer(x)\n",
    "        feat = x[0, -1]\n",
    "        query_vec = feat.detach()  # optionally allow grad\n",
    "        mem_read = self.memory.query(query_vec)\n",
    "        full_feat = torch.cat([feat, mem_read], dim=-1)\n",
    "        logits = self.policy_head(full_feat)\n",
    "        value = self.value_head(full_feat)\n",
    "   \n",
    "        aux_preds = {}\n",
    "        for aux in self.aux_modules:\n",
    "            aux_preds[aux.name] = aux.head(feat)\n",
    "        return logits, value.squeeze(-1), aux_preds\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0215acdb-2e38-4237-8586-782d99d98d85",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.distributions import Categorical\n",
    "import torch.nn.functional as F\n",
    "import time\n",
    "from collections import defaultdict\n",
    "\n",
    "# ──────────────────────────────────────────────────────────────\n",
    "# 2. Reward Normalizer\n",
    "# ──────────────────────────────────────────────────────────────\n",
    "\n",
    "class RewardNormalizer:\n",
    "    \"\"\"\n",
    "    Running mean/variance normalizer for rewards.\n",
    "    Stabilizes learning by normalizing returns online.\n",
    "    \"\"\"\n",
    "    def __init__(self, epsilon=1e-8):\n",
    "        self.mean = 0.0\n",
    "        self.var = 1.0\n",
    "        self.count = 1e-4  # prevents division by zero\n",
    "        self.epsilon = epsilon\n",
    "\n",
    "    def update(self, rewards):\n",
    "        \"\"\"\n",
    "        Updates running statistics given a batch of rewards.\n",
    "        Args:\n",
    "            rewards (array-like): list or np.ndarray of rewards\n",
    "        \"\"\"\n",
    "        rewards = np.array(rewards)\n",
    "        batch_mean = rewards.mean()\n",
    "        batch_var = rewards.var()\n",
    "        batch_count = len(rewards)\n",
    "        # Online update of mean/var\n",
    "        self.mean = (self.mean * self.count + batch_mean * batch_count) / (self.count + batch_count)\n",
    "        self.var = (self.var * self.count + batch_var * batch_count) / (self.count + batch_count)\n",
    "        self.count += batch_count\n",
    "\n",
    "    def normalize(self, rewards):\n",
    "        \"\"\"\n",
    "        Normalizes a batch of rewards based on running statistics.\n",
    "        Args:\n",
    "            rewards (array-like): list or np.ndarray of rewards\n",
    "        Returns:\n",
    "            List of normalized rewards\n",
    "        \"\"\"\n",
    "        rewards = np.array(rewards)\n",
    "        return ((rewards - self.mean) / (np.sqrt(self.var) + self.epsilon)).tolist()\n",
    "\n",
    "# ──────────────────────────────────────────────────────────────\n",
    "# 3. State Counter for Intrinsic Reward (Exploration Bonus)\n",
    "# ──────────────────────────────────────────────────────────────\n",
    "\n",
    "class StateCounter:\n",
    "    \"\"\"\n",
    "    Counts state visitations for intrinsic motivation (exploration).\n",
    "    Implements simple count-based exploration bonus.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self.counts = defaultdict(int)\n",
    "\n",
    "    def count(self, obs):\n",
    "        \"\"\"\n",
    "        Increments and returns the visit count for a discretized observation.\n",
    "        Args:\n",
    "            obs (np.ndarray): observation\n",
    "        Returns:\n",
    "            int: visit count\n",
    "        \"\"\"\n",
    "        key = tuple(np.round(obs, 2))  # discretize for generalization\n",
    "        self.counts[key] += 1\n",
    "        return self.counts[key]\n",
    "\n",
    "    def intrinsic_reward(self, obs):\n",
    "        \"\"\"\n",
    "        Returns intrinsic reward: inversely proportional to sqrt of count.\n",
    "        Args:\n",
    "            obs (np.ndarray): observation\n",
    "        Returns:\n",
    "            float: exploration bonus\n",
    "        \"\"\"\n",
    "        c = self.count(obs)\n",
    "        return 1.0 / np.sqrt(c)\n",
    "\n",
    "# ──────────────────────────────────────────────────────────────\n",
    "# 4. Generalized Advantage Estimation (GAE) and Explained Variance\n",
    "# ──────────────────────────────────────────────────────────────\n",
    "\n",
    "def compute_explained_variance(y_pred, y_true):\n",
    "    \"\"\"\n",
    "    Computes explained variance between prediction and ground-truth.\n",
    "    Used for value function diagnostics in RL.\n",
    "    \"\"\"\n",
    "    var_y = torch.var(y_true)\n",
    "    if var_y == 0:\n",
    "        return torch.tensor(0.0)\n",
    "    return 1 - torch.var(y_true - y_pred) / (var_y + 1e-8)\n",
    "\n",
    "def compute_gae(rewards, values, gamma=0.99, lam=0.95, last_value=0.0):\n",
    "    \"\"\"\n",
    "    Compute Generalized Advantage Estimation (GAE) for a trajectory.\n",
    "    Args:\n",
    "        rewards (torch.Tensor): reward sequence [T]\n",
    "        values (torch.Tensor): value sequence [T]\n",
    "        gamma (float): discount factor\n",
    "        lam (float): GAE lambda\n",
    "        last_value (float): bootstrap value after final state\n",
    "    Returns:\n",
    "        torch.Tensor: advantage sequence [T]\n",
    "    \"\"\"\n",
    "    T = len(rewards)\n",
    "    advantages = torch.zeros(T, dtype=torch.float32, device=values.device)\n",
    "    gae = 0.0\n",
    "    # concatenate last value for bootstrap\n",
    "    values_ext = torch.cat([values, torch.tensor([last_value], dtype=torch.float32, device=values.device)])\n",
    "    for t in reversed(range(T)):\n",
    "        delta = rewards[t] + gamma * values_ext[t + 1] - values_ext[t]\n",
    "        gae = delta + gamma * lam * gae\n",
    "        advantages[t] = gae\n",
    "    return advantages\n",
    "\n",
    "# ──────────────────────────────────────────────────────────────\n",
    "# 5. Memory Transformer Policy with Auxiliary Head\n",
    "# ──────────────────────────────────────────────────────────────\n",
    "\n",
    "class MemoryTransformerPolicy(nn.Module):\n",
    "    \"\"\"\n",
    "    MemoryTransformerPolicy\n",
    "    ----------------------\n",
    "    Transformer-based policy for sequence decision tasks.\n",
    "    - Processes full trajectory as input (not just current state)\n",
    "    - Supports auxiliary head for supervised tasks (e.g., cue recall)\n",
    "\n",
    "    Args:\n",
    "        obs_dim (int): Dimension of observation vector\n",
    "        mem_dim (int): Transformer embedding size\n",
    "        nhead (int): Number of attention heads\n",
    "    \"\"\"\n",
    "    def __init__(self, obs_dim, mem_dim=32, nhead=4):\n",
    "        super().__init__()\n",
    "        self.mem_dim = mem_dim\n",
    "        self.embed = nn.Linear(obs_dim, mem_dim)\n",
    "        self.pos_embed = nn.Embedding(256, mem_dim)\n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=mem_dim, nhead=nhead, batch_first=True)\n",
    "        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=1)\n",
    "        self.policy_head = nn.Linear(mem_dim, 2)   # Action logits\n",
    "        self.value_head = nn.Linear(mem_dim, 1)    # Value prediction\n",
    "        self.aux_head = nn.Linear(mem_dim, 2)      # Auxiliary: predict initial cue\n",
    "\n",
    "    def forward(self, trajectory):\n",
    "        \"\"\"\n",
    "        Forward pass: processes trajectory and outputs policy, value, and aux predictions.\n",
    "        Args:\n",
    "            trajectory (torch.Tensor): [T, obs_dim] trajectory\n",
    "        Returns:\n",
    "            logits (torch.Tensor): [2,] action logits\n",
    "            value (torch.Tensor): [1,] state value\n",
    "            aux_pred (torch.Tensor): [2,] auxiliary logits (for cue prediction)\n",
    "        \"\"\"\n",
    "        T = trajectory.shape[0]\n",
    "        x = self.embed(trajectory)\n",
    "        pos = torch.arange(T, device=trajectory.device)\n",
    "        x = x + self.pos_embed(pos)\n",
    "        x = x.unsqueeze(0)  # [1, T, mem_dim]\n",
    "        x = self.transformer(x)\n",
    "        feat = x[0, -1]  # use final state embedding for decision\n",
    "        logits = self.policy_head(feat)\n",
    "        value = self.value_head(feat)\n",
    "        aux_pred = self.aux_head(feat)\n",
    "        return logits, value.squeeze(-1), aux_pred\n",
    "\n",
    "# ──────────────────────────────────────────────────────────────\n",
    "# 6. HER-enabled MemoryPPO Trainer\n",
    "# ──────────────────────────────────────────────────────────────\n",
    "\n",
    "class ExternalMemoryPPO:\n",
    "    \"\"\"\n",
    "    ExternalMemoryPPO\n",
    "    ---------\n",
    "    PPO trainer for memory-based RL tasks.\n",
    "    - Supports Hindsight Experience Replay (HER)\n",
    "    - Reward normalization, auxiliary losses, intrinsic/exploration bonuses\n",
    "    - Sequence-based trajectory modeling\n",
    "\n",
    "    Args:\n",
    "        policy_class: policy network class\n",
    "        env: Gymnasium environment\n",
    "        device: PyTorch device\n",
    "        (other RL hyperparameters)\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self, \n",
    "        policy_class, \n",
    "        env, \n",
    "        verbose=0,\n",
    "        learning_rate=1e-3, \n",
    "        gamma=0.99, \n",
    "        lam=0.95, \n",
    "        device=\"cpu\",\n",
    "        her=True,                 # Enable HER\n",
    "        reward_norm=False,         # Enable reward normalization\n",
    "        aux=True,                 # Enable auxiliary loss\n",
    "        intrinsic_expl=True,      # Enable intrinsic exploration\n",
    "        intrinsic_eta=0.05,        # Intrinsic bonus multiplier\n",
    "        memory=None,      \n",
    "        ent_coef=0.01\n",
    "    ):\n",
    "        self.env = env\n",
    "        self.device = torch.device(device)\n",
    "        self.gamma = gamma\n",
    "        self.lam = lam\n",
    "        self.ent_coef=ent_coef\n",
    "        self.verbose = verbose\n",
    "        self.memory = memory\n",
    "        self.policy = policy_class(obs_dim=env.observation_space.shape[0], memory=memory).to(self.device)\n",
    "        #self.policy = torch.jit.script(self.policy)\n",
    "        self.optimizer = optim.Adam(self.policy.parameters(), lr=learning_rate)\n",
    "        self.training_steps = 0\n",
    "        self.episode_rewards = []\n",
    "        self.episode_lengths = []\n",
    "        self.her = her\n",
    "        self.reward_norm = reward_norm\n",
    "        self.aux = aux\n",
    "        self.intrinsic_expl = intrinsic_expl\n",
    "        self.intrinsic_eta = intrinsic_eta\n",
    "        self.reward_normalizer = RewardNormalizer()\n",
    "        self.state_counter = StateCounter()\n",
    "        self.trajectory = []\n",
    "    \n",
    "    def reset_trajectory(self):\n",
    "        \"\"\"\n",
    "        Resets internal trajectory buffer after each episode or reset.\n",
    "        \"\"\"\n",
    "        self.trajectory = []\n",
    "        \n",
    "    def run_episode(self, her_target=None):\n",
    "        \"\"\"\n",
    "        Executes a full episode in the environment.\n",
    "        Supports HER by relabelling the initial cue if provided.\n",
    "        Returns trajectory data for training.\n",
    "        \"\"\"\n",
    "        obs, _ = self.env.reset()\n",
    "        if her_target is not None:\n",
    "            obs[0] = her_target  # Relabel initial cue with HER goal\n",
    "        if self.memory is not None:\n",
    "            self.memory.reset()\n",
    "        done = False\n",
    "        trajectory = []\n",
    "        rewards = []\n",
    "        actions = []\n",
    "        log_probs = []\n",
    "        values = []\n",
    "        entropies_ep = []\n",
    "        aux_preds = []\n",
    "        t = 0\n",
    "        gate_history = []\n",
    "        memory_size_history = []\n",
    "        attn_weights = None\n",
    "        initial_cue = int(obs[0])  # Ground-truth cue for auxiliary prediction\n",
    "\n",
    "        while not done:\n",
    "            obs_t = torch.tensor(obs, dtype=torch.float32, device=self.device)\n",
    "            if self.memory is not None:\n",
    "                self.memory.write(obs_t)\n",
    "                # Log memory gate and size\n",
    "                gate_prob = self.memory.gate(obs_t).item()\n",
    "                gate_history.append(gate_prob)\n",
    "                memory_size_history.append(len(self.memory.keys))\n",
    "            trajectory.append(obs_t)\n",
    "            traj = torch.stack(trajectory)\n",
    "            logits, value, aux_pred = self.policy(traj, obs_t)#self.policy(traj)\n",
    "            dist = Categorical(logits=logits)\n",
    "            action = dist.sample()\n",
    "            log_prob = dist.log_prob(action)\n",
    "            entropy = dist.entropy()\n",
    "            obs, reward, done, _, _ = self.env.step(action.item())\n",
    "            # Intrinsic (exploration) reward\n",
    "            if self.intrinsic_expl:\n",
    "                reward += self.intrinsic_eta * self.state_counter.intrinsic_reward(obs)\n",
    "            actions.append(action)\n",
    "            log_probs.append(log_prob)\n",
    "            rewards.append(torch.tensor(reward, dtype=torch.float32, device=self.device))\n",
    "            values.append(value)\n",
    "            entropies_ep.append(entropy)\n",
    "            aux_preds.append(aux_pred)\n",
    "            t += 1\n",
    "\n",
    "        if self.memory is not None:\n",
    "            attn_weights = self.memory.last_attn  # (from query at last policy call)\n",
    "        return {\n",
    "            \"trajectory\": trajectory,\n",
    "            \"actions\": actions,\n",
    "            \"rewards\": rewards,\n",
    "            \"log_probs\": log_probs,\n",
    "            \"values\": values,\n",
    "            \"entropies\": entropies_ep,\n",
    "            \"aux_preds\": aux_preds,\n",
    "            \"initial_cue\": initial_cue,\n",
    "            \"gate_history\": gate_history,\n",
    "            \"memory_size_history\": memory_size_history,\n",
    "            \"attn_weights\": attn_weights\n",
    "        }\n",
    "\n",
    "    def learn(self, total_timesteps=2000, log_interval=100):\n",
    "        \"\"\"\n",
    "        Main training loop for PPO.\n",
    "        Collects rollouts, computes losses, performs optimization, and logs metrics.\n",
    "        Supports HER, reward normalization, auxiliary loss, and intrinsic bonuses.\n",
    "        \"\"\"\n",
    "        steps = 0\n",
    "        episodes = 0\n",
    "        all_returns = []\n",
    "        start_time = time.time()\n",
    "        aux_losses = []\n",
    "\n",
    "        while steps < total_timesteps:\n",
    "            # ---- Run normal episode ----\n",
    "            episode = self.run_episode()\n",
    "            # ---- Reward normalization ----\n",
    "            if self.reward_norm:\n",
    "                self.reward_normalizer.update([r.item() for r in episode[\"rewards\"]])\n",
    "                episode[\"rewards\"] = [torch.tensor(rn, dtype=torch.float32, device=self.device)\n",
    "                                      for rn in self.reward_normalizer.normalize([r.item() for r in episode[\"rewards\"]])]\n",
    "\n",
    "            # ---- HER relabelling ----\n",
    "            if self.her:\n",
    "                her_target = int(episode[\"actions\"][-1].item())\n",
    "                her_episode = self.run_episode(her_target=her_target)\n",
    "                for k in [\"trajectory\", \"actions\", \"rewards\", \"log_probs\", \"values\", \"entropies\", \"aux_preds\"]:\n",
    "                    episode[k] += her_episode[k]\n",
    "                initial_cue = [episode[\"initial_cue\"], her_target]\n",
    "            else:\n",
    "                initial_cue = [episode[\"initial_cue\"]]\n",
    "\n",
    "            # ---- Batchify for loss ----\n",
    "            trajectory = episode[\"trajectory\"]\n",
    "            actions = episode[\"actions\"]\n",
    "            rewards = episode[\"rewards\"]\n",
    "            log_probs = episode[\"log_probs\"]\n",
    "            values = episode[\"values\"]\n",
    "            entropies_ep = episode[\"entropies\"]\n",
    "            aux_preds = episode[\"aux_preds\"]\n",
    "            T = len(rewards)\n",
    "\n",
    "            rewards_t = torch.stack(rewards)\n",
    "            values_t = torch.stack(values)\n",
    "            log_probs_t = torch.stack(log_probs)\n",
    "            actions_t = torch.stack(actions)\n",
    "            aux_preds_t = torch.stack(aux_preds)\n",
    "            last_value = 0.0\n",
    "            advantages = compute_gae(rewards_t, values_t, gamma=self.gamma, lam=self.lam, last_value=last_value)\n",
    "            returns = advantages + values_t.detach()\n",
    "\n",
    "            # ---- Losses ----\n",
    "            policy_loss = -(log_probs_t * advantages.detach()).sum()\n",
    "            value_loss = F.mse_loss(values_t, returns)\n",
    "            entropy_mean = torch.stack(entropies_ep).mean()\n",
    "            explained_var = compute_explained_variance(values_t, returns)\n",
    "\n",
    "            # Auxiliary (supervised) loss for cue recall\n",
    "            aux_loss = torch.tensor(0.0, device=self.device)\n",
    "            if self.aux:\n",
    "                if self.her:\n",
    "                    cues = torch.tensor(initial_cue, dtype=torch.long, device=self.device)       # [2]\n",
    "                    aux_preds_to_use = torch.stack([aux_preds_t[0], aux_preds_t[T // 2]])        # [2,2]\n",
    "                else:\n",
    "                    cues = torch.tensor([initial_cue[0]], dtype=torch.long, device=self.device)  # [1]\n",
    "                    aux_preds_to_use = aux_preds_t[0].unsqueeze(0)                               # [1,2]\n",
    "                assert aux_preds_to_use.shape[0] == cues.shape[0], \\\n",
    "                    f\"Shape mismatch: preds {aux_preds_to_use.shape}, cues {cues.shape}\"\n",
    "                aux_loss = F.cross_entropy(aux_preds_to_use, cues)\n",
    "                aux_losses.append(aux_loss.item())\n",
    "\n",
    "            # ---- Total loss ----\n",
    "            loss = policy_loss + 0.5 * value_loss + 0.1 * aux_loss - self.ent_coef * entropy_mean\n",
    "\n",
    "            self.optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "\n",
    "            total_reward = sum([r.item() for r in rewards])\n",
    "            self.episode_rewards.append(total_reward)\n",
    "            self.episode_lengths.append(T)\n",
    "            episodes += 1\n",
    "            steps += T\n",
    "\n",
    "            # SB3-style logging\n",
    "            if episodes % log_interval == 0 and self.verbose == 1:\n",
    "                elapsed = time.time() - start_time\n",
    "                mean_rew = np.mean(self.episode_rewards[-log_interval:])\n",
    "                mean_len = np.mean(self.episode_lengths[-log_interval:])\n",
    "                fps = int(steps / (elapsed + 1e-8))\n",
    "                adv_mean = advantages.mean().item()\n",
    "                adv_std = advantages.std().item()\n",
    "                mean_entropy = entropy_mean.item()\n",
    "                mean_aux = np.mean(aux_losses[-log_interval:]) if aux_losses else 0.0\n",
    "                print(\"-\" * 40)\n",
    "                print(f\"| rollout/               |\")\n",
    "                print(f\"|    ep_len_mean         | {mean_len:8.2f}\")\n",
    "                print(f\"|    ep_rew_mean         | {mean_rew:8.2f}\")\n",
    "                print(f\"|    policy_entropy      | {mean_entropy:8.3f}\")\n",
    "                print(f\"|    advantage_mean      | {adv_mean:8.3f}\")\n",
    "                print(f\"|    advantage_std       | {adv_std:8.3f}\")\n",
    "                print(f\"|    aux_loss_mean       | {mean_aux:8.3f}\")\n",
    "                print(f\"| time/                  |\")\n",
    "                print(f\"|    fps                 | {fps:8d}\")\n",
    "                print(f\"|    episodes            | {episodes:8d}\")\n",
    "                print(f\"|    time_elapsed        | {elapsed:8.1f}\")\n",
    "                print(f\"|    total_timesteps     | {steps:8d}\")\n",
    "                print(f\"| train/                 |\")\n",
    "                print(f\"|    loss                | {loss.item():8.3f}\")\n",
    "                print(f\"|    policy_loss         | {policy_loss.item():8.3f}\")\n",
    "                print(f\"|    value_loss          | {value_loss.item():8.3f}\")\n",
    "                print(f\"|    explained_variance  | {explained_var.item():8.3f}\")\n",
    "                print(f\"|    n_updates           | {episodes:8d}\")\n",
    "                print(f\"| memory/                |\")\n",
    "                print(f\"|    gate_history        | {len(set(episode['gate_history']))}\")\n",
    "                print(f\"|    gate_history        | {episode['gate_history']}\")\n",
    "                print(f\"|    memory_size_history | {episode['memory_size_history']}\")\n",
    "                print(f\"|    attn_weights        | {episode['attn_weights']}\")\n",
    "                print(\"-\" * 40)\n",
    "        \n",
    "        if self.verbose == 1:\n",
    "            print(f\"Training complete. Total episodes: {episodes}, total steps: {steps}\")\n",
    "    \n",
    "\n",
    "    def predict(self, obs, deterministic=False, done=False, log_diagnostics=False):\n",
    "        \"\"\"\n",
    "        Predicts action given observation using the full trajectory.\n",
    "        Optionally resets buffer if episode done.\n",
    "        \"\"\"\n",
    "        obs_t = torch.tensor(obs, dtype=torch.float32, device=self.device)\n",
    "        self.trajectory.append(obs_t)\n",
    "        traj = torch.stack(self.trajectory)\n",
    "        self.policy.eval()\n",
    "        with torch.no_grad():\n",
    "            logits, _, _ = self.policy(traj, obs_t)\n",
    "            if deterministic:\n",
    "                action = torch.argmax(logits).item()\n",
    "            else:\n",
    "                dist = Categorical(logits=logits)\n",
    "                action = dist.sample().item()\n",
    "            # Optionally, here you could log self.memory.gate(obs_t).item() and self.memory.last_attn\n",
    "        self.policy.train()\n",
    "        if done:\n",
    "            self.reset_trajectory()\n",
    "        return action\n",
    "        \n",
    "    def save(self, path=\"memoryppo.pt\"):\n",
    "        \"\"\"\n",
    "        Saves model parameters to file.\n",
    "        \"\"\"\n",
    "        torch.save(self.policy.state_dict(), path)\n",
    "\n",
    "    def load(self, path=\"memoryppo.pt\"):\n",
    "        \"\"\"\n",
    "        Loads model parameters from file.\n",
    "        \"\"\"\n",
    "        self.policy.load_state_dict(torch.load(path, map_location=self.device))\n",
    "\n",
    "    def evaluate(self, n_episodes=10, deterministic=False, verbose=True):\n",
    "        \"\"\"\n",
    "        Runs evaluation episodes to estimate mean and std of return.\n",
    "        \"\"\"\n",
    "        returns = []\n",
    "        for _ in range(n_episodes):\n",
    "            obs, _ = self.env.reset()\n",
    "            self.reset_trajectory()\n",
    "            done = False\n",
    "            total_reward = 0.0\n",
    "            while not done:\n",
    "                action = self.predict(obs, deterministic=deterministic)\n",
    "                obs, reward, done, _, _ = self.env.step(action)\n",
    "                total_reward += reward\n",
    "                if done:\n",
    "                    self.reset_trajectory()\n",
    "            returns.append(total_reward)\n",
    "        mean_return = np.mean(returns)\n",
    "        std_return = np.std(returns)\n",
    "        if verbose:\n",
    "            print(f\"Evaluation over {n_episodes} episodes: mean return {mean_return:.2f}, std {std_return:.2f}\")\n",
    "        return mean_return, std_return\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1916cb54-bbc6-44dd-937c-2276ddc353df",
   "metadata": {},
   "source": [
    "# Auxiliar Logic as a plugin\n",
    "\n",
    "### Usage:\n",
    "```\n",
    "# Auxiliary heads ====================\n",
    "\n",
    "class CueAuxModule(MraAuxiliaryModule):\n",
    "    def __init__(self, feat_dim, n_classes=2):\n",
    "        super().__init__(\"cue\")\n",
    "        self.head = nn.Linear(feat_dim, n_classes)\n",
    "\n",
    "    def aux_loss(self, pred, target, context=None):\n",
    "        # pred: [B, n_classes], target: [B]\n",
    "        return F.cross_entropy(pred, target)\n",
    "\n",
    "# Auxiliary head injection ============\n",
    "aux_modules = [\n",
    "    CueAuxModule(feat_dim=32),\n",
    "    ConfidenceAuxModule(feat_dim=32),\n",
    "    # MistakeAuxModule, FutureAuxModule, etc...\n",
    "]\n",
    "\n",
    "\n",
    "agent = ExternalMemoryPPO(\n",
    "    policy_class=MemoryTransformerPolicy,\n",
    "    env=env,\n",
    "    aux_modules=aux_modules,\n",
    "    # ...\n",
    ")\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "909c5cb0-82cb-44a8-841a-ed4945a73c06",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MraAuxiliaryModule:\n",
    "    \"\"\"\n",
    "    A plugin system so agent can compute and optimize all auxiliary losses together.\n",
    "    * each module can have its own head and loss\n",
    "    * plug and play\n",
    "    * integrated logs on rollout\n",
    "\n",
    "    \"\"\"\n",
    "    def __init__(self, name, task=\"classification\"):\n",
    "        self.name = name\n",
    "        self.head = None  # will be nn.Module subclass\n",
    "        self.task = task\n",
    "\n",
    "    def aux_loss(self, pred, target, context=None):\n",
    "        # Should be overridden by subclass\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def get_head(self, feat_dim):\n",
    "        # Should be overridden to return an nn.Module\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def aux_metrics(self, pred, target, context=None):\n",
    "        \"\"\"\n",
    "        Returns a dict of any diagnostics, e.g. accuracy, mse, etc.\n",
    "        User can override this but by default he can use the\n",
    "        corresponding task metric\n",
    "        \"\"\"\n",
    "        if self.task == \"classification\":\n",
    "            return self.__classification_metric(pred,target,context)\n",
    "        else:\n",
    "            return self.__regression_metric(pred,target,context)\n",
    "\n",
    "    def __classification_metric(self,pred,target,context=None):\n",
    "        pred_label = pred.argmax(dim=-1)\n",
    "        correct = (pred_label == target).float()\n",
    "        acc = correct.mean().item()\n",
    "        return {'acc': acc}\n",
    "        \n",
    "    def __regression_metric(self,pred,target,context=None):\n",
    "        mse = F.mse_loss(torch.sigmoid(pred.squeeze(-1)), target.float()).item()\n",
    "        return {'mse': mse}\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "be524894-bd4a-46e6-ae5e-5564eb7dff97",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CueAuxModule(MraAuxiliaryModule):\n",
    "    def __init__(self, feat_dim, n_classes=2):\n",
    "        super().__init__(\"cue\")\n",
    "        self.head = nn.Linear(feat_dim, n_classes)\n",
    "\n",
    "    def aux_loss(self, pred, target, context=None):\n",
    "        return F.cross_entropy(pred, target)\n",
    "\n",
    "\n",
    "\n",
    "class ConfidenceAuxModule(MraAuxiliaryModule):\n",
    "    def __init__(self, feat_dim):\n",
    "        super().__init__(\"confidence\")\n",
    "        self.head = nn.Linear(feat_dim, 1)\n",
    "\n",
    "    def aux_loss(self, pred, target, context=None):\n",
    "        return F.mse_loss(torch.sigmoid(pred.squeeze(-1)), target.float())\n",
    "\n",
    "    def aux_metrics(self, pred, target, context=None):\n",
    "        mse = F.mse_loss(torch.sigmoid(pred.squeeze(-1)), target.float()).item()\n",
    "        return {'mse': mse}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7810b1d2-5b36-4567-88d9-7e56fd004fd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EarlyStoppingMonitor:\n",
    "    def __init__(self, target=0.95, min_logs=5):\n",
    "        self.target = target\n",
    "        self.min_logs = min_logs\n",
    "        self.history = []\n",
    "\n",
    "    def update(self, rew):\n",
    "        self.history.append(rew)\n",
    "        if len(self.history) > self.min_logs:\n",
    "            self.history.pop(0)\n",
    "        return self.should_stop()\n",
    "\n",
    "    def should_stop(self):\n",
    "        if len(self.history) < self.min_logs:\n",
    "            return False\n",
    "        # Stop if all recent rewards exceed target\n",
    "        return all(r >= self.target for r in self.history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6913c742-1b1d-47e9-8402-ea418e17d4a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class RNDModule(nn.Module):\n",
    "    def __init__(self, obs_dim, emb_dim=32):\n",
    "        super().__init__()\n",
    "        # Fixed random target net\n",
    "        self.target = nn.Sequential(\n",
    "            nn.Linear(obs_dim, emb_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(emb_dim, emb_dim)\n",
    "        )\n",
    "        # Predictor net\n",
    "        self.predictor = nn.Sequential(\n",
    "            nn.Linear(obs_dim, emb_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(emb_dim, emb_dim)\n",
    "        )\n",
    "        # Freeze target net\n",
    "        for p in self.target.parameters():\n",
    "            p.requires_grad = False\n",
    "\n",
    "    def forward(self, obs):\n",
    "        with torch.no_grad():\n",
    "            target_emb = self.target(obs)\n",
    "        pred_emb = self.predictor(obs)\n",
    "        # Return MSE as novelty signal\n",
    "        return F.mse_loss(pred_emb, target_emb, reduction='none').mean(dim=-1)\n",
    "        \n",
    "class RewardNormalizer:\n",
    "    def __init__(self, epsilon=1e-8):\n",
    "        self.mean = 0.0\n",
    "        self.var = 1.0\n",
    "        self.count = 1e-4\n",
    "        self.epsilon = epsilon\n",
    "\n",
    "    def update(self, rewards):\n",
    "        rewards = np.array(rewards)\n",
    "        batch_mean = rewards.mean()\n",
    "        batch_var = rewards.var()\n",
    "        batch_count = len(rewards)\n",
    "        self.mean = (self.mean * self.count + batch_mean * batch_count) / (self.count + batch_count)\n",
    "        self.var = (self.var * self.count + batch_var * batch_count) / (self.count + batch_count)\n",
    "        self.count += batch_count\n",
    "\n",
    "    def normalize(self, rewards):\n",
    "        rewards = np.array(rewards)\n",
    "        return ((rewards - self.mean) / (np.sqrt(self.var) + self.epsilon)).tolist()\n",
    "\n",
    "# ──────────────────────────────────────────────────────────────\n",
    "# 3. State Counter for Intrinsic Reward (Exploration Bonus)\n",
    "# ──────────────────────────────────────────────────────────────\n",
    "\n",
    "class StateCounter:\n",
    "    def __init__(self):\n",
    "        self.counts = defaultdict(int)\n",
    "\n",
    "    def count(self, obs):\n",
    "        key = tuple(np.round(obs, 2))\n",
    "        self.counts[key] += 1\n",
    "        return self.counts[key]\n",
    "\n",
    "    def intrinsic_reward(self, obs):\n",
    "        c = self.count(obs)\n",
    "        return 1.0 / np.sqrt(c)\n",
    "\n",
    "# ──────────────────────────────────────────────────────────────\n",
    "# 4. Generalized Advantage Estimation (GAE) and Explained Variance\n",
    "# ──────────────────────────────────────────────────────────────\n",
    "\n",
    "def compute_explained_variance(y_pred, y_true):\n",
    "    var_y = torch.var(y_true)\n",
    "    if var_y == 0:\n",
    "        return torch.tensor(0.0)\n",
    "    return 1 - torch.var(y_true - y_pred) / (var_y + 1e-8)\n",
    "\n",
    "def compute_gae(rewards, values, gamma=0.99, lam=0.95, last_value=0.0):\n",
    "    T = len(rewards)\n",
    "    advantages = torch.zeros(T, dtype=torch.float32, device=values.device)\n",
    "    gae = 0.0\n",
    "    values_ext = torch.cat([values, torch.tensor([last_value], dtype=torch.float32, device=values.device)])\n",
    "    for t in reversed(range(T)):\n",
    "        delta = rewards[t] + gamma * values_ext[t + 1] - values_ext[t]\n",
    "        gae = delta + gamma * lam * gae\n",
    "        advantages[t] = gae\n",
    "    return advantages\n",
    "\n",
    "def print_sb3_style_log_box(stats):\n",
    "    # Flatten stats for max width calculation\n",
    "    all_rows = []\n",
    "    for section in stats:\n",
    "        all_rows.append((section[\"header\"] + \"/\", None, True))\n",
    "        for k, v in section[\"stats\"].items():\n",
    "            all_rows.append((k, v, False))\n",
    "    # Compute widths\n",
    "    key_width = max(\n",
    "        len(\"    \" + k) if not is_section else len(k)\n",
    "        for k, v, is_section in all_rows\n",
    "    )\n",
    "    val_width = 10\n",
    "    box_width = 2 + key_width + 3 + val_width \n",
    "\n",
    "    def fmt_row(label, value, is_section):\n",
    "        if is_section:\n",
    "            return f\"| {label:<{key_width}}|{' ' * val_width} |\"\n",
    "        else:\n",
    "            # Format value: float = 8.3f, int = 8d, tensor fallback\n",
    "            if hasattr(value, 'item'):\n",
    "                value = value.item()\n",
    "            if isinstance(value, float):\n",
    "                s_value = f\"{value:8.3f}\"\n",
    "            elif isinstance(value, int):\n",
    "                s_value = f\"{value:8d}\"\n",
    "            else:\n",
    "                s_value = str(value)\n",
    "            s_value_centered = f\"{s_value:^{val_width}}\"\n",
    "            return f\"|    {label:<{key_width-4}} |{s_value_centered} |\"\n",
    "\n",
    "    print(\"-\" * box_width)\n",
    "    for section in stats:\n",
    "        print(fmt_row(section[\"header\"] + \"/\", None, True))\n",
    "        for k, v in section[\"stats\"].items():\n",
    "            print(fmt_row(k, v, False))\n",
    "    print(\"-\" * box_width)\n",
    "\n",
    "# ──────────────────────────────────────────────────────────────\n",
    "# 5. Memory Transformer Policy WITH MULTI-AUX SUPPORT\n",
    "# ──────────────────────────────────────────────────────────────\n",
    "\n",
    "class MemoryTransformerPolicy(nn.Module):\n",
    "    __version__     = \"1.1.0\"\n",
    "    __description__ = \"Now supports injection of custom auxiliary modules\"\n",
    "    \n",
    "    def __init__(self, obs_dim, mem_dim=32, nhead=4, aux_modules=None, **kwargs):\n",
    "        super().__init__()\n",
    "        self.mem_dim = mem_dim\n",
    "        self.embed = nn.Linear(obs_dim, mem_dim)\n",
    "        self.pos_embed = nn.Embedding(256, mem_dim)\n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=mem_dim, nhead=nhead, batch_first=True)\n",
    "        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=1)\n",
    "        self.policy_head = nn.Linear(mem_dim, 2)\n",
    "        self.value_head = nn.Linear(mem_dim, 1)\n",
    "        self.aux_modules = aux_modules if aux_modules is not None else []\n",
    "\n",
    "    def forward(self, trajectory, obs_t=None):\n",
    "        T = trajectory.shape[0]\n",
    "        x = self.embed(trajectory)\n",
    "        pos = torch.arange(T, device=trajectory.device)\n",
    "        x = x + self.pos_embed(pos)\n",
    "        x = x.unsqueeze(0)\n",
    "        x = self.transformer(x)\n",
    "        feat = x[0, -1]\n",
    "        logits = self.policy_head(feat)\n",
    "        value = self.value_head(feat)\n",
    "        aux_preds = {}\n",
    "        for aux in self.aux_modules:\n",
    "            aux_preds[aux.name] = aux.head(feat)\n",
    "        return logits, value.squeeze(-1), aux_preds\n",
    "\n",
    "# ──────────────────────────────────────────────────────────────\n",
    "# 6. Multi-Aux HER-enabled MemoryPPO Trainer (with metric logging)\n",
    "# ──────────────────────────────────────────────────────────────\n",
    "\n",
    "class ExternalMemoryPPO:\n",
    "    def __init__(\n",
    "        self, \n",
    "        policy_class, \n",
    "        env, \n",
    "        verbose=0,\n",
    "        learning_rate=1e-3, \n",
    "        gamma=0.99, \n",
    "        lam=0.95, \n",
    "        device=\"cpu\",\n",
    "        her=False,\n",
    "        reward_norm=False,\n",
    "        intrinsic_expl=True,\n",
    "        intrinsic_eta=0.01,\n",
    "        ent_coef=0.01,\n",
    "        memory=None,\n",
    "        aux_modules=None,\n",
    "        use_rnd=True, rnd_emb_dim=32, rnd_lr=1e-3,\n",
    "    ):\n",
    "        self.env = env\n",
    "        self.device = torch.device(device)\n",
    "        self.gamma = gamma\n",
    "        self.lam = lam\n",
    "        self.ent_coef=ent_coef\n",
    "        self.verbose = verbose\n",
    "        self.memory = memory\n",
    "        self.aux_modules = aux_modules if aux_modules is not None else []\n",
    "        self.aux = len(self.aux_modules) > 0\n",
    "        self.policy = policy_class(\n",
    "            obs_dim=env.observation_space.shape[0], \n",
    "            memory=memory,\n",
    "            aux_modules=self.aux_modules\n",
    "        ).to(self.device)\n",
    "        self.optimizer = optim.Adam(self.policy.parameters(), lr=learning_rate)\n",
    "        self.training_steps = 0\n",
    "        self.episode_rewards = []\n",
    "        self.episode_lengths = []\n",
    "        self.her = her\n",
    "        self.reward_norm = reward_norm\n",
    "        self.intrinsic_expl = intrinsic_expl\n",
    "        self.intrinsic_eta = intrinsic_eta\n",
    "        self.reward_normalizer = RewardNormalizer()\n",
    "        self.state_counter = StateCounter()\n",
    "        self.use_rnd = use_rnd\n",
    "        if self.use_rnd:\n",
    "            self.rnd = RNDModule(env.observation_space.shape[0], emb_dim=rnd_emb_dim).to(self.device)\n",
    "            self.rnd_optimizer = torch.optim.Adam(self.rnd.predictor.parameters(), lr=rnd_lr)\n",
    "        self.trajectory = []\n",
    "\n",
    "    def reset_trajectory(self):\n",
    "        self.trajectory = []\n",
    "\n",
    "    def run_episode(self, her_target=None):\n",
    "        obs, _ = self.env.reset()\n",
    "        if her_target is not None:\n",
    "            obs[0] = her_target\n",
    "        if self.memory is not None:\n",
    "            self.memory.reset()\n",
    "        done = False\n",
    "        trajectory = []\n",
    "        rewards = []\n",
    "        actions = []\n",
    "        log_probs = []\n",
    "        values = []\n",
    "        entropies_ep = []\n",
    "        aux_preds_list = []\n",
    "        gate_history = []\n",
    "        memory_size_history = []\n",
    "        attn_weights = None\n",
    "        initial_cue = int(obs[0])\n",
    "        aux_targets_ep = {aux.name: [] for aux in self.aux_modules}\n",
    "\n",
    "        while not done:\n",
    "            obs_t = torch.tensor(obs, dtype=torch.float32, device=self.device)\n",
    "            if self.memory is not None:\n",
    "                self.memory.write(obs_t)\n",
    "                gate_prob = self.memory.gate(obs_t).item()\n",
    "                gate_history.append(gate_prob)\n",
    "                memory_size_history.append(len(self.memory.keys))\n",
    "            trajectory.append(obs_t)\n",
    "            traj = torch.stack(trajectory)\n",
    "            logits, value, aux_preds = self.policy(traj, obs_t)\n",
    "            dist = Categorical(logits=logits)\n",
    "            action = dist.sample()\n",
    "            log_prob = dist.log_prob(action)\n",
    "            entropy = dist.entropy()\n",
    "            obs, reward, done, _, _ = self.env.step(action.item())\n",
    "     \n",
    "            if self.intrinsic_expl:\n",
    "                reward += self.intrinsic_eta * self.state_counter.intrinsic_reward(obs)\n",
    "            rnd_intrinsic = 0.0\n",
    "            if self.use_rnd:\n",
    "                with torch.no_grad():\n",
    "                    obs_rnd = obs_t.unsqueeze(0)\n",
    "                    rnd_intrinsic = self.rnd(obs_rnd).item()\n",
    "                    reward += self.intrinsic_eta * rnd_intrinsic\n",
    "            actions.append(action)\n",
    "            log_probs.append(log_prob)\n",
    "            rewards.append(torch.tensor(reward, dtype=torch.float32, device=self.device))\n",
    "            values.append(value)\n",
    "            entropies_ep.append(entropy)\n",
    "            aux_preds_list.append(aux_preds)\n",
    "            # Assign per-step targets for each aux (replace with your logic)\n",
    "            for aux in self.aux_modules:\n",
    "                if aux.name == \"cue\":\n",
    "                    aux_targets_ep[aux.name].append(initial_cue)\n",
    "                elif aux.name == \"next_obs\":\n",
    "                    aux_targets_ep[aux.name].append(torch.tensor(obs, dtype=torch.float32))\n",
    "                elif aux.name == \"confidence\":\n",
    "                    dist = Categorical(logits=logits)\n",
    "                    entropy = dist.entropy().item()\n",
    "                    confidence = 1.0 - entropy  # Or scale appropriately\n",
    "                    aux_targets_ep[aux.name].append(confidence)\n",
    "                elif aux.name == \"event\":\n",
    "                    event_flag = getattr(self.env, \"event_flag\", 0)\n",
    "                    aux_targets_ep[aux.name].append(event_flag)\n",
    "                elif aux.name == \"oracle_action\":\n",
    "                    oracle_action = getattr(self.env, \"oracle_action\", None)\n",
    "                    aux_targets_ep[aux.name].append(oracle_action)\n",
    "                else:\n",
    "                    aux_targets_ep[aux.name].append(0)  # Or np.nan\n",
    "                    \n",
    "        if self.memory is not None:\n",
    "            attn_weights = self.memory.last_attn\n",
    "        if self.use_rnd:\n",
    "            obs_batch = torch.stack([torch.tensor(np.array(o), dtype=torch.float32, device=self.device) for o in trajectory])\n",
    "            rnd_loss = self.rnd(obs_batch).mean()\n",
    "            self.rnd_optimizer.zero_grad()\n",
    "            rnd_loss.backward()\n",
    "            self.rnd_optimizer.step()\n",
    "        return {\n",
    "            \"trajectory\": trajectory,\n",
    "            \"actions\": actions,\n",
    "            \"rewards\": rewards,\n",
    "            \"log_probs\": log_probs,\n",
    "            \"values\": values,\n",
    "            \"entropies\": entropies_ep,\n",
    "            \"aux_preds\": aux_preds_list,\n",
    "            \"aux_targets\": aux_targets_ep,\n",
    "            \"initial_cue\": initial_cue,\n",
    "            \"gate_history\": gate_history,\n",
    "            \"memory_size_history\": memory_size_history,\n",
    "            \"attn_weights\": attn_weights\n",
    "        }\n",
    "\n",
    "    def learn(self, total_timesteps=2000, log_interval=100):\n",
    "        steps = 0\n",
    "        episodes = 0\n",
    "        all_returns = []\n",
    "        start_time = time.time()\n",
    "        aux_losses = []\n",
    "\n",
    "        while steps < total_timesteps:\n",
    "            episode = self.run_episode()\n",
    "            if self.reward_norm:\n",
    "                self.reward_normalizer.update([r.item() for r in episode[\"rewards\"]])\n",
    "                episode[\"rewards\"] = [torch.tensor(rn, dtype=torch.float32, device=self.device)\n",
    "                                      for rn in self.reward_normalizer.normalize([r.item() for r in episode[\"rewards\"]])]\n",
    "\n",
    "            trajectory = episode[\"trajectory\"]\n",
    "            actions = episode[\"actions\"]\n",
    "            rewards = episode[\"rewards\"]\n",
    "            log_probs = episode[\"log_probs\"]\n",
    "            values = episode[\"values\"]\n",
    "            entropies_ep = episode[\"entropies\"]\n",
    "            aux_preds = episode[\"aux_preds\"]\n",
    "            aux_targets = episode[\"aux_targets\"]\n",
    "            T = len(rewards)\n",
    "\n",
    "            rewards_t = torch.stack(rewards)\n",
    "            values_t = torch.stack(values)\n",
    "            log_probs_t = torch.stack(log_probs)\n",
    "            actions_t = torch.stack(actions)\n",
    "            last_value = 0.0\n",
    "            advantages = compute_gae(rewards_t, values_t, gamma=self.gamma, lam=self.lam, last_value=last_value)\n",
    "            returns = advantages + values_t.detach()\n",
    "\n",
    "            policy_loss = -(log_probs_t * advantages.detach()).sum()\n",
    "            value_loss = F.mse_loss(values_t, returns)\n",
    "            entropy_mean = torch.stack(entropies_ep).mean()\n",
    "            explained_var = compute_explained_variance(values_t, returns)\n",
    "\n",
    "            # ---- Multi-Aux Loss and Metrics ----\n",
    "            aux_loss_total = torch.tensor(0.0, device=self.device)\n",
    "            aux_metrics_log = {}\n",
    "            if self.aux:\n",
    "                for aux in self.aux_modules:\n",
    "                    preds = torch.stack([ap[aux.name] for ap in aux_preds])\n",
    "                    targets = torch.tensor(aux_targets[aux.name], device=self.device)\n",
    "                    # For confidence, targets might be float!\n",
    "                    if preds.dim() != targets.dim():\n",
    "                        targets = targets.squeeze(-1)\n",
    "                    loss = aux.aux_loss(preds, targets)\n",
    "                    aux_loss_total += loss\n",
    "                    # Metrics\n",
    "                    metrics = aux.aux_metrics(preds, targets)\n",
    "                    aux_metrics_log[aux.name] = metrics\n",
    "                aux_losses.append(aux_loss_total.item())\n",
    "\n",
    "            loss = policy_loss + 0.5 * value_loss + 0.1 * aux_loss_total - self.ent_coef * entropy_mean\n",
    "\n",
    "            self.optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "\n",
    "            total_reward = sum([r.item() for r in rewards])\n",
    "            self.episode_rewards.append(total_reward)\n",
    "            self.episode_lengths.append(T)\n",
    "            episodes += 1\n",
    "            steps += T\n",
    "\n",
    "            # Logging\n",
    "            if episodes % log_interval == 0 and self.verbose == 1:\n",
    "                elapsed = int(time.time() - start_time)\n",
    "                mean_rew = np.mean(self.episode_rewards[-log_interval:])\n",
    "                std_rew = np.std(self.episode_rewards[-log_interval:])\n",
    "                mean_len = np.mean(self.episode_lengths[-log_interval:])\n",
    "                fps = int(steps / (elapsed + 1e-8))\n",
    "                adv_mean = advantages.mean().item()\n",
    "                adv_std = advantages.std().item()\n",
    "                mean_entropy = entropy_mean.item()\n",
    "                mean_aux = np.mean(aux_losses[-log_interval:]) if aux_losses else 0.0\n",
    "                stats = [{\n",
    "                    \"header\":\"rollout\",\n",
    "                    \"stats\":dict(\n",
    "                        ep_len_mean=mean_len,\n",
    "                        ep_rew_mean=mean_rew,\n",
    "                        ep_rew_std=std_rew,\n",
    "                        policy_entropy=mean_entropy,\n",
    "                        advantage_mean=adv_mean,\n",
    "                        advantage_std=adv_std,\n",
    "                        aux_loss_mean=mean_aux\n",
    "                    )},{\n",
    "                    \"header\":\"time\",\n",
    "                    \"stats\":dict(\n",
    "                        fps=fps,\n",
    "                        episodes=episodes,\n",
    "                        time_elapsed=elapsed,\n",
    "                        total_timesteps=steps,\n",
    "                    )},{\n",
    "                    \"header\":\"train\",\n",
    "                    \"stats\":dict(\n",
    "                        loss=loss.item(),\n",
    "                        policy_loss=policy_loss.item(),\n",
    "                        value_loss=value_loss.item(),\n",
    "                        explained_variance=explained_var.item(),\n",
    "                        n_updates=episodes\n",
    "                    )}\n",
    "                ]\n",
    "                if len(aux_metrics_log.items()) > 0:\n",
    "                    aux_stats = {\n",
    "                        \"header\": \"aux_train\",\n",
    "                        \"stats\": {}\n",
    "                    }\n",
    "                    for aux_name, metrics in aux_metrics_log.items():\n",
    "                        for k, v in metrics.items():\n",
    "                            aux_stats[\"stats\"][f\"aux_{aux_name}_{k}\"] = v\n",
    "                    stats.append(aux_stats)\n",
    "                if self.use_rnd:\n",
    "                    mean_rnd_bonus = np.mean([self.rnd(torch.tensor(np.array(o), dtype=torch.float32, device=self.device).unsqueeze(0)).item() for o in trajectory])\n",
    "                    stats.append({\n",
    "                        \"header\": \"rnd_net_dist\",\n",
    "                        \"stats\": {\"mean_rnd_bonus\":mean_rnd_bonus}\n",
    "                    })\n",
    "                \n",
    "                print_sb3_style_log_box(stats)\n",
    "      \n",
    "        if self.verbose == 1:\n",
    "            print(f\"Training complete. Total episodes: {episodes}, total steps: {steps}\")\n",
    "\n",
    "    def predict(self, obs, deterministic=False, done=False, log_diagnostics=False):\n",
    "        obs_t = torch.tensor(obs, dtype=torch.float32, device=self.device)\n",
    "        self.trajectory.append(obs_t)\n",
    "        traj = torch.stack(self.trajectory)\n",
    "        self.policy.eval()\n",
    "        with torch.no_grad():\n",
    "            logits, _, _ = self.policy(traj, obs_t)\n",
    "            if deterministic:\n",
    "                action = torch.argmax(logits).item()\n",
    "            else:\n",
    "                dist = Categorical(logits=logits)\n",
    "                action = dist.sample().item()\n",
    "        self.policy.train()\n",
    "        if done:\n",
    "            self.reset_trajectory()\n",
    "        return action\n",
    "\n",
    "    def save(self, path=\"memoryppo.pt\"):\n",
    "        torch.save(self.policy.state_dict(), path)\n",
    "\n",
    "    def load(self, path=\"memoryppo.pt\"):\n",
    "        self.policy.load_state_dict(torch.load(path, map_location=self.device))\n",
    "\n",
    "    def evaluate(self, n_episodes=10, deterministic=False, verbose=True):\n",
    "        returns = []\n",
    "        for _ in range(n_episodes):\n",
    "            obs, _ = self.env.reset()\n",
    "            self.reset_trajectory()\n",
    "            done = False\n",
    "            total_reward = 0.0\n",
    "            while not done:\n",
    "                action = self.predict(obs, deterministic=deterministic)\n",
    "                obs, reward, done, _, _ = self.env.step(action)\n",
    "                total_reward += reward\n",
    "                if done:\n",
    "                    self.reset_trajectory()\n",
    "            returns.append(total_reward)\n",
    "        mean_return = np.mean(returns)\n",
    "        std_return = np.std(returns)\n",
    "        if verbose:\n",
    "            print(f\"Evaluation over {n_episodes} episodes: mean return {mean_return:.2f}, std {std_return:.2f}\")\n",
    "        return mean_return, std_return\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b16241c-6b9e-4a9d-b735-de607d86708a",
   "metadata": {},
   "source": [
    "# Strategic Episodic memory\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "886ef328-4372-46f1-bf0d-9302de15328a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class RNDModule(nn.Module):\n",
    "    def __init__(self, obs_dim, emb_dim=32):\n",
    "        super().__init__()\n",
    "        # Fixed random target net\n",
    "        self.target = nn.Sequential(\n",
    "            nn.Linear(obs_dim, emb_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(emb_dim, emb_dim)\n",
    "        )\n",
    "        # Predictor net\n",
    "        self.predictor = nn.Sequential(\n",
    "            nn.Linear(obs_dim, emb_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(emb_dim, emb_dim)\n",
    "        )\n",
    "        # Freeze target net\n",
    "        for p in self.target.parameters():\n",
    "            p.requires_grad = False\n",
    "\n",
    "    def forward(self, obs):\n",
    "        with torch.no_grad():\n",
    "            target_emb = self.target(obs)\n",
    "        pred_emb = self.predictor(obs)\n",
    "        # Return MSE as novelty signal\n",
    "        return F.mse_loss(pred_emb, target_emb, reduction='none').mean(dim=-1)\n",
    "        \n",
    "class RewardNormalizer:\n",
    "    def __init__(self, epsilon=1e-8):\n",
    "        self.mean = 0.0\n",
    "        self.var = 1.0\n",
    "        self.count = 1e-4\n",
    "        self.epsilon = epsilon\n",
    "\n",
    "    def update(self, rewards):\n",
    "        rewards = np.array(rewards)\n",
    "        batch_mean = rewards.mean()\n",
    "        batch_var = rewards.var()\n",
    "        batch_count = len(rewards)\n",
    "        self.mean = (self.mean * self.count + batch_mean * batch_count) / (self.count + batch_count)\n",
    "        self.var = (self.var * self.count + batch_var * batch_count) / (self.count + batch_count)\n",
    "        self.count += batch_count\n",
    "\n",
    "    def normalize(self, rewards):\n",
    "        rewards = np.array(rewards)\n",
    "        return ((rewards - self.mean) / (np.sqrt(self.var) + self.epsilon)).tolist()\n",
    "\n",
    "# ──────────────────────────────────────────────────────────────\n",
    "# 3. State Counter for Intrinsic Reward (Exploration Bonus)\n",
    "# ──────────────────────────────────────────────────────────────\n",
    "\n",
    "class StateCounter:\n",
    "    def __init__(self):\n",
    "        self.counts = defaultdict(int)\n",
    "\n",
    "    def count(self, obs):\n",
    "        key = tuple(np.round(obs, 2))\n",
    "        self.counts[key] += 1\n",
    "        return self.counts[key]\n",
    "\n",
    "    def intrinsic_reward(self, obs):\n",
    "        c = self.count(obs)\n",
    "        return 1.0 / np.sqrt(c)\n",
    "\n",
    "# ──────────────────────────────────────────────────────────────\n",
    "# 4. Generalized Advantage Estimation (GAE) and Explained Variance\n",
    "# ──────────────────────────────────────────────────────────────\n",
    "\n",
    "def compute_explained_variance(y_pred, y_true):\n",
    "    var_y = torch.var(y_true)\n",
    "    if var_y == 0:\n",
    "        return torch.tensor(0.0)\n",
    "    return 1 - torch.var(y_true - y_pred) / (var_y + 1e-8)\n",
    "\n",
    "def compute_gae(rewards, values, gamma=0.99, lam=0.95, last_value=0.0):\n",
    "    T = len(rewards)\n",
    "    advantages = torch.zeros(T, dtype=torch.float32, device=values.device)\n",
    "    gae = 0.0\n",
    "    values_ext = torch.cat([values, torch.tensor([last_value], dtype=torch.float32, device=values.device)])\n",
    "    for t in reversed(range(T)):\n",
    "        delta = rewards[t] + gamma * values_ext[t + 1] - values_ext[t]\n",
    "        gae = delta + gamma * lam * gae\n",
    "        advantages[t] = gae\n",
    "    return advantages\n",
    "\n",
    "def print_sb3_style_log_box(stats):\n",
    "    # Flatten stats for max width calculation\n",
    "    all_rows = []\n",
    "    for section in stats:\n",
    "        all_rows.append((section[\"header\"] + \"/\", None, True))\n",
    "        for k, v in section[\"stats\"].items():\n",
    "            all_rows.append((k, v, False))\n",
    "    # Compute widths\n",
    "    key_width = max(\n",
    "        len(\"    \" + k) if not is_section else len(k)\n",
    "        for k, v, is_section in all_rows\n",
    "    )\n",
    "    val_width = 10\n",
    "    box_width = 2 + key_width + 3 + val_width \n",
    "\n",
    "    def fmt_row(label, value, is_section):\n",
    "        if is_section:\n",
    "            return f\"| {label:<{key_width}}|{' ' * val_width} |\"\n",
    "        else:\n",
    "            # Format value: float = 8.3f, int = 8d, tensor fallback\n",
    "            if hasattr(value, 'item'):\n",
    "                value = value.item()\n",
    "            if isinstance(value, float):\n",
    "                s_value = f\"{value:8.3f}\"\n",
    "            elif isinstance(value, int):\n",
    "                s_value = f\"{value:8d}\"\n",
    "            else:\n",
    "                s_value = str(value)\n",
    "            s_value_centered = f\"{s_value:^{val_width}}\"\n",
    "            return f\"|    {label:<{key_width-4}} |{s_value_centered} |\"\n",
    "\n",
    "    print(\"-\" * box_width)\n",
    "    for section in stats:\n",
    "        print(fmt_row(section[\"header\"] + \"/\", None, True))\n",
    "        for k, v in section[\"stats\"].items():\n",
    "            print(fmt_row(k, v, False))\n",
    "    print(\"-\" * box_width)\n",
    "\n",
    "# ──────────────────────────────────────────────────────────────\n",
    "# 5. Memory Transformer Policy WITH MULTI-AUX SUPPORT\n",
    "# ──────────────────────────────────────────────────────────────\n",
    "\n",
    "class MemoryTransformerPolicy(nn.Module):\n",
    "    __version__     = \"1.1.0\"\n",
    "    __description__ = \"Now supports injection of custom auxiliary modules\"\n",
    "    \n",
    "    def __init__(self, obs_dim, mem_dim=32, nhead=4, aux_modules=None, **kwargs):\n",
    "        super().__init__()\n",
    "        self.mem_dim = mem_dim\n",
    "        self.embed = nn.Linear(obs_dim, mem_dim)\n",
    "        self.pos_embed = nn.Embedding(256, mem_dim)\n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=mem_dim, nhead=nhead, batch_first=True)\n",
    "        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=1)\n",
    "        self.policy_head = nn.Linear(mem_dim, 2)\n",
    "        self.value_head = nn.Linear(mem_dim, 1)\n",
    "        self.aux_modules = aux_modules if aux_modules is not None else []\n",
    "\n",
    "    def forward(self, trajectory, obs_t=None):\n",
    "        T = trajectory.shape[0]\n",
    "        x = self.embed(trajectory)\n",
    "        pos = torch.arange(T, device=trajectory.device)\n",
    "        x = x + self.pos_embed(pos)\n",
    "        x = x.unsqueeze(0)\n",
    "        x = self.transformer(x)\n",
    "        feat = x[0, -1]\n",
    "        logits = self.policy_head(feat)\n",
    "        value = self.value_head(feat)\n",
    "        aux_preds = {}\n",
    "        for aux in self.aux_modules:\n",
    "            aux_preds[aux.name] = aux.head(feat)\n",
    "        return logits, value.squeeze(-1), aux_preds\n",
    "\n",
    "# ──────────────────────────────────────────────────────────────\n",
    "# 6. Multi-Aux HER-enabled MemoryPPO Trainer (with metric logging)\n",
    "# ──────────────────────────────────────────────────────────────\n",
    "\n",
    "class ExternalMemoryPPO:\n",
    "    def __init__(\n",
    "        self, \n",
    "        policy_class, \n",
    "        env, \n",
    "        verbose=0,\n",
    "        learning_rate=1e-3, \n",
    "        gamma=0.99, \n",
    "        lam=0.95, \n",
    "        device=\"cpu\",\n",
    "        her=False,\n",
    "        reward_norm=False,\n",
    "        intrinsic_expl=True,\n",
    "        intrinsic_eta=0.01,\n",
    "        ent_coef=0.01,\n",
    "        memory=None,\n",
    "        aux_modules=None,\n",
    "        use_rnd=True, rnd_emb_dim=32, rnd_lr=1e-3,\n",
    "    ):\n",
    "        self.env = env\n",
    "        self.device = torch.device(device)\n",
    "        self.gamma = gamma\n",
    "        self.lam = lam\n",
    "        self.ent_coef=ent_coef\n",
    "        self.verbose = verbose\n",
    "        self.memory = memory\n",
    "        self.aux_modules = aux_modules if aux_modules is not None else []\n",
    "        self.aux = len(self.aux_modules) > 0\n",
    "        self.policy = policy_class(\n",
    "            obs_dim=env.observation_space.shape[0], \n",
    "            memory=memory,\n",
    "            aux_modules=self.aux_modules\n",
    "        ).to(self.device)\n",
    "        self.optimizer = optim.Adam(self.policy.parameters(), lr=learning_rate)\n",
    "        self.training_steps = 0\n",
    "        self.episode_rewards = []\n",
    "        self.episode_lengths = []\n",
    "        self.her = her\n",
    "        self.reward_norm = reward_norm\n",
    "        self.intrinsic_expl = intrinsic_expl\n",
    "        self.intrinsic_eta = intrinsic_eta\n",
    "        self.reward_normalizer = RewardNormalizer()\n",
    "        self.state_counter = StateCounter()\n",
    "        self.use_rnd = use_rnd\n",
    "        if self.use_rnd:\n",
    "            self.rnd = RNDModule(env.observation_space.shape[0], emb_dim=rnd_emb_dim).to(self.device)\n",
    "            self.rnd_optimizer = torch.optim.Adam(self.rnd.predictor.parameters(), lr=rnd_lr)\n",
    "        self.trajectory = []\n",
    "\n",
    "    def reset_trajectory(self):\n",
    "        self.trajectory = []\n",
    "\n",
    "    def run_episode(self, her_target=None):\n",
    "        obs, _ = self.env.reset()\n",
    "        done = False\n",
    "        trajectory = []\n",
    "        rewards = []\n",
    "        actions = []\n",
    "        log_probs = []\n",
    "        values = []\n",
    "        entropies_ep = []\n",
    "        aux_preds_list = []\n",
    "        # for memory context\n",
    "        rewards_float = []\n",
    "        actions_int = []\n",
    "    \n",
    "        while not done:\n",
    "            obs_t = torch.tensor(obs, dtype=torch.float32, device=self.device)\n",
    "            trajectory.append(obs_t)\n",
    "            # Store actions/rewards for memory context\n",
    "            if len(actions) > 0:\n",
    "                actions_int.append(actions[-1].item())\n",
    "                rewards_float.append(rewards[-1].item())\n",
    "            else:\n",
    "                actions_int.append(0)\n",
    "                rewards_float.append(0.0)\n",
    "            traj = torch.stack(trajectory)\n",
    "            logits, value, aux_preds = self.policy(traj, obs_t, actions=torch.tensor(actions_int, device=self.device) if actions_int else None, rewards=torch.tensor(rewards_float, device=self.device) if rewards_float else None)\n",
    "            dist = Categorical(logits=logits)\n",
    "            action = dist.sample()\n",
    "            log_prob = dist.log_prob(action)\n",
    "            entropy = dist.entropy()\n",
    "            obs, reward, done, _, _ = self.env.step(action.item())\n",
    "            actions.append(action)\n",
    "            log_probs.append(log_prob)\n",
    "            rewards.append(torch.tensor(reward, dtype=torch.float32, device=self.device))\n",
    "            values.append(value)\n",
    "            entropies_ep.append(entropy)\n",
    "            aux_preds_list.append(aux_preds)\n",
    "            if self.aux:\n",
    "                # Assign per-step targets for each aux (replace with your logic)\n",
    "                for aux in self.aux_modules:\n",
    "                    if aux.name == \"cue\":\n",
    "                        aux_targets_ep[aux.name].append(initial_cue)\n",
    "                    elif aux.name == \"next_obs\":\n",
    "                        aux_targets_ep[aux.name].append(torch.tensor(obs, dtype=torch.float32))\n",
    "                    elif aux.name == \"confidence\":\n",
    "                        dist = Categorical(logits=logits)\n",
    "                        entropy = dist.entropy().item()\n",
    "                        confidence = 1.0 - entropy  # Or scale appropriately\n",
    "                        aux_targets_ep[aux.name].append(confidence)\n",
    "                    elif aux.name == \"event\":\n",
    "                        event_flag = getattr(self.env, \"event_flag\", 0)\n",
    "                        aux_targets_ep[aux.name].append(event_flag)\n",
    "                    elif aux.name == \"oracle_action\":\n",
    "                        oracle_action = getattr(self.env, \"oracle_action\", None)\n",
    "                        aux_targets_ep[aux.name].append(oracle_action)\n",
    "                    else:\n",
    "                        aux_targets_ep[aux.name].append(0)  # Or np.nan\n",
    "                        \n",
    "            if self.memory is not None:\n",
    "                attn_weights = self.memory.last_attn\n",
    "            if self.use_rnd:\n",
    "                obs_batch = torch.stack([torch.tensor(np.array(o), dtype=torch.float32, device=self.device) for o in trajectory])\n",
    "                rnd_loss = self.rnd(obs_batch).mean()\n",
    "                self.rnd_optimizer.zero_grad()\n",
    "                rnd_loss.backward()\n",
    "                self.rnd_optimizer.step()\n",
    "            return {\n",
    "                \"trajectory\": trajectory,\n",
    "                \"actions\": actions,\n",
    "                \"rewards\": rewards,\n",
    "                \"log_probs\": log_probs,\n",
    "                \"values\": values,\n",
    "                \"entropies\": entropies_ep,\n",
    "                \"aux_preds\": aux_preds_list,\n",
    "                \"aux_targets\": aux_targets_ep,\n",
    "                \"initial_cue\": initial_cue,\n",
    "                \"gate_history\": gate_history,\n",
    "                \"memory_size_history\": memory_size_history,\n",
    "                \"attn_weights\": attn_weights\n",
    "            }\n",
    "\n",
    "    def learn(self, total_timesteps=2000, log_interval=100):\n",
    "        steps = 0\n",
    "        episodes = 0\n",
    "        all_returns = []\n",
    "        start_time = time.time()\n",
    "        aux_losses = []\n",
    "\n",
    "        while steps < total_timesteps:\n",
    "            episode = self.run_episode()\n",
    "            if self.reward_norm:\n",
    "                self.reward_normalizer.update([r.item() for r in episode[\"rewards\"]])\n",
    "                episode[\"rewards\"] = [torch.tensor(rn, dtype=torch.float32, device=self.device)\n",
    "                                      for rn in self.reward_normalizer.normalize([r.item() for r in episode[\"rewards\"]])]\n",
    "\n",
    "            trajectory = episode[\"trajectory\"]\n",
    "            actions = episode[\"actions\"]\n",
    "            rewards = episode[\"rewards\"]\n",
    "            log_probs = episode[\"log_probs\"]\n",
    "            values = episode[\"values\"]\n",
    "            entropies_ep = episode[\"entropies\"]\n",
    "            aux_preds = episode[\"aux_preds\"]\n",
    "            aux_targets = episode[\"aux_targets\"]\n",
    "            T = len(rewards)\n",
    "\n",
    "            rewards_t = torch.stack(rewards)\n",
    "            values_t = torch.stack(values)\n",
    "            log_probs_t = torch.stack(log_probs)\n",
    "            actions_t = torch.stack(actions)\n",
    "            last_value = 0.0\n",
    "            advantages = compute_gae(rewards_t, values_t, gamma=self.gamma, lam=self.lam, last_value=last_value)\n",
    "            returns = advantages + values_t.detach()\n",
    "\n",
    "            policy_loss = -(log_probs_t * advantages.detach()).sum()\n",
    "            value_loss = F.mse_loss(values_t, returns)\n",
    "            entropy_mean = torch.stack(entropies_ep).mean()\n",
    "            explained_var = compute_explained_variance(values_t, returns)\n",
    "\n",
    "            # ---- Multi-Aux Loss and Metrics ----\n",
    "            aux_loss_total = torch.tensor(0.0, device=self.device)\n",
    "            aux_metrics_log = {}\n",
    "            if self.aux:\n",
    "                for aux in self.aux_modules:\n",
    "                    preds = torch.stack([ap[aux.name] for ap in aux_preds])\n",
    "                    targets = torch.tensor(aux_targets[aux.name], device=self.device)\n",
    "                    # For confidence, targets might be float!\n",
    "                    if preds.dim() != targets.dim():\n",
    "                        targets = targets.squeeze(-1)\n",
    "                    loss = aux.aux_loss(preds, targets)\n",
    "                    aux_loss_total += loss\n",
    "                    # Metrics\n",
    "                    metrics = aux.aux_metrics(preds, targets)\n",
    "                    aux_metrics_log[aux.name] = metrics\n",
    "                aux_losses.append(aux_loss_total.item())\n",
    "\n",
    "            loss = policy_loss + 0.5 * value_loss + 0.1 * aux_loss_total - self.ent_coef * entropy_mean\n",
    "\n",
    "            self.optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "\n",
    "            total_reward = sum([r.item() for r in rewards])\n",
    "            self.episode_rewards.append(total_reward)\n",
    "            self.episode_lengths.append(T)\n",
    "            episodes += 1\n",
    "            steps += T\n",
    "\n",
    "            # Logging\n",
    "            if episodes % log_interval == 0 and self.verbose == 1:\n",
    "                elapsed = int(time.time() - start_time)\n",
    "                mean_rew = np.mean(self.episode_rewards[-log_interval:])\n",
    "                std_rew = np.std(self.episode_rewards[-log_interval:])\n",
    "                mean_len = np.mean(self.episode_lengths[-log_interval:])\n",
    "                fps = int(steps / (elapsed + 1e-8))\n",
    "                adv_mean = advantages.mean().item()\n",
    "                adv_std = advantages.std().item()\n",
    "                mean_entropy = entropy_mean.item()\n",
    "                mean_aux = np.mean(aux_losses[-log_interval:]) if aux_losses else 0.0\n",
    "                stats = [{\n",
    "                    \"header\":\"rollout\",\n",
    "                    \"stats\":dict(\n",
    "                        ep_len_mean=mean_len,\n",
    "                        ep_rew_mean=mean_rew,\n",
    "                        ep_rew_std=std_rew,\n",
    "                        policy_entropy=mean_entropy,\n",
    "                        advantage_mean=adv_mean,\n",
    "                        advantage_std=adv_std,\n",
    "                        aux_loss_mean=mean_aux\n",
    "                    )},{\n",
    "                    \"header\":\"time\",\n",
    "                    \"stats\":dict(\n",
    "                        fps=fps,\n",
    "                        episodes=episodes,\n",
    "                        time_elapsed=elapsed,\n",
    "                        total_timesteps=steps,\n",
    "                    )},{\n",
    "                    \"header\":\"train\",\n",
    "                    \"stats\":dict(\n",
    "                        loss=loss.item(),\n",
    "                        policy_loss=policy_loss.item(),\n",
    "                        value_loss=value_loss.item(),\n",
    "                        explained_variance=explained_var.item(),\n",
    "                        n_updates=episodes\n",
    "                    )}\n",
    "                ]\n",
    "                if len(aux_metrics_log.items()) > 0:\n",
    "                    aux_stats = {\n",
    "                        \"header\": \"aux_train\",\n",
    "                        \"stats\": {}\n",
    "                    }\n",
    "                    for aux_name, metrics in aux_metrics_log.items():\n",
    "                        for k, v in metrics.items():\n",
    "                            aux_stats[\"stats\"][f\"aux_{aux_name}_{k}\"] = v\n",
    "                    stats.append(aux_stats)\n",
    "                if self.use_rnd:\n",
    "                    mean_rnd_bonus = np.mean([self.rnd(torch.tensor(np.array(o), dtype=torch.float32, device=self.device).unsqueeze(0)).item() for o in trajectory])\n",
    "                    stats.append({\n",
    "                        \"header\": \"rnd_net_dist\",\n",
    "                        \"stats\": {\"mean_rnd_bonus\":mean_rnd_bonus}\n",
    "                    })\n",
    "                \n",
    "                print_sb3_style_log_box(stats)\n",
    "      \n",
    "        if self.verbose == 1:\n",
    "            print(f\"Training complete. Total episodes: {episodes}, total steps: {steps}\")\n",
    "\n",
    "    def predict(self, obs, deterministic=False, done=False, log_diagnostics=False):\n",
    "        obs_t = torch.tensor(obs, dtype=torch.float32, device=self.device)\n",
    "        self.trajectory.append(obs_t)\n",
    "        traj = torch.stack(self.trajectory)\n",
    "        self.policy.eval()\n",
    "        with torch.no_grad():\n",
    "            logits, _, _ = self.policy(traj, obs_t)\n",
    "            if deterministic:\n",
    "                action = torch.argmax(logits).item()\n",
    "            else:\n",
    "                dist = Categorical(logits=logits)\n",
    "                action = dist.sample().item()\n",
    "        self.policy.train()\n",
    "        if done:\n",
    "            self.reset_trajectory()\n",
    "        return action\n",
    "\n",
    "    def save(self, path=\"memoryppo.pt\"):\n",
    "        torch.save(self.policy.state_dict(), path)\n",
    "\n",
    "    def load(self, path=\"memoryppo.pt\"):\n",
    "        self.policy.load_state_dict(torch.load(path, map_location=self.device))\n",
    "\n",
    "    def evaluate(self, n_episodes=10, deterministic=False, verbose=True):\n",
    "        returns = []\n",
    "        for _ in range(n_episodes):\n",
    "            obs, _ = self.env.reset()\n",
    "            self.reset_trajectory()\n",
    "            done = False\n",
    "            total_reward = 0.0\n",
    "            while not done:\n",
    "                action = self.predict(obs, deterministic=deterministic)\n",
    "                obs, reward, done, _, _ = self.env.step(action)\n",
    "                total_reward += reward\n",
    "                if done:\n",
    "                    self.reset_trajectory()\n",
    "            returns.append(total_reward)\n",
    "        mean_return = np.mean(returns)\n",
    "        std_return = np.std(returns)\n",
    "        if verbose:\n",
    "            print(f\"Evaluation over {n_episodes} episodes: mean return {mean_return:.2f}, std {std_return:.2f}\")\n",
    "        return mean_return, std_return\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "68026bd6-e438-4faf-87a4-8b32e9e1f02f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "\n",
    "class StrategicMemoryBuffer(nn.Module):\n",
    "    def __init__(self, obs_dim, action_dim, mem_dim=32, max_entries=100, device='cpu'):\n",
    "        super().__init__()\n",
    "        self.obs_dim = obs_dim\n",
    "        self.action_dim = action_dim\n",
    "        self.mem_dim = mem_dim\n",
    "        self.max_entries = max_entries\n",
    "        self.device = device\n",
    "        self.reset()\n",
    "        # Only need this! (input_proj to mem_dim, encoder uses mem_dim)\n",
    "        self.embedding_proj = nn.Linear(obs_dim + action_dim + 1, mem_dim)\n",
    "        self.encoder = nn.TransformerEncoder(\n",
    "            nn.TransformerEncoderLayer(d_model=mem_dim, nhead=2, batch_first=True),\n",
    "            num_layers=1\n",
    "        )\n",
    "\n",
    "    def reset(self):\n",
    "        self.entries = []\n",
    "\n",
    "    def add_entry(self, trajectory, outcome):\n",
    "        traj = torch.tensor(\n",
    "            [np.concatenate([obs, [action], [reward]]) for obs, action, reward in trajectory],\n",
    "            dtype=torch.float32, device=self.device\n",
    "        )  # [T, obs_dim+action_dim+1]\n",
    "        traj_proj = self.embedding_proj(traj)  # [T, mem_dim]\n",
    "        mem_embed = self.encoder(traj_proj.unsqueeze(0)).mean(dim=1).squeeze(0)  # [mem_dim]\n",
    "        entry = {\n",
    "            'trajectory': trajectory,\n",
    "            'outcome': outcome,\n",
    "            'embedding': mem_embed.detach()\n",
    "        }\n",
    "        self.entries.append(entry)\n",
    "        if len(self.entries) > self.max_entries:\n",
    "            self.entries = self.entries[-self.max_entries:]\n",
    "\n",
    "    def retrieve(self, context_trajectory):\n",
    "        if len(self.entries) == 0:\n",
    "            return torch.zeros(self.mem_dim, device=self.device), None\n",
    "        traj = torch.tensor(\n",
    "            [np.concatenate([obs, [action], [reward]]) for obs, action, reward in context_trajectory],\n",
    "            dtype=torch.float32, device=self.device\n",
    "        )\n",
    "        traj_proj = self.embedding_proj(traj)\n",
    "        context_embed = self.encoder(traj_proj.unsqueeze(0)).mean(dim=1).squeeze(0)  # [mem_dim]\n",
    "        mem_embeddings = torch.stack([e['embedding'] for e in self.entries])  # [N, mem_dim]\n",
    "        attn_logits = torch.matmul(mem_embeddings, context_embed)\n",
    "        attn = torch.softmax(attn_logits, dim=0)\n",
    "        mem_readout = (attn.unsqueeze(1) * mem_embeddings).sum(dim=0)\n",
    "        return mem_readout, attn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "183c6d59-ffd7-4e74-8bc4-0940350df798",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MemoryTransformerPolicy(nn.Module):\n",
    "    __version__     = \"1.2.0\"\n",
    "    __description__ = \"Hint-free memory retrieval using strategic memory buffer\"\n",
    "\n",
    "    def __init__(self, obs_dim, mem_dim=32, nhead=4, memory=None, aux_modules=None, **kwargs):\n",
    "        super().__init__()\n",
    "        self.mem_dim = mem_dim\n",
    "        self.embed = nn.Linear(obs_dim, mem_dim)\n",
    "        self.pos_embed = nn.Embedding(256, mem_dim)\n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=mem_dim, nhead=nhead, batch_first=True)\n",
    "        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=1)\n",
    "        # Memory will be concatenated, so double the input size to heads\n",
    "        self.policy_head = nn.Linear(mem_dim + mem_dim, 2)\n",
    "        self.value_head = nn.Linear(mem_dim + mem_dim, 1)\n",
    "        self.aux_modules = aux_modules if aux_modules is not None else []\n",
    "        self.memory = memory\n",
    "\n",
    "    def forward(self, trajectory, obs_t=None, actions=None, rewards=None):\n",
    "        T = trajectory.shape[0]\n",
    "        x = self.embed(trajectory)\n",
    "        pos = torch.arange(T, device=trajectory.device)\n",
    "        x = x + self.pos_embed(pos)\n",
    "        x = x.unsqueeze(0)\n",
    "        x = self.transformer(x)\n",
    "        feat = x[0, -1]  # [mem_dim]\n",
    "    \n",
    "        # ---- PATCH: robust history alignment ----\n",
    "        if self.memory is not None and actions is not None and rewards is not None:\n",
    "            actions_list = actions.tolist()\n",
    "            rewards_list = rewards.tolist()\n",
    "            # Pad if actions/rewards are shorter than T (common on first step)\n",
    "            if len(actions_list) < T:\n",
    "                actions_list = [0] * (T - len(actions_list)) + actions_list\n",
    "            if len(rewards_list) < T:\n",
    "                rewards_list = [0.0] * (T - len(rewards_list)) + rewards_list\n",
    "            context_traj = []\n",
    "            for i in range(T):\n",
    "                context_traj.append((\n",
    "                    trajectory[i].cpu().numpy(),\n",
    "                    actions_list[i],\n",
    "                    rewards_list[i]\n",
    "                ))\n",
    "            mem_readout, attn = self.memory.retrieve(context_traj)\n",
    "        else:\n",
    "            mem_readout = torch.zeros_like(feat)\n",
    "        final_feat = torch.cat([feat, mem_readout], dim=-1)\n",
    "        logits = self.policy_head(final_feat)\n",
    "        value = self.value_head(final_feat)\n",
    "        aux_preds = {}\n",
    "        for aux in self.aux_modules:\n",
    "            aux_preds[aux.name] = aux.head(final_feat)\n",
    "        return logits, value.squeeze(-1), aux_preds\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "605e4e1f-1d05-4796-88db-7e75e4e3e190",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class ExternalMemoryPPO:\n",
    "    def __init__(\n",
    "        self, \n",
    "        policy_class, \n",
    "        env, \n",
    "        verbose=0,\n",
    "        learning_rate=1e-3, \n",
    "        gamma=0.99, \n",
    "        lam=0.95, \n",
    "        device=\"cpu\",\n",
    "        her=False,\n",
    "        reward_norm=False,\n",
    "        intrinsic_expl=True,\n",
    "        intrinsic_eta=0.01,\n",
    "        ent_coef=0.01,\n",
    "        memory=None,\n",
    "        aux_modules=None,\n",
    "        use_rnd=True, rnd_emb_dim=32, rnd_lr=1e-3,\n",
    "    ):\n",
    "        self.env = env\n",
    "        self.device = torch.device(device)\n",
    "        self.gamma = gamma\n",
    "        self.lam = lam\n",
    "        self.ent_coef=ent_coef\n",
    "        self.verbose = verbose\n",
    "        self.memory = memory\n",
    "        self.aux_modules = aux_modules if aux_modules is not None else []\n",
    "        self.aux = len(self.aux_modules) > 0\n",
    "        self.policy = policy_class(\n",
    "            obs_dim=env.observation_space.shape[0], \n",
    "            memory=memory,\n",
    "            aux_modules=self.aux_modules\n",
    "        ).to(self.device)\n",
    "        self.optimizer = optim.Adam(self.policy.parameters(), lr=learning_rate)\n",
    "        self.training_steps = 0\n",
    "        self.episode_rewards = []\n",
    "        self.episode_lengths = []\n",
    "        self.her = her\n",
    "        self.reward_norm = reward_norm\n",
    "        self.intrinsic_expl = intrinsic_expl\n",
    "        self.intrinsic_eta = intrinsic_eta\n",
    "        self.reward_normalizer = RewardNormalizer()\n",
    "        self.state_counter = StateCounter()\n",
    "        self.use_rnd = use_rnd\n",
    "        if self.use_rnd:\n",
    "            self.rnd = RNDModule(env.observation_space.shape[0], emb_dim=rnd_emb_dim).to(self.device)\n",
    "            self.rnd_optimizer = torch.optim.Adam(self.rnd.predictor.parameters(), lr=rnd_lr)\n",
    "        self.trajectory = []\n",
    "\n",
    "    def reset_trajectory(self):\n",
    "        self.trajectory = []\n",
    "\n",
    "    def run_episode(self, her_target=None):\n",
    "        obs, _ = self.env.reset()\n",
    "        if her_target is not None:\n",
    "            obs[0] = her_target\n",
    "        # self.memory.reset()  # REMOVE unless you want to clear between episodes\n",
    "    \n",
    "        done = False\n",
    "        trajectory = []\n",
    "        actions = []\n",
    "        rewards = []\n",
    "        log_probs = []\n",
    "        values = []\n",
    "        entropies_ep = []\n",
    "        aux_preds_list = []\n",
    "        gate_history = []\n",
    "        memory_size_history = []\n",
    "        attn_weights = None\n",
    "        initial_cue = int(obs[0])\n",
    "        aux_targets_ep = {aux.name: [] for aux in self.aux_modules}\n",
    "    \n",
    "        # For memory context: action/reward at t matches obs_t\n",
    "        context_traj = []  # (obs, action, reward) for the memory buffer\n",
    "    \n",
    "        while not done:\n",
    "            obs_t = torch.tensor(obs, dtype=torch.float32, device=self.device)\n",
    "            trajectory.append(obs_t)\n",
    "            traj = torch.stack(trajectory)\n",
    "            # For correct context, fill with 0 if first step (no previous action/reward)\n",
    "            action_for_mem = actions[-1].item() if len(actions) > 0 else 0\n",
    "            reward_for_mem = rewards[-1].item() if len(rewards) > 0 else 0.0\n",
    "            context_traj.append((obs_t.cpu().numpy(), action_for_mem, reward_for_mem))\n",
    "    \n",
    "            logits, value, aux_preds = self.policy(\n",
    "                traj, obs_t,\n",
    "                actions=torch.tensor([a.item() for a in actions], device=self.device) if actions else None,\n",
    "                rewards=torch.tensor([r.item() for r in rewards], device=self.device) if rewards else None\n",
    "            )\n",
    "            dist = Categorical(logits=logits)\n",
    "            action = dist.sample()\n",
    "            log_prob = dist.log_prob(action)\n",
    "            entropy = dist.entropy()\n",
    "            obs, reward, done, _, _ = self.env.step(action.item())\n",
    "    \n",
    "            if self.intrinsic_expl:\n",
    "                reward += self.intrinsic_eta * self.state_counter.intrinsic_reward(obs)\n",
    "            rnd_intrinsic = 0.0\n",
    "            if self.use_rnd:\n",
    "                with torch.no_grad():\n",
    "                    obs_rnd = obs_t.unsqueeze(0)\n",
    "                    rnd_intrinsic = self.rnd(obs_rnd).item()\n",
    "                    reward += self.intrinsic_eta * rnd_intrinsic\n",
    "    \n",
    "            actions.append(action)\n",
    "            log_probs.append(log_prob)\n",
    "            rewards.append(torch.tensor(reward, dtype=torch.float32, device=self.device))\n",
    "            values.append(value)\n",
    "            entropies_ep.append(entropy)\n",
    "            aux_preds_list.append(aux_preds)\n",
    "    \n",
    "            # (aux logic stays the same)\n",
    "            for aux in self.aux_modules:\n",
    "                if aux.name == \"cue\":\n",
    "                    aux_targets_ep[aux.name].append(initial_cue)\n",
    "                elif aux.name == \"next_obs\":\n",
    "                    aux_targets_ep[aux.name].append(torch.tensor(obs, dtype=torch.float32))\n",
    "                elif aux.name == \"confidence\":\n",
    "                    dist = Categorical(logits=logits)\n",
    "                    entropy = dist.entropy().item()\n",
    "                    confidence = 1.0 - entropy  # Or scale appropriately\n",
    "                    aux_targets_ep[aux.name].append(confidence)\n",
    "                elif aux.name == \"event\":\n",
    "                    event_flag = getattr(self.env, \"event_flag\", 0)\n",
    "                    aux_targets_ep[aux.name].append(event_flag)\n",
    "                elif aux.name == \"oracle_action\":\n",
    "                    oracle_action = getattr(self.env, \"oracle_action\", None)\n",
    "                    aux_targets_ep[aux.name].append(oracle_action)\n",
    "                else:\n",
    "                    aux_targets_ep[aux.name].append(0)\n",
    "    \n",
    "        # ----- WRITE TO MEMORY: full episode only -----\n",
    "        if self.memory is not None:\n",
    "            # Use the full episode context (obs, action, reward) tuples\n",
    "            # For strict \"no hint\", do NOT provide outcome label/score except as total reward (which agent already receives)\n",
    "            outcome = sum([r.item() for r in rewards])\n",
    "            self.memory.add_entry(context_traj, outcome)\n",
    "    \n",
    "        if self.memory is not None and hasattr(self.memory, 'last_attn'):\n",
    "            attn_weights = self.memory.last_attn\n",
    "        if self.use_rnd:\n",
    "            obs_batch = torch.stack([torch.tensor(np.array(o), dtype=torch.float32, device=self.device) for o in trajectory])\n",
    "            rnd_loss = self.rnd(obs_batch).mean()\n",
    "            self.rnd_optimizer.zero_grad()\n",
    "            rnd_loss.backward()\n",
    "            self.rnd_optimizer.step()\n",
    "    \n",
    "        return {\n",
    "            \"trajectory\": trajectory,\n",
    "            \"actions\": actions,\n",
    "            \"rewards\": rewards,\n",
    "            \"log_probs\": log_probs,\n",
    "            \"values\": values,\n",
    "            \"entropies\": entropies_ep,\n",
    "            \"aux_preds\": aux_preds_list,\n",
    "            \"aux_targets\": aux_targets_ep,\n",
    "            \"initial_cue\": initial_cue,\n",
    "            \"gate_history\": gate_history,\n",
    "            \"memory_size_history\": memory_size_history,\n",
    "            \"attn_weights\": attn_weights\n",
    "        }\n",
    "\n",
    "\n",
    "    def learn(self, total_timesteps=2000, log_interval=100):\n",
    "        steps = 0\n",
    "        episodes = 0\n",
    "        all_returns = []\n",
    "        start_time = time.time()\n",
    "        aux_losses = []\n",
    "\n",
    "        while steps < total_timesteps:\n",
    "            episode = self.run_episode()\n",
    "            if self.reward_norm:\n",
    "                self.reward_normalizer.update([r.item() for r in episode[\"rewards\"]])\n",
    "                episode[\"rewards\"] = [torch.tensor(rn, dtype=torch.float32, device=self.device)\n",
    "                                      for rn in self.reward_normalizer.normalize([r.item() for r in episode[\"rewards\"]])]\n",
    "\n",
    "            trajectory = episode[\"trajectory\"]\n",
    "            actions = episode[\"actions\"]\n",
    "            rewards = episode[\"rewards\"]\n",
    "            log_probs = episode[\"log_probs\"]\n",
    "            values = episode[\"values\"]\n",
    "            entropies_ep = episode[\"entropies\"]\n",
    "            aux_preds = episode[\"aux_preds\"]\n",
    "            aux_targets = episode[\"aux_targets\"]\n",
    "            T = len(rewards)\n",
    "\n",
    "            rewards_t = torch.stack(rewards)\n",
    "            values_t = torch.stack(values)\n",
    "            log_probs_t = torch.stack(log_probs)\n",
    "            actions_t = torch.stack(actions)\n",
    "            last_value = 0.0\n",
    "            advantages = compute_gae(rewards_t, values_t, gamma=self.gamma, lam=self.lam, last_value=last_value)\n",
    "            returns = advantages + values_t.detach()\n",
    "\n",
    "            policy_loss = -(log_probs_t * advantages.detach()).sum()\n",
    "            value_loss = F.mse_loss(values_t, returns)\n",
    "            entropy_mean = torch.stack(entropies_ep).mean()\n",
    "            explained_var = compute_explained_variance(values_t, returns)\n",
    "\n",
    "            # ---- Multi-Aux Loss and Metrics ----\n",
    "            aux_loss_total = torch.tensor(0.0, device=self.device)\n",
    "            aux_metrics_log = {}\n",
    "            if self.aux:\n",
    "                for aux in self.aux_modules:\n",
    "                    preds = torch.stack([ap[aux.name] for ap in aux_preds])\n",
    "                    targets = torch.tensor(aux_targets[aux.name], device=self.device)\n",
    "                    # For confidence, targets might be float!\n",
    "                    if preds.dim() != targets.dim():\n",
    "                        targets = targets.squeeze(-1)\n",
    "                    loss = aux.aux_loss(preds, targets)\n",
    "                    aux_loss_total += loss\n",
    "                    # Metrics\n",
    "                    metrics = aux.aux_metrics(preds, targets)\n",
    "                    aux_metrics_log[aux.name] = metrics\n",
    "                aux_losses.append(aux_loss_total.item())\n",
    "\n",
    "            loss = policy_loss + 0.5 * value_loss + 0.1 * aux_loss_total - self.ent_coef * entropy_mean\n",
    "\n",
    "            self.optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "\n",
    "            total_reward = sum([r.item() for r in rewards])\n",
    "            self.episode_rewards.append(total_reward)\n",
    "            self.episode_lengths.append(T)\n",
    "            episodes += 1\n",
    "            steps += T\n",
    "\n",
    "            # Logging\n",
    "            if episodes % log_interval == 0 and self.verbose == 1:\n",
    "                elapsed = int(time.time() - start_time)\n",
    "                mean_rew = np.mean(self.episode_rewards[-log_interval:])\n",
    "                std_rew = np.std(self.episode_rewards[-log_interval:])\n",
    "                mean_len = np.mean(self.episode_lengths[-log_interval:])\n",
    "                fps = int(steps / (elapsed + 1e-8))\n",
    "                adv_mean = advantages.mean().item()\n",
    "                adv_std = advantages.std().item()\n",
    "                mean_entropy = entropy_mean.item()\n",
    "                mean_aux = np.mean(aux_losses[-log_interval:]) if aux_losses else 0.0\n",
    "                stats = [{\n",
    "                    \"header\":\"rollout\",\n",
    "                    \"stats\":dict(\n",
    "                        ep_len_mean=mean_len,\n",
    "                        ep_rew_mean=mean_rew,\n",
    "                        ep_rew_std=std_rew,\n",
    "                        policy_entropy=mean_entropy,\n",
    "                        advantage_mean=adv_mean,\n",
    "                        advantage_std=adv_std,\n",
    "                        aux_loss_mean=mean_aux\n",
    "                    )},{\n",
    "                    \"header\":\"time\",\n",
    "                    \"stats\":dict(\n",
    "                        fps=fps,\n",
    "                        episodes=episodes,\n",
    "                        time_elapsed=elapsed,\n",
    "                        total_timesteps=steps,\n",
    "                    )},{\n",
    "                    \"header\":\"train\",\n",
    "                    \"stats\":dict(\n",
    "                        loss=loss.item(),\n",
    "                        policy_loss=policy_loss.item(),\n",
    "                        value_loss=value_loss.item(),\n",
    "                        explained_variance=explained_var.item(),\n",
    "                        n_updates=episodes\n",
    "                    )}\n",
    "                ]\n",
    "                if len(aux_metrics_log.items()) > 0:\n",
    "                    aux_stats = {\n",
    "                        \"header\": \"aux_train\",\n",
    "                        \"stats\": {}\n",
    "                    }\n",
    "                    for aux_name, metrics in aux_metrics_log.items():\n",
    "                        for k, v in metrics.items():\n",
    "                            aux_stats[\"stats\"][f\"aux_{aux_name}_{k}\"] = v\n",
    "                    stats.append(aux_stats)\n",
    "                if self.use_rnd:\n",
    "                    mean_rnd_bonus = np.mean([self.rnd(torch.tensor(np.array(o), dtype=torch.float32, device=self.device).unsqueeze(0)).item() for o in trajectory])\n",
    "                    stats.append({\n",
    "                        \"header\": \"rnd_net_dist\",\n",
    "                        \"stats\": {\"mean_rnd_bonus\":mean_rnd_bonus}\n",
    "                    })\n",
    "                \n",
    "                print_sb3_style_log_box(stats)\n",
    "      \n",
    "        if self.verbose == 1:\n",
    "            print(f\"Training complete. Total episodes: {episodes}, total steps: {steps}\")\n",
    "\n",
    "    def predict(self, obs, deterministic=False, done=False, reward=0.0):\n",
    "        obs_t = torch.tensor(obs, dtype=torch.float32, device=self.device)\n",
    "        # --- Track trajectory buffer ---\n",
    "        if not hasattr(self, \"trajectory_buffer\") or self.trajectory_buffer is None:\n",
    "            self.trajectory_buffer = []\n",
    "        if len(self.trajectory_buffer) == 0:\n",
    "            # Initial step: action/reward are dummy\n",
    "            self.trajectory_buffer.append((obs_t.cpu().numpy(), 0, 0.0))\n",
    "        else:\n",
    "            # Last taken action, last received reward\n",
    "            last_action = self.last_action if hasattr(self, \"last_action\") else 0\n",
    "            last_reward = self.last_reward if hasattr(self, \"last_reward\") else 0.0\n",
    "            self.trajectory_buffer.append((obs_t.cpu().numpy(), last_action, last_reward))\n",
    "        # Build context for memory\n",
    "        context_traj = self.trajectory_buffer.copy()\n",
    "        # Actions/rewards arrays for input to policy\n",
    "        actions_int = [a for _, a, _ in context_traj]\n",
    "        rewards_float = [r for _, _, r in context_traj]\n",
    "        obs_stack = torch.stack([torch.tensor(o, dtype=torch.float32, device=self.device) for o, _, _ in context_traj])\n",
    "        logits, _, _ = self.policy(\n",
    "            obs_stack, obs_t,\n",
    "            actions=torch.tensor(actions_int, device=self.device),\n",
    "            rewards=torch.tensor(rewards_float, device=self.device)\n",
    "        )\n",
    "        if deterministic:\n",
    "            action = torch.argmax(logits).item()\n",
    "        else:\n",
    "            dist = Categorical(logits=logits)\n",
    "            action = dist.sample().item()\n",
    "        self.last_action = action\n",
    "        self.last_reward = reward\n",
    "        if done:\n",
    "            self.trajectory_buffer = []\n",
    "        return action\n",
    "\n",
    "    def save(self, path=\"memoryppo.pt\"):\n",
    "        torch.save(self.policy.state_dict(), path)\n",
    "\n",
    "    def load(self, path=\"memoryppo.pt\"):\n",
    "        self.policy.load_state_dict(torch.load(path, map_location=self.device))\n",
    "\n",
    "    def evaluate(self, n_episodes=10, deterministic=False, verbose=True):\n",
    "        returns = []\n",
    "        for _ in range(n_episodes):\n",
    "            obs, _ = self.env.reset()\n",
    "            self.trajectory_buffer = []\n",
    "            done = False\n",
    "            total_reward = 0.0\n",
    "            last_reward = 0.0\n",
    "            while not done:\n",
    "                action = self.predict(obs, deterministic=deterministic, reward=last_reward)\n",
    "                obs, reward, done, _, _ = self.env.step(action)\n",
    "                total_reward += reward\n",
    "                last_reward = reward\n",
    "                if done:\n",
    "                    self.trajectory_buffer = []\n",
    "            returns.append(total_reward)\n",
    "        mean_return = np.mean(returns)\n",
    "        std_return = np.std(returns)\n",
    "        if verbose:\n",
    "            print(f\"Evaluation over {n_episodes} episodes: mean return {mean_return:.2f}, std {std_return:.2f}\")\n",
    "        return mean_return, std_return\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "6bcfdaf3-5b3a-4954-9efd-06dce7ac9b19",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        |   64.000  |\n",
      "|    ep_rew_mean        |   -0.175  |\n",
      "|    ep_rew_std         |    0.983  |\n",
      "|    policy_entropy     |    0.245  |\n",
      "|    advantage_mean     |    0.339  |\n",
      "|    advantage_std      |    0.322  |\n",
      "|    aux_loss_mean      |    0.000  |\n",
      "| time/                 |           |\n",
      "|    fps                |      168  |\n",
      "|    episodes           |      100  |\n",
      "|    time_elapsed       |       38  |\n",
      "|    total_timesteps    |     6400  |\n",
      "| train/                |           |\n",
      "|    loss               |    5.045  |\n",
      "|    policy_loss        |    4.962  |\n",
      "|    value_loss         |    0.217  |\n",
      "|    explained_variance |   -0.028  |\n",
      "|    n_updates          |      100  |\n",
      "| rnd_net_dist/         |           |\n",
      "|    mean_rnd_bonus     |    0.000  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        |   64.000  |\n",
      "|    ep_rew_mean        |    0.027  |\n",
      "|    ep_rew_std         |    1.000  |\n",
      "|    policy_entropy     |    0.122  |\n",
      "|    advantage_mean     |   -0.285  |\n",
      "|    advantage_std      |    0.301  |\n",
      "|    aux_loss_mean      |    0.000  |\n",
      "| time/                 |           |\n",
      "|    fps                |      168  |\n",
      "|    episodes           |      200  |\n",
      "|    time_elapsed       |       76  |\n",
      "|    total_timesteps    |    12800  |\n",
      "| train/                |           |\n",
      "|    loss               |   -2.019  |\n",
      "|    policy_loss        |   -2.093  |\n",
      "|    value_loss         |    0.171  |\n",
      "|    explained_variance |   -0.215  |\n",
      "|    n_updates          |      200  |\n",
      "| rnd_net_dist/         |           |\n",
      "|    mean_rnd_bonus     |    0.000  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        |   64.000  |\n",
      "|    ep_rew_mean        |   -0.095  |\n",
      "|    ep_rew_std         |    0.995  |\n",
      "|    policy_entropy     |    0.046  |\n",
      "|    advantage_mean     |    0.242  |\n",
      "|    advantage_std      |    0.292  |\n",
      "|    aux_loss_mean      |    0.000  |\n",
      "| time/                 |           |\n",
      "|    fps                |      166  |\n",
      "|    episodes           |      300  |\n",
      "|    time_elapsed       |      115  |\n",
      "|    total_timesteps    |    19200  |\n",
      "| train/                |           |\n",
      "|    loss               |    0.180  |\n",
      "|    policy_loss        |    0.113  |\n",
      "|    value_loss         |    0.143  |\n",
      "|    explained_variance |   -0.325  |\n",
      "|    n_updates          |      300  |\n",
      "| rnd_net_dist/         |           |\n",
      "|    mean_rnd_bonus     |    0.000  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        |   64.000  |\n",
      "|    ep_rew_mean        |   -0.056  |\n",
      "|    ep_rew_std         |    0.998  |\n",
      "|    policy_entropy     |    0.087  |\n",
      "|    advantage_mean     |   -0.268  |\n",
      "|    advantage_std      |    0.263  |\n",
      "|    aux_loss_mean      |    0.000  |\n",
      "| time/                 |           |\n",
      "|    fps                |      167  |\n",
      "|    episodes           |      400  |\n",
      "|    time_elapsed       |      153  |\n",
      "|    total_timesteps    |    25600  |\n",
      "| train/                |           |\n",
      "|    loss               |   -0.553  |\n",
      "|    policy_loss        |   -0.614  |\n",
      "|    value_loss         |    0.140  |\n",
      "|    explained_variance |    0.056  |\n",
      "|    n_updates          |      400  |\n",
      "| rnd_net_dist/         |           |\n",
      "|    mean_rnd_bonus     |    0.000  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        |   64.000  |\n",
      "|    ep_rew_mean        |    0.144  |\n",
      "|    ep_rew_std         |    0.990  |\n",
      "|    policy_entropy     |    0.053  |\n",
      "|    advantage_mean     |   -0.350  |\n",
      "|    advantage_std      |    0.318  |\n",
      "|    aux_loss_mean      |    0.000  |\n",
      "| time/                 |           |\n",
      "|    fps                |      166  |\n",
      "|    episodes           |      500  |\n",
      "|    time_elapsed       |      192  |\n",
      "|    total_timesteps    |    32000  |\n",
      "| train/                |           |\n",
      "|    loss               |   -0.090  |\n",
      "|    policy_loss        |   -0.195  |\n",
      "|    value_loss         |    0.222  |\n",
      "|    explained_variance |   -0.033  |\n",
      "|    n_updates          |      500  |\n",
      "| rnd_net_dist/         |           |\n",
      "|    mean_rnd_bonus     |    0.000  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        |   64.000  |\n",
      "|    ep_rew_mean        |    0.083  |\n",
      "|    ep_rew_std         |    0.997  |\n",
      "|    policy_entropy     |    0.046  |\n",
      "|    advantage_mean     |   -0.205  |\n",
      "|    advantage_std      |    0.228  |\n",
      "|    aux_loss_mean      |    0.000  |\n",
      "| time/                 |           |\n",
      "|    fps                |      166  |\n",
      "|    episodes           |      600  |\n",
      "|    time_elapsed       |      231  |\n",
      "|    total_timesteps    |    38400  |\n",
      "| train/                |           |\n",
      "|    loss               |   -0.054  |\n",
      "|    policy_loss        |   -0.096  |\n",
      "|    value_loss         |    0.093  |\n",
      "|    explained_variance |    0.033  |\n",
      "|    n_updates          |      600  |\n",
      "| rnd_net_dist/         |           |\n",
      "|    mean_rnd_bonus     |    0.000  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        |   64.000  |\n",
      "|    ep_rew_mean        |    0.003  |\n",
      "|    ep_rew_std         |    1.000  |\n",
      "|    policy_entropy     |    0.030  |\n",
      "|    advantage_mean     |    0.221  |\n",
      "|    advantage_std      |    0.246  |\n",
      "|    aux_loss_mean      |    0.000  |\n",
      "| time/                 |           |\n",
      "|    fps                |      165  |\n",
      "|    episodes           |      700  |\n",
      "|    time_elapsed       |      270  |\n",
      "|    total_timesteps    |    44800  |\n",
      "| train/                |           |\n",
      "|    loss               |    1.311  |\n",
      "|    policy_loss        |    1.259  |\n",
      "|    value_loss         |    0.108  |\n",
      "|    explained_variance |   -0.016  |\n",
      "|    n_updates          |      700  |\n",
      "| rnd_net_dist/         |           |\n",
      "|    mean_rnd_bonus     |    0.000  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        |   64.000  |\n",
      "|    ep_rew_mean        |    0.063  |\n",
      "|    ep_rew_std         |    0.998  |\n",
      "|    policy_entropy     |    0.041  |\n",
      "|    advantage_mean     |    0.285  |\n",
      "|    advantage_std      |    0.291  |\n",
      "|    aux_loss_mean      |    0.000  |\n",
      "| time/                 |           |\n",
      "|    fps                |      165  |\n",
      "|    episodes           |      800  |\n",
      "|    time_elapsed       |      309  |\n",
      "|    total_timesteps    |    51200  |\n",
      "| train/                |           |\n",
      "|    loss               |    0.191  |\n",
      "|    policy_loss        |    0.113  |\n",
      "|    value_loss         |    0.165  |\n",
      "|    explained_variance |   -0.089  |\n",
      "|    n_updates          |      800  |\n",
      "| rnd_net_dist/         |           |\n",
      "|    mean_rnd_bonus     |    0.000  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        |   64.000  |\n",
      "|    ep_rew_mean        |   -0.017  |\n",
      "|    ep_rew_std         |    1.000  |\n",
      "|    policy_entropy     |    0.031  |\n",
      "|    advantage_mean     |    0.117  |\n",
      "|    advantage_std      |    0.186  |\n",
      "|    aux_loss_mean      |    0.000  |\n",
      "| time/                 |           |\n",
      "|    fps                |      165  |\n",
      "|    episodes           |      900  |\n",
      "|    time_elapsed       |      348  |\n",
      "|    total_timesteps    |    57600  |\n",
      "| train/                |           |\n",
      "|    loss               |    0.063  |\n",
      "|    policy_loss        |    0.042  |\n",
      "|    value_loss         |    0.048  |\n",
      "|    explained_variance |   -0.067  |\n",
      "|    n_updates          |      900  |\n",
      "| rnd_net_dist/         |           |\n",
      "|    mean_rnd_bonus     |    0.000  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        |   64.000  |\n",
      "|    ep_rew_mean        |    0.003  |\n",
      "|    ep_rew_std         |    1.000  |\n",
      "|    policy_entropy     |    0.027  |\n",
      "|    advantage_mean     |   -0.293  |\n",
      "|    advantage_std      |    0.287  |\n",
      "|    aux_loss_mean      |    0.000  |\n",
      "| time/                 |           |\n",
      "|    fps                |      165  |\n",
      "|    episodes           |     1000  |\n",
      "|    time_elapsed       |      387  |\n",
      "|    total_timesteps    |    64000  |\n",
      "| train/                |           |\n",
      "|    loss               |   -3.538  |\n",
      "|    policy_loss        |   -3.619  |\n",
      "|    value_loss         |    0.166  |\n",
      "|    explained_variance |    0.014  |\n",
      "|    n_updates          |     1000  |\n",
      "| rnd_net_dist/         |           |\n",
      "|    mean_rnd_bonus     |    0.000  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        |   64.000  |\n",
      "|    ep_rew_mean        |   -0.038  |\n",
      "|    ep_rew_std         |    0.999  |\n",
      "|    policy_entropy     |    0.047  |\n",
      "|    advantage_mean     |   -0.203  |\n",
      "|    advantage_std      |    0.228  |\n",
      "|    aux_loss_mean      |    0.000  |\n",
      "| time/                 |           |\n",
      "|    fps                |      165  |\n",
      "|    episodes           |     1100  |\n",
      "|    time_elapsed       |      426  |\n",
      "|    total_timesteps    |    70400  |\n",
      "| train/                |           |\n",
      "|    loss               |   -2.940  |\n",
      "|    policy_loss        |   -2.981  |\n",
      "|    value_loss         |    0.092  |\n",
      "|    explained_variance |    0.060  |\n",
      "|    n_updates          |     1100  |\n",
      "| rnd_net_dist/         |           |\n",
      "|    mean_rnd_bonus     |    0.000  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        |   64.000  |\n",
      "|    ep_rew_mean        |   -0.018  |\n",
      "|    ep_rew_std         |    1.000  |\n",
      "|    policy_entropy     |    0.026  |\n",
      "|    advantage_mean     |    0.256  |\n",
      "|    advantage_std      |    0.271  |\n",
      "|    aux_loss_mean      |    0.000  |\n",
      "| time/                 |           |\n",
      "|    fps                |      165  |\n",
      "|    episodes           |     1200  |\n",
      "|    time_elapsed       |      465  |\n",
      "|    total_timesteps    |    76800  |\n",
      "| train/                |           |\n",
      "|    loss               |    0.127  |\n",
      "|    policy_loss        |    0.061  |\n",
      "|    value_loss         |    0.138  |\n",
      "|    explained_variance |   -0.080  |\n",
      "|    n_updates          |     1200  |\n",
      "| rnd_net_dist/         |           |\n",
      "|    mean_rnd_bonus     |    0.000  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        |   64.000  |\n",
      "|    ep_rew_mean        |   -0.018  |\n",
      "|    ep_rew_std         |    1.000  |\n",
      "|    policy_entropy     |    0.014  |\n",
      "|    advantage_mean     |   -0.251  |\n",
      "|    advantage_std      |    0.270  |\n",
      "|    aux_loss_mean      |    0.000  |\n",
      "| time/                 |           |\n",
      "|    fps                |      165  |\n",
      "|    episodes           |     1300  |\n",
      "|    time_elapsed       |      504  |\n",
      "|    total_timesteps    |    83200  |\n",
      "| train/                |           |\n",
      "|    loss               |    0.037  |\n",
      "|    policy_loss        |   -0.029  |\n",
      "|    value_loss         |    0.135  |\n",
      "|    explained_variance |   -0.079  |\n",
      "|    n_updates          |     1300  |\n",
      "| rnd_net_dist/         |           |\n",
      "|    mean_rnd_bonus     |    0.000  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        |   64.000  |\n",
      "|    ep_rew_mean        |   -0.058  |\n",
      "|    ep_rew_std         |    0.998  |\n",
      "|    policy_entropy     |    0.008  |\n",
      "|    advantage_mean     |    0.271  |\n",
      "|    advantage_std      |    0.273  |\n",
      "|    aux_loss_mean      |    0.000  |\n",
      "| time/                 |           |\n",
      "|    fps                |      164  |\n",
      "|    episodes           |     1400  |\n",
      "|    time_elapsed       |      544  |\n",
      "|    total_timesteps    |    89600  |\n",
      "| train/                |           |\n",
      "|    loss               |    0.088  |\n",
      "|    policy_loss        |    0.016  |\n",
      "|    value_loss         |    0.147  |\n",
      "|    explained_variance |   -0.010  |\n",
      "|    n_updates          |     1400  |\n",
      "| rnd_net_dist/         |           |\n",
      "|    mean_rnd_bonus     |    0.000  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        |   64.000  |\n",
      "|    ep_rew_mean        |    0.002  |\n",
      "|    ep_rew_std         |    1.000  |\n",
      "|    policy_entropy     |    0.013  |\n",
      "|    advantage_mean     |    0.315  |\n",
      "|    advantage_std      |    0.301  |\n",
      "|    aux_loss_mean      |    0.000  |\n",
      "| time/                 |           |\n",
      "|    fps                |      164  |\n",
      "|    episodes           |     1500  |\n",
      "|    time_elapsed       |      583  |\n",
      "|    total_timesteps    |    96000  |\n",
      "| train/                |           |\n",
      "|    loss               |    0.124  |\n",
      "|    policy_loss        |    0.032  |\n",
      "|    value_loss         |    0.188  |\n",
      "|    explained_variance |   -0.025  |\n",
      "|    n_updates          |     1500  |\n",
      "| rnd_net_dist/         |           |\n",
      "|    mean_rnd_bonus     |    0.000  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        |   64.000  |\n",
      "|    ep_rew_mean        |    0.042  |\n",
      "|    ep_rew_std         |    0.999  |\n",
      "|    policy_entropy     |    0.018  |\n",
      "|    advantage_mean     |    0.240  |\n",
      "|    advantage_std      |    0.258  |\n",
      "|    aux_loss_mean      |    0.000  |\n",
      "| time/                 |           |\n",
      "|    fps                |      164  |\n",
      "|    episodes           |     1600  |\n",
      "|    time_elapsed       |      622  |\n",
      "|    total_timesteps    |   102400  |\n",
      "| train/                |           |\n",
      "|    loss               |    0.095  |\n",
      "|    policy_loss        |    0.035  |\n",
      "|    value_loss         |    0.123  |\n",
      "|    explained_variance |   -0.027  |\n",
      "|    n_updates          |     1600  |\n",
      "| rnd_net_dist/         |           |\n",
      "|    mean_rnd_bonus     |    0.000  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        |   64.000  |\n",
      "|    ep_rew_mean        |    0.042  |\n",
      "|    ep_rew_std         |    0.999  |\n",
      "|    policy_entropy     |    0.013  |\n",
      "|    advantage_mean     |   -0.205  |\n",
      "|    advantage_std      |    0.235  |\n",
      "|    aux_loss_mean      |    0.000  |\n",
      "| time/                 |           |\n",
      "|    fps                |      164  |\n",
      "|    episodes           |     1700  |\n",
      "|    time_elapsed       |      661  |\n",
      "|    total_timesteps    |   108800  |\n",
      "| train/                |           |\n",
      "|    loss               |    0.025  |\n",
      "|    policy_loss        |   -0.022  |\n",
      "|    value_loss         |    0.097  |\n",
      "|    explained_variance |   -0.003  |\n",
      "|    n_updates          |     1700  |\n",
      "| rnd_net_dist/         |           |\n",
      "|    mean_rnd_bonus     |    0.000  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        |   64.000  |\n",
      "|    ep_rew_mean        |    0.182  |\n",
      "|    ep_rew_std         |    0.984  |\n",
      "|    policy_entropy     |    0.022  |\n",
      "|    advantage_mean     |   -0.298  |\n",
      "|    advantage_std      |    0.291  |\n",
      "|    aux_loss_mean      |    0.000  |\n",
      "| time/                 |           |\n",
      "|    fps                |      164  |\n",
      "|    episodes           |     1800  |\n",
      "|    time_elapsed       |      701  |\n",
      "|    total_timesteps    |   115200  |\n",
      "| train/                |           |\n",
      "|    loss               |    0.028  |\n",
      "|    policy_loss        |   -0.056  |\n",
      "|    value_loss         |    0.173  |\n",
      "|    explained_variance |   -0.010  |\n",
      "|    n_updates          |     1800  |\n",
      "| rnd_net_dist/         |           |\n",
      "|    mean_rnd_bonus     |    0.000  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        |   64.000  |\n",
      "|    ep_rew_mean        |   -0.138  |\n",
      "|    ep_rew_std         |    0.990  |\n",
      "|    policy_entropy     |    0.040  |\n",
      "|    advantage_mean     |   -0.202  |\n",
      "|    advantage_std      |    0.235  |\n",
      "|    aux_loss_mean      |    0.000  |\n",
      "| time/                 |           |\n",
      "|    fps                |      164  |\n",
      "|    episodes           |     1900  |\n",
      "|    time_elapsed       |      740  |\n",
      "|    total_timesteps    |   121600  |\n",
      "| train/                |           |\n",
      "|    loss               |   -3.722  |\n",
      "|    policy_loss        |   -3.765  |\n",
      "|    value_loss         |    0.095  |\n",
      "|    explained_variance |   -0.020  |\n",
      "|    n_updates          |     1900  |\n",
      "| rnd_net_dist/         |           |\n",
      "|    mean_rnd_bonus     |    0.000  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        |   64.000  |\n",
      "|    ep_rew_mean        |    0.162  |\n",
      "|    ep_rew_std         |    0.987  |\n",
      "|    policy_entropy     |    0.679  |\n",
      "|    advantage_mean     |   -0.215  |\n",
      "|    advantage_std      |    0.242  |\n",
      "|    aux_loss_mean      |    0.000  |\n",
      "| time/                 |           |\n",
      "|    fps                |      164  |\n",
      "|    episodes           |     2000  |\n",
      "|    time_elapsed       |      779  |\n",
      "|    total_timesteps    |   128000  |\n",
      "| train/                |           |\n",
      "|    loss               |   -8.422  |\n",
      "|    policy_loss        |   -8.406  |\n",
      "|    value_loss         |    0.104  |\n",
      "|    explained_variance |   -0.006  |\n",
      "|    n_updates          |     2000  |\n",
      "| rnd_net_dist/         |           |\n",
      "|    mean_rnd_bonus     |    0.000  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        |   64.000  |\n",
      "|    ep_rew_mean        |    0.142  |\n",
      "|    ep_rew_std         |    0.990  |\n",
      "|    policy_entropy     |    0.677  |\n",
      "|    advantage_mean     |   -0.146  |\n",
      "|    advantage_std      |    0.198  |\n",
      "|    aux_loss_mean      |    0.000  |\n",
      "| time/                 |           |\n",
      "|    fps                |      164  |\n",
      "|    episodes           |     2100  |\n",
      "|    time_elapsed       |      818  |\n",
      "|    total_timesteps    |   134400  |\n",
      "| train/                |           |\n",
      "|    loss               |   -6.847  |\n",
      "|    policy_loss        |   -6.810  |\n",
      "|    value_loss         |    0.060  |\n",
      "|    explained_variance |    0.014  |\n",
      "|    n_updates          |     2100  |\n",
      "| rnd_net_dist/         |           |\n",
      "|    mean_rnd_bonus     |    0.000  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        |   64.000  |\n",
      "|    ep_rew_mean        |    0.282  |\n",
      "|    ep_rew_std         |    0.960  |\n",
      "|    policy_entropy     |    0.557  |\n",
      "|    advantage_mean     |   -0.158  |\n",
      "|    advantage_std      |    0.206  |\n",
      "|    aux_loss_mean      |    0.000  |\n",
      "| time/                 |           |\n",
      "|    fps                |      164  |\n",
      "|    episodes           |     2200  |\n",
      "|    time_elapsed       |      857  |\n",
      "|    total_timesteps    |   140800  |\n",
      "| train/                |           |\n",
      "|    loss               |   -6.505  |\n",
      "|    policy_loss        |   -6.483  |\n",
      "|    value_loss         |    0.067  |\n",
      "|    explained_variance |   -0.008  |\n",
      "|    n_updates          |     2200  |\n",
      "| rnd_net_dist/         |           |\n",
      "|    mean_rnd_bonus     |    0.000  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        |   64.000  |\n",
      "|    ep_rew_mean        |    0.782  |\n",
      "|    ep_rew_std         |    0.626  |\n",
      "|    policy_entropy     |    0.234  |\n",
      "|    advantage_mean     |    0.023  |\n",
      "|    advantage_std      |    0.128  |\n",
      "|    aux_loss_mean      |    0.000  |\n",
      "| time/                 |           |\n",
      "|    fps                |      164  |\n",
      "|    episodes           |     2300  |\n",
      "|    time_elapsed       |      896  |\n",
      "|    total_timesteps    |   147200  |\n",
      "| train/                |           |\n",
      "|    loss               |   -0.141  |\n",
      "|    policy_loss        |   -0.126  |\n",
      "|    value_loss         |    0.017  |\n",
      "|    explained_variance |   -0.013  |\n",
      "|    n_updates          |     2300  |\n",
      "| rnd_net_dist/         |           |\n",
      "|    mean_rnd_bonus     |    0.000  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        |   64.000  |\n",
      "|    ep_rew_mean        |    0.902  |\n",
      "|    ep_rew_std         |    0.436  |\n",
      "|    policy_entropy     |    0.100  |\n",
      "|    advantage_mean     |    0.003  |\n",
      "|    advantage_std      |    0.116  |\n",
      "|    aux_loss_mean      |    0.000  |\n",
      "| time/                 |           |\n",
      "|    fps                |      164  |\n",
      "|    episodes           |     2400  |\n",
      "|    time_elapsed       |      935  |\n",
      "|    total_timesteps    |   153600  |\n",
      "| train/                |           |\n",
      "|    loss               |   -0.199  |\n",
      "|    policy_loss        |   -0.196  |\n",
      "|    value_loss         |    0.013  |\n",
      "|    explained_variance |   -0.012  |\n",
      "|    n_updates          |     2400  |\n",
      "| rnd_net_dist/         |           |\n",
      "|    mean_rnd_bonus     |    0.000  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        |   64.000  |\n",
      "|    ep_rew_mean        |    0.962  |\n",
      "|    ep_rew_std         |    0.280  |\n",
      "|    policy_entropy     |    0.053  |\n",
      "|    advantage_mean     |    0.016  |\n",
      "|    advantage_std      |    0.123  |\n",
      "|    aux_loss_mean      |    0.000  |\n",
      "| time/                 |           |\n",
      "|    fps                |      164  |\n",
      "|    episodes           |     2500  |\n",
      "|    time_elapsed       |      974  |\n",
      "|    total_timesteps    |   160000  |\n",
      "| train/                |           |\n",
      "|    loss               |    1.344  |\n",
      "|    policy_loss        |    1.341  |\n",
      "|    value_loss         |    0.015  |\n",
      "|    explained_variance |    0.022  |\n",
      "|    n_updates          |     2500  |\n",
      "| rnd_net_dist/         |           |\n",
      "|    mean_rnd_bonus     |    0.000  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        |   64.000  |\n",
      "|    ep_rew_mean        |    1.002  |\n",
      "|    ep_rew_std         |    0.000  |\n",
      "|    policy_entropy     |    0.102  |\n",
      "|    advantage_mean     |   -0.013  |\n",
      "|    advantage_std      |    0.105  |\n",
      "|    aux_loss_mean      |    0.000  |\n",
      "| time/                 |           |\n",
      "|    fps                |      164  |\n",
      "|    episodes           |     2600  |\n",
      "|    time_elapsed       |     1014  |\n",
      "|    total_timesteps    |   166400  |\n",
      "| train/                |           |\n",
      "|    loss               |   -0.369  |\n",
      "|    policy_loss        |   -0.365  |\n",
      "|    value_loss         |    0.011  |\n",
      "|    explained_variance |    0.032  |\n",
      "|    n_updates          |     2600  |\n",
      "| rnd_net_dist/         |           |\n",
      "|    mean_rnd_bonus     |    0.000  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        |   64.000  |\n",
      "|    ep_rew_mean        |    0.942  |\n",
      "|    ep_rew_std         |    0.341  |\n",
      "|    policy_entropy     |    0.055  |\n",
      "|    advantage_mean     |    0.005  |\n",
      "|    advantage_std      |    0.115  |\n",
      "|    aux_loss_mean      |    0.000  |\n",
      "| time/                 |           |\n",
      "|    fps                |      164  |\n",
      "|    episodes           |     2700  |\n",
      "|    time_elapsed       |     1052  |\n",
      "|    total_timesteps    |   172800  |\n",
      "| train/                |           |\n",
      "|    loss               |    0.041  |\n",
      "|    policy_loss        |    0.040  |\n",
      "|    value_loss         |    0.013  |\n",
      "|    explained_variance |    0.053  |\n",
      "|    n_updates          |     2700  |\n",
      "| rnd_net_dist/         |           |\n",
      "|    mean_rnd_bonus     |    0.000  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        |   64.000  |\n",
      "|    ep_rew_mean        |    0.962  |\n",
      "|    ep_rew_std         |    0.280  |\n",
      "|    policy_entropy     |    0.058  |\n",
      "|    advantage_mean     |   -0.002  |\n",
      "|    advantage_std      |    0.107  |\n",
      "|    aux_loss_mean      |    0.000  |\n",
      "| time/                 |           |\n",
      "|    fps                |      164  |\n",
      "|    episodes           |     2800  |\n",
      "|    time_elapsed       |     1091  |\n",
      "|    total_timesteps    |   179200  |\n",
      "| train/                |           |\n",
      "|    loss               |   -0.314  |\n",
      "|    policy_loss        |   -0.313  |\n",
      "|    value_loss         |    0.011  |\n",
      "|    explained_variance |    0.113  |\n",
      "|    n_updates          |     2800  |\n",
      "| rnd_net_dist/         |           |\n",
      "|    mean_rnd_bonus     |    0.000  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        |   64.000  |\n",
      "|    ep_rew_mean        |    1.001  |\n",
      "|    ep_rew_std         |    0.000  |\n",
      "|    policy_entropy     |    0.030  |\n",
      "|    advantage_mean     |   -0.009  |\n",
      "|    advantage_std      |    0.104  |\n",
      "|    aux_loss_mean      |    0.000  |\n",
      "| time/                 |           |\n",
      "|    fps                |      164  |\n",
      "|    episodes           |     2900  |\n",
      "|    time_elapsed       |     1130  |\n",
      "|    total_timesteps    |   185600  |\n",
      "| train/                |           |\n",
      "|    loss               |    0.138  |\n",
      "|    policy_loss        |    0.135  |\n",
      "|    value_loss         |    0.011  |\n",
      "|    explained_variance |    0.096  |\n",
      "|    n_updates          |     2900  |\n",
      "| rnd_net_dist/         |           |\n",
      "|    mean_rnd_bonus     |    0.000  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        |   64.000  |\n",
      "|    ep_rew_mean        |    0.981  |\n",
      "|    ep_rew_std         |    0.199  |\n",
      "|    policy_entropy     |    0.039  |\n",
      "|    advantage_mean     |    0.013  |\n",
      "|    advantage_std      |    0.115  |\n",
      "|    aux_loss_mean      |    0.000  |\n",
      "| time/                 |           |\n",
      "|    fps                |      164  |\n",
      "|    episodes           |     3000  |\n",
      "|    time_elapsed       |     1169  |\n",
      "|    total_timesteps    |   192000  |\n",
      "| train/                |           |\n",
      "|    loss               |    0.036  |\n",
      "|    policy_loss        |    0.033  |\n",
      "|    value_loss         |    0.013  |\n",
      "|    explained_variance |    0.136  |\n",
      "|    n_updates          |     3000  |\n",
      "| rnd_net_dist/         |           |\n",
      "|    mean_rnd_bonus     |    0.000  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        |   64.000  |\n",
      "|    ep_rew_mean        |    0.981  |\n",
      "|    ep_rew_std         |    0.199  |\n",
      "|    policy_entropy     |    0.047  |\n",
      "|    advantage_mean     |   -0.017  |\n",
      "|    advantage_std      |    0.099  |\n",
      "|    aux_loss_mean      |    0.000  |\n",
      "| time/                 |           |\n",
      "|    fps                |      164  |\n",
      "|    episodes           |     3100  |\n",
      "|    time_elapsed       |     1207  |\n",
      "|    total_timesteps    |   198400  |\n",
      "| train/                |           |\n",
      "|    loss               |    0.175  |\n",
      "|    policy_loss        |    0.175  |\n",
      "|    value_loss         |    0.010  |\n",
      "|    explained_variance |    0.125  |\n",
      "|    n_updates          |     3100  |\n",
      "| rnd_net_dist/         |           |\n",
      "|    mean_rnd_bonus     |    0.000  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        |   64.000  |\n",
      "|    ep_rew_mean        |    1.001  |\n",
      "|    ep_rew_std         |    0.000  |\n",
      "|    policy_entropy     |    0.037  |\n",
      "|    advantage_mean     |   -0.014  |\n",
      "|    advantage_std      |    0.095  |\n",
      "|    aux_loss_mean      |    0.000  |\n",
      "| time/                 |           |\n",
      "|    fps                |      164  |\n",
      "|    episodes           |     3200  |\n",
      "|    time_elapsed       |     1246  |\n",
      "|    total_timesteps    |   204800  |\n",
      "| train/                |           |\n",
      "|    loss               |    0.132  |\n",
      "|    policy_loss        |    0.132  |\n",
      "|    value_loss         |    0.009  |\n",
      "|    explained_variance |    0.225  |\n",
      "|    n_updates          |     3200  |\n",
      "| rnd_net_dist/         |           |\n",
      "|    mean_rnd_bonus     |    0.000  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        |   64.000  |\n",
      "|    ep_rew_mean        |    1.001  |\n",
      "|    ep_rew_std         |    0.000  |\n",
      "|    policy_entropy     |    0.056  |\n",
      "|    advantage_mean     |   -0.007  |\n",
      "|    advantage_std      |    0.092  |\n",
      "|    aux_loss_mean      |    0.000  |\n",
      "| time/                 |           |\n",
      "|    fps                |      164  |\n",
      "|    episodes           |     3300  |\n",
      "|    time_elapsed       |     1285  |\n",
      "|    total_timesteps    |   211200  |\n",
      "| train/                |           |\n",
      "|    loss               |    0.045  |\n",
      "|    policy_loss        |    0.047  |\n",
      "|    value_loss         |    0.008  |\n",
      "|    explained_variance |    0.341  |\n",
      "|    n_updates          |     3300  |\n",
      "| rnd_net_dist/         |           |\n",
      "|    mean_rnd_bonus     |    0.000  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        |   64.000  |\n",
      "|    ep_rew_mean        |    1.001  |\n",
      "|    ep_rew_std         |    0.000  |\n",
      "|    policy_entropy     |    0.041  |\n",
      "|    advantage_mean     |   -0.010  |\n",
      "|    advantage_std      |    0.086  |\n",
      "|    aux_loss_mean      |    0.000  |\n",
      "| time/                 |           |\n",
      "|    fps                |      164  |\n",
      "|    episodes           |     3400  |\n",
      "|    time_elapsed       |     1324  |\n",
      "|    total_timesteps    |   217600  |\n",
      "| train/                |           |\n",
      "|    loss               |   -0.182  |\n",
      "|    policy_loss        |   -0.182  |\n",
      "|    value_loss         |    0.007  |\n",
      "|    explained_variance |    0.414  |\n",
      "|    n_updates          |     3400  |\n",
      "| rnd_net_dist/         |           |\n",
      "|    mean_rnd_bonus     |    0.000  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        |   64.000  |\n",
      "|    ep_rew_mean        |    0.981  |\n",
      "|    ep_rew_std         |    0.199  |\n",
      "|    policy_entropy     |    0.054  |\n",
      "|    advantage_mean     |   -0.009  |\n",
      "|    advantage_std      |    0.079  |\n",
      "|    aux_loss_mean      |    0.000  |\n",
      "| time/                 |           |\n",
      "|    fps                |      164  |\n",
      "|    episodes           |     3500  |\n",
      "|    time_elapsed       |     1363  |\n",
      "|    total_timesteps    |   224000  |\n",
      "| train/                |           |\n",
      "|    loss               |    0.059  |\n",
      "|    policy_loss        |    0.061  |\n",
      "|    value_loss         |    0.006  |\n",
      "|    explained_variance |    0.520  |\n",
      "|    n_updates          |     3500  |\n",
      "| rnd_net_dist/         |           |\n",
      "|    mean_rnd_bonus     |    0.000  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        |   64.000  |\n",
      "|    ep_rew_mean        |    1.001  |\n",
      "|    ep_rew_std         |    0.000  |\n",
      "|    policy_entropy     |    0.055  |\n",
      "|    advantage_mean     |    0.002  |\n",
      "|    advantage_std      |    0.074  |\n",
      "|    aux_loss_mean      |    0.000  |\n",
      "| time/                 |           |\n",
      "|    fps                |      164  |\n",
      "|    episodes           |     3600  |\n",
      "|    time_elapsed       |     1401  |\n",
      "|    total_timesteps    |   230400  |\n",
      "| train/                |           |\n",
      "|    loss               |    0.011  |\n",
      "|    policy_loss        |    0.013  |\n",
      "|    value_loss         |    0.005  |\n",
      "|    explained_variance |    0.638  |\n",
      "|    n_updates          |     3600  |\n",
      "| rnd_net_dist/         |           |\n",
      "|    mean_rnd_bonus     |    0.000  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        |   64.000  |\n",
      "|    ep_rew_mean        |    0.981  |\n",
      "|    ep_rew_std         |    0.199  |\n",
      "|    policy_entropy     |    0.027  |\n",
      "|    advantage_mean     |   -0.016  |\n",
      "|    advantage_std      |    0.066  |\n",
      "|    aux_loss_mean      |    0.000  |\n",
      "| time/                 |           |\n",
      "|    fps                |      164  |\n",
      "|    episodes           |     3700  |\n",
      "|    time_elapsed       |     1441  |\n",
      "|    total_timesteps    |   236800  |\n",
      "| train/                |           |\n",
      "|    loss               |    0.100  |\n",
      "|    policy_loss        |    0.101  |\n",
      "|    value_loss         |    0.004  |\n",
      "|    explained_variance |    0.659  |\n",
      "|    n_updates          |     3700  |\n",
      "| rnd_net_dist/         |           |\n",
      "|    mean_rnd_bonus     |    0.000  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        |   64.000  |\n",
      "|    ep_rew_mean        |    1.001  |\n",
      "|    ep_rew_std         |    0.000  |\n",
      "|    policy_entropy     |    0.076  |\n",
      "|    advantage_mean     |   -0.002  |\n",
      "|    advantage_std      |    0.059  |\n",
      "|    aux_loss_mean      |    0.000  |\n",
      "| time/                 |           |\n",
      "|    fps                |      164  |\n",
      "|    episodes           |     3800  |\n",
      "|    time_elapsed       |     1479  |\n",
      "|    total_timesteps    |   243200  |\n",
      "| train/                |           |\n",
      "|    loss               |    0.303  |\n",
      "|    policy_loss        |    0.309  |\n",
      "|    value_loss         |    0.003  |\n",
      "|    explained_variance |    0.764  |\n",
      "|    n_updates          |     3800  |\n",
      "| rnd_net_dist/         |           |\n",
      "|    mean_rnd_bonus     |    0.000  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        |   64.000  |\n",
      "|    ep_rew_mean        |    1.001  |\n",
      "|    ep_rew_std         |    0.000  |\n",
      "|    policy_entropy     |    0.083  |\n",
      "|    advantage_mean     |   -0.011  |\n",
      "|    advantage_std      |    0.058  |\n",
      "|    aux_loss_mean      |    0.000  |\n",
      "| time/                 |           |\n",
      "|    fps                |      164  |\n",
      "|    episodes           |     3900  |\n",
      "|    time_elapsed       |     1518  |\n",
      "|    total_timesteps    |   249600  |\n",
      "| train/                |           |\n",
      "|    loss               |    0.360  |\n",
      "|    policy_loss        |    0.366  |\n",
      "|    value_loss         |    0.003  |\n",
      "|    explained_variance |    0.755  |\n",
      "|    n_updates          |     3900  |\n",
      "| rnd_net_dist/         |           |\n",
      "|    mean_rnd_bonus     |    0.000  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        |   64.000  |\n",
      "|    ep_rew_mean        |    0.961  |\n",
      "|    ep_rew_std         |    0.280  |\n",
      "|    policy_entropy     |    0.025  |\n",
      "|    advantage_mean     |    0.002  |\n",
      "|    advantage_std      |    0.055  |\n",
      "|    aux_loss_mean      |    0.000  |\n",
      "| time/                 |           |\n",
      "|    fps                |      164  |\n",
      "|    episodes           |     4000  |\n",
      "|    time_elapsed       |     1557  |\n",
      "|    total_timesteps    |   256000  |\n",
      "| train/                |           |\n",
      "|    loss               |    0.073  |\n",
      "|    policy_loss        |    0.074  |\n",
      "|    value_loss         |    0.003  |\n",
      "|    explained_variance |    0.815  |\n",
      "|    n_updates          |     4000  |\n",
      "| rnd_net_dist/         |           |\n",
      "|    mean_rnd_bonus     |    0.000  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        |   64.000  |\n",
      "|    ep_rew_mean        |    1.001  |\n",
      "|    ep_rew_std         |    0.000  |\n",
      "|    policy_entropy     |    0.054  |\n",
      "|    advantage_mean     |    0.000  |\n",
      "|    advantage_std      |    0.048  |\n",
      "|    aux_loss_mean      |    0.000  |\n",
      "| time/                 |           |\n",
      "|    fps                |      164  |\n",
      "|    episodes           |     4100  |\n",
      "|    time_elapsed       |     1596  |\n",
      "|    total_timesteps    |   262400  |\n",
      "| train/                |           |\n",
      "|    loss               |    0.020  |\n",
      "|    policy_loss        |    0.024  |\n",
      "|    value_loss         |    0.002  |\n",
      "|    explained_variance |    0.863  |\n",
      "|    n_updates          |     4100  |\n",
      "| rnd_net_dist/         |           |\n",
      "|    mean_rnd_bonus     |    0.000  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        |   64.000  |\n",
      "|    ep_rew_mean        |    0.981  |\n",
      "|    ep_rew_std         |    0.199  |\n",
      "|    policy_entropy     |    0.025  |\n",
      "|    advantage_mean     |   -0.003  |\n",
      "|    advantage_std      |    0.042  |\n",
      "|    aux_loss_mean      |    0.000  |\n",
      "| time/                 |           |\n",
      "|    fps                |      164  |\n",
      "|    episodes           |     4200  |\n",
      "|    time_elapsed       |     1635  |\n",
      "|    total_timesteps    |   268800  |\n",
      "| train/                |           |\n",
      "|    loss               |   -0.014  |\n",
      "|    policy_loss        |   -0.012  |\n",
      "|    value_loss         |    0.002  |\n",
      "|    explained_variance |    0.897  |\n",
      "|    n_updates          |     4200  |\n",
      "| rnd_net_dist/         |           |\n",
      "|    mean_rnd_bonus     |    0.000  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        |   64.000  |\n",
      "|    ep_rew_mean        |    0.981  |\n",
      "|    ep_rew_std         |    0.199  |\n",
      "|    policy_entropy     |    0.028  |\n",
      "|    advantage_mean     |   -0.016  |\n",
      "|    advantage_std      |    0.038  |\n",
      "|    aux_loss_mean      |    0.000  |\n",
      "| time/                 |           |\n",
      "|    fps                |      164  |\n",
      "|    episodes           |     4300  |\n",
      "|    time_elapsed       |     1674  |\n",
      "|    total_timesteps    |   275200  |\n",
      "| train/                |           |\n",
      "|    loss               |   -0.015  |\n",
      "|    policy_loss        |   -0.013  |\n",
      "|    value_loss         |    0.002  |\n",
      "|    explained_variance |    0.906  |\n",
      "|    n_updates          |     4300  |\n",
      "| rnd_net_dist/         |           |\n",
      "|    mean_rnd_bonus     |    0.000  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        |   64.000  |\n",
      "|    ep_rew_mean        |    1.001  |\n",
      "|    ep_rew_std         |    0.000  |\n",
      "|    policy_entropy     |    0.027  |\n",
      "|    advantage_mean     |    0.004  |\n",
      "|    advantage_std      |    0.038  |\n",
      "|    aux_loss_mean      |    0.000  |\n",
      "| time/                 |           |\n",
      "|    fps                |      164  |\n",
      "|    episodes           |     4400  |\n",
      "|    time_elapsed       |     1713  |\n",
      "|    total_timesteps    |   281600  |\n",
      "| train/                |           |\n",
      "|    loss               |   -0.001  |\n",
      "|    policy_loss        |    0.001  |\n",
      "|    value_loss         |    0.001  |\n",
      "|    explained_variance |    0.919  |\n",
      "|    n_updates          |     4400  |\n",
      "| rnd_net_dist/         |           |\n",
      "|    mean_rnd_bonus     |    0.000  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        |   64.000  |\n",
      "|    ep_rew_mean        |    1.001  |\n",
      "|    ep_rew_std         |    0.000  |\n",
      "|    policy_entropy     |    0.034  |\n",
      "|    advantage_mean     |   -0.003  |\n",
      "|    advantage_std      |    0.030  |\n",
      "|    aux_loss_mean      |    0.000  |\n",
      "| time/                 |           |\n",
      "|    fps                |      164  |\n",
      "|    episodes           |     4500  |\n",
      "|    time_elapsed       |     1751  |\n",
      "|    total_timesteps    |   288000  |\n",
      "| train/                |           |\n",
      "|    loss               |   -0.029  |\n",
      "|    policy_loss        |   -0.026  |\n",
      "|    value_loss         |    0.001  |\n",
      "|    explained_variance |    0.950  |\n",
      "|    n_updates          |     4500  |\n",
      "| rnd_net_dist/         |           |\n",
      "|    mean_rnd_bonus     |    0.000  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        |   64.000  |\n",
      "|    ep_rew_mean        |    1.001  |\n",
      "|    ep_rew_std         |    0.000  |\n",
      "|    policy_entropy     |    0.034  |\n",
      "|    advantage_mean     |   -0.002  |\n",
      "|    advantage_std      |    0.035  |\n",
      "|    aux_loss_mean      |    0.000  |\n",
      "| time/                 |           |\n",
      "|    fps                |      164  |\n",
      "|    episodes           |     4600  |\n",
      "|    time_elapsed       |     1790  |\n",
      "|    total_timesteps    |   294400  |\n",
      "| train/                |           |\n",
      "|    loss               |   -0.035  |\n",
      "|    policy_loss        |   -0.032  |\n",
      "|    value_loss         |    0.001  |\n",
      "|    explained_variance |    0.933  |\n",
      "|    n_updates          |     4600  |\n",
      "| rnd_net_dist/         |           |\n",
      "|    mean_rnd_bonus     |    0.000  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        |   64.000  |\n",
      "|    ep_rew_mean        |    0.981  |\n",
      "|    ep_rew_std         |    0.199  |\n",
      "|    policy_entropy     |    0.016  |\n",
      "|    advantage_mean     |   -0.014  |\n",
      "|    advantage_std      |    0.039  |\n",
      "|    aux_loss_mean      |    0.000  |\n",
      "| time/                 |           |\n",
      "|    fps                |      164  |\n",
      "|    episodes           |     4700  |\n",
      "|    time_elapsed       |     1829  |\n",
      "|    total_timesteps    |   300800  |\n",
      "| train/                |           |\n",
      "|    loss               |    0.006  |\n",
      "|    policy_loss        |    0.007  |\n",
      "|    value_loss         |    0.002  |\n",
      "|    explained_variance |    0.905  |\n",
      "|    n_updates          |     4700  |\n",
      "| rnd_net_dist/         |           |\n",
      "|    mean_rnd_bonus     |    0.000  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        |   64.000  |\n",
      "|    ep_rew_mean        |    1.001  |\n",
      "|    ep_rew_std         |    0.000  |\n",
      "|    policy_entropy     |    0.037  |\n",
      "|    advantage_mean     |    0.006  |\n",
      "|    advantage_std      |    0.037  |\n",
      "|    aux_loss_mean      |    0.000  |\n",
      "| time/                 |           |\n",
      "|    fps                |      164  |\n",
      "|    episodes           |     4800  |\n",
      "|    time_elapsed       |     1868  |\n",
      "|    total_timesteps    |   307200  |\n",
      "| train/                |           |\n",
      "|    loss               |   -0.014  |\n",
      "|    policy_loss        |   -0.011  |\n",
      "|    value_loss         |    0.001  |\n",
      "|    explained_variance |    0.925  |\n",
      "|    n_updates          |     4800  |\n",
      "| rnd_net_dist/         |           |\n",
      "|    mean_rnd_bonus     |    0.000  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        |   64.000  |\n",
      "|    ep_rew_mean        |    0.981  |\n",
      "|    ep_rew_std         |    0.199  |\n",
      "|    policy_entropy     |    0.016  |\n",
      "|    advantage_mean     |   -0.032  |\n",
      "|    advantage_std      |    0.032  |\n",
      "|    aux_loss_mean      |    0.000  |\n",
      "| time/                 |           |\n",
      "|    fps                |      164  |\n",
      "|    episodes           |     4900  |\n",
      "|    time_elapsed       |     1906  |\n",
      "|    total_timesteps    |   313600  |\n",
      "| train/                |           |\n",
      "|    loss               |    0.015  |\n",
      "|    policy_loss        |    0.015  |\n",
      "|    value_loss         |    0.002  |\n",
      "|    explained_variance |    0.925  |\n",
      "|    n_updates          |     4900  |\n",
      "| rnd_net_dist/         |           |\n",
      "|    mean_rnd_bonus     |    0.000  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        |   64.000  |\n",
      "|    ep_rew_mean        |    1.001  |\n",
      "|    ep_rew_std         |    0.000  |\n",
      "|    policy_entropy     |    0.023  |\n",
      "|    advantage_mean     |   -0.003  |\n",
      "|    advantage_std      |    0.027  |\n",
      "|    aux_loss_mean      |    0.000  |\n",
      "| time/                 |           |\n",
      "|    fps                |      164  |\n",
      "|    episodes           |     5000  |\n",
      "|    time_elapsed       |     1945  |\n",
      "|    total_timesteps    |   320000  |\n",
      "| train/                |           |\n",
      "|    loss               |    0.001  |\n",
      "|    policy_loss        |    0.003  |\n",
      "|    value_loss         |    0.001  |\n",
      "|    explained_variance |    0.959  |\n",
      "|    n_updates          |     5000  |\n",
      "| rnd_net_dist/         |           |\n",
      "|    mean_rnd_bonus     |    0.000  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        |   64.000  |\n",
      "|    ep_rew_mean        |    1.001  |\n",
      "|    ep_rew_std         |    0.000  |\n",
      "|    policy_entropy     |    0.019  |\n",
      "|    advantage_mean     |    0.006  |\n",
      "|    advantage_std      |    0.029  |\n",
      "|    aux_loss_mean      |    0.000  |\n",
      "| time/                 |           |\n",
      "|    fps                |      164  |\n",
      "|    episodes           |     5100  |\n",
      "|    time_elapsed       |     1984  |\n",
      "|    total_timesteps    |   326400  |\n",
      "| train/                |           |\n",
      "|    loss               |   -0.032  |\n",
      "|    policy_loss        |   -0.031  |\n",
      "|    value_loss         |    0.001  |\n",
      "|    explained_variance |    0.955  |\n",
      "|    n_updates          |     5100  |\n",
      "| rnd_net_dist/         |           |\n",
      "|    mean_rnd_bonus     |    0.000  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        |   64.000  |\n",
      "|    ep_rew_mean        |    1.001  |\n",
      "|    ep_rew_std         |    0.000  |\n",
      "|    policy_entropy     |    0.019  |\n",
      "|    advantage_mean     |   -0.002  |\n",
      "|    advantage_std      |    0.028  |\n",
      "|    aux_loss_mean      |    0.000  |\n",
      "| time/                 |           |\n",
      "|    fps                |      164  |\n",
      "|    episodes           |     5200  |\n",
      "|    time_elapsed       |     2025  |\n",
      "|    total_timesteps    |   332800  |\n",
      "| train/                |           |\n",
      "|    loss               |    0.031  |\n",
      "|    policy_loss        |    0.032  |\n",
      "|    value_loss         |    0.001  |\n",
      "|    explained_variance |    0.960  |\n",
      "|    n_updates          |     5200  |\n",
      "| rnd_net_dist/         |           |\n",
      "|    mean_rnd_bonus     |    0.000  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        |   64.000  |\n",
      "|    ep_rew_mean        |    1.001  |\n",
      "|    ep_rew_std         |    0.000  |\n",
      "|    policy_entropy     |    0.023  |\n",
      "|    advantage_mean     |    0.002  |\n",
      "|    advantage_std      |    0.028  |\n",
      "|    aux_loss_mean      |    0.000  |\n",
      "| time/                 |           |\n",
      "|    fps                |      164  |\n",
      "|    episodes           |     5300  |\n",
      "|    time_elapsed       |     2064  |\n",
      "|    total_timesteps    |   339200  |\n",
      "| train/                |           |\n",
      "|    loss               |    0.010  |\n",
      "|    policy_loss        |    0.012  |\n",
      "|    value_loss         |    0.001  |\n",
      "|    explained_variance |    0.958  |\n",
      "|    n_updates          |     5300  |\n",
      "| rnd_net_dist/         |           |\n",
      "|    mean_rnd_bonus     |    0.000  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        |   64.000  |\n",
      "|    ep_rew_mean        |    1.001  |\n",
      "|    ep_rew_std         |    0.000  |\n",
      "|    policy_entropy     |    0.020  |\n",
      "|    advantage_mean     |    0.004  |\n",
      "|    advantage_std      |    0.031  |\n",
      "|    aux_loss_mean      |    0.000  |\n",
      "| time/                 |           |\n",
      "|    fps                |      164  |\n",
      "|    episodes           |     5400  |\n",
      "|    time_elapsed       |     2103  |\n",
      "|    total_timesteps    |   345600  |\n",
      "| train/                |           |\n",
      "|    loss               |    0.025  |\n",
      "|    policy_loss        |    0.026  |\n",
      "|    value_loss         |    0.001  |\n",
      "|    explained_variance |    0.948  |\n",
      "|    n_updates          |     5400  |\n",
      "| rnd_net_dist/         |           |\n",
      "|    mean_rnd_bonus     |    0.000  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        |   64.000  |\n",
      "|    ep_rew_mean        |    1.001  |\n",
      "|    ep_rew_std         |    0.000  |\n",
      "|    policy_entropy     |    0.014  |\n",
      "|    advantage_mean     |   -0.004  |\n",
      "|    advantage_std      |    0.023  |\n",
      "|    aux_loss_mean      |    0.000  |\n",
      "| time/                 |           |\n",
      "|    fps                |      164  |\n",
      "|    episodes           |     5500  |\n",
      "|    time_elapsed       |     2142  |\n",
      "|    total_timesteps    |   352000  |\n",
      "| train/                |           |\n",
      "|    loss               |    0.013  |\n",
      "|    policy_loss        |    0.014  |\n",
      "|    value_loss         |    0.001  |\n",
      "|    explained_variance |    0.970  |\n",
      "|    n_updates          |     5500  |\n",
      "| rnd_net_dist/         |           |\n",
      "|    mean_rnd_bonus     |    0.000  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        |   64.000  |\n",
      "|    ep_rew_mean        |    1.001  |\n",
      "|    ep_rew_std         |    0.000  |\n",
      "|    policy_entropy     |    0.023  |\n",
      "|    advantage_mean     |   -0.003  |\n",
      "|    advantage_std      |    0.025  |\n",
      "|    aux_loss_mean      |    0.000  |\n",
      "| time/                 |           |\n",
      "|    fps                |      164  |\n",
      "|    episodes           |     5600  |\n",
      "|    time_elapsed       |     2181  |\n",
      "|    total_timesteps    |   358400  |\n",
      "| train/                |           |\n",
      "|    loss               |   -0.000  |\n",
      "|    policy_loss        |    0.001  |\n",
      "|    value_loss         |    0.001  |\n",
      "|    explained_variance |    0.965  |\n",
      "|    n_updates          |     5600  |\n",
      "| rnd_net_dist/         |           |\n",
      "|    mean_rnd_bonus     |    0.000  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        |   64.000  |\n",
      "|    ep_rew_mean        |    1.001  |\n",
      "|    ep_rew_std         |    0.000  |\n",
      "|    policy_entropy     |    0.022  |\n",
      "|    advantage_mean     |    0.001  |\n",
      "|    advantage_std      |    0.032  |\n",
      "|    aux_loss_mean      |    0.000  |\n",
      "| time/                 |           |\n",
      "|    fps                |      164  |\n",
      "|    episodes           |     5700  |\n",
      "|    time_elapsed       |     2220  |\n",
      "|    total_timesteps    |   364800  |\n",
      "| train/                |           |\n",
      "|    loss               |    0.027  |\n",
      "|    policy_loss        |    0.029  |\n",
      "|    value_loss         |    0.001  |\n",
      "|    explained_variance |    0.944  |\n",
      "|    n_updates          |     5700  |\n",
      "| rnd_net_dist/         |           |\n",
      "|    mean_rnd_bonus     |    0.000  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        |   64.000  |\n",
      "|    ep_rew_mean        |    1.001  |\n",
      "|    ep_rew_std         |    0.000  |\n",
      "|    policy_entropy     |    0.022  |\n",
      "|    advantage_mean     |   -0.005  |\n",
      "|    advantage_std      |    0.026  |\n",
      "|    aux_loss_mean      |    0.000  |\n",
      "| time/                 |           |\n",
      "|    fps                |      164  |\n",
      "|    episodes           |     5800  |\n",
      "|    time_elapsed       |     2259  |\n",
      "|    total_timesteps    |   371200  |\n",
      "| train/                |           |\n",
      "|    loss               |   -0.014  |\n",
      "|    policy_loss        |   -0.012  |\n",
      "|    value_loss         |    0.001  |\n",
      "|    explained_variance |    0.963  |\n",
      "|    n_updates          |     5800  |\n",
      "| rnd_net_dist/         |           |\n",
      "|    mean_rnd_bonus     |    0.000  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        |   64.000  |\n",
      "|    ep_rew_mean        |    0.981  |\n",
      "|    ep_rew_std         |    0.199  |\n",
      "|    policy_entropy     |    0.012  |\n",
      "|    advantage_mean     |    0.002  |\n",
      "|    advantage_std      |    0.029  |\n",
      "|    aux_loss_mean      |    0.000  |\n",
      "| time/                 |           |\n",
      "|    fps                |      164  |\n",
      "|    episodes           |     5900  |\n",
      "|    time_elapsed       |     2298  |\n",
      "|    total_timesteps    |   377600  |\n",
      "| train/                |           |\n",
      "|    loss               |   -0.014  |\n",
      "|    policy_loss        |   -0.013  |\n",
      "|    value_loss         |    0.001  |\n",
      "|    explained_variance |    0.957  |\n",
      "|    n_updates          |     5900  |\n",
      "| rnd_net_dist/         |           |\n",
      "|    mean_rnd_bonus     |    0.000  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        |   64.000  |\n",
      "|    ep_rew_mean        |    1.001  |\n",
      "|    ep_rew_std         |    0.000  |\n",
      "|    policy_entropy     |    0.012  |\n",
      "|    advantage_mean     |    0.000  |\n",
      "|    advantage_std      |    0.028  |\n",
      "|    aux_loss_mean      |    0.000  |\n",
      "| time/                 |           |\n",
      "|    fps                |      164  |\n",
      "|    episodes           |     6000  |\n",
      "|    time_elapsed       |     2337  |\n",
      "|    total_timesteps    |   384000  |\n",
      "| train/                |           |\n",
      "|    loss               |   -0.012  |\n",
      "|    policy_loss        |   -0.011  |\n",
      "|    value_loss         |    0.001  |\n",
      "|    explained_variance |    0.957  |\n",
      "|    n_updates          |     6000  |\n",
      "| rnd_net_dist/         |           |\n",
      "|    mean_rnd_bonus     |    0.000  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        |   64.000  |\n",
      "|    ep_rew_mean        |    0.981  |\n",
      "|    ep_rew_std         |    0.199  |\n",
      "|    policy_entropy     |    0.013  |\n",
      "|    advantage_mean     |   -0.006  |\n",
      "|    advantage_std      |    0.023  |\n",
      "|    aux_loss_mean      |    0.000  |\n",
      "| time/                 |           |\n",
      "|    fps                |      164  |\n",
      "|    episodes           |     6100  |\n",
      "|    time_elapsed       |     2376  |\n",
      "|    total_timesteps    |   390400  |\n",
      "| train/                |           |\n",
      "|    loss               |    0.006  |\n",
      "|    policy_loss        |    0.007  |\n",
      "|    value_loss         |    0.001  |\n",
      "|    explained_variance |    0.969  |\n",
      "|    n_updates          |     6100  |\n",
      "| rnd_net_dist/         |           |\n",
      "|    mean_rnd_bonus     |    0.000  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        |   64.000  |\n",
      "|    ep_rew_mean        |    0.981  |\n",
      "|    ep_rew_std         |    0.199  |\n",
      "|    policy_entropy     |    0.012  |\n",
      "|    advantage_mean     |    0.008  |\n",
      "|    advantage_std      |    0.026  |\n",
      "|    aux_loss_mean      |    0.000  |\n",
      "| time/                 |           |\n",
      "|    fps                |      164  |\n",
      "|    episodes           |     6200  |\n",
      "|    time_elapsed       |     2415  |\n",
      "|    total_timesteps    |   396800  |\n",
      "| train/                |           |\n",
      "|    loss               |    0.007  |\n",
      "|    policy_loss        |    0.008  |\n",
      "|    value_loss         |    0.001  |\n",
      "|    explained_variance |    0.965  |\n",
      "|    n_updates          |     6200  |\n",
      "| rnd_net_dist/         |           |\n",
      "|    mean_rnd_bonus     |    0.000  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        |   64.000  |\n",
      "|    ep_rew_mean        |    1.001  |\n",
      "|    ep_rew_std         |    0.000  |\n",
      "|    policy_entropy     |    0.012  |\n",
      "|    advantage_mean     |    0.002  |\n",
      "|    advantage_std      |    0.030  |\n",
      "|    aux_loss_mean      |    0.000  |\n",
      "| time/                 |           |\n",
      "|    fps                |      164  |\n",
      "|    episodes           |     6300  |\n",
      "|    time_elapsed       |     2454  |\n",
      "|    total_timesteps    |   403200  |\n",
      "| train/                |           |\n",
      "|    loss               |    0.013  |\n",
      "|    policy_loss        |    0.014  |\n",
      "|    value_loss         |    0.001  |\n",
      "|    explained_variance |    0.951  |\n",
      "|    n_updates          |     6300  |\n",
      "| rnd_net_dist/         |           |\n",
      "|    mean_rnd_bonus     |    0.000  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        |   64.000  |\n",
      "|    ep_rew_mean        |    1.001  |\n",
      "|    ep_rew_std         |    0.000  |\n",
      "|    policy_entropy     |    0.012  |\n",
      "|    advantage_mean     |   -0.003  |\n",
      "|    advantage_std      |    0.023  |\n",
      "|    aux_loss_mean      |    0.000  |\n",
      "| time/                 |           |\n",
      "|    fps                |      164  |\n",
      "|    episodes           |     6400  |\n",
      "|    time_elapsed       |     2492  |\n",
      "|    total_timesteps    |   409600  |\n",
      "| train/                |           |\n",
      "|    loss               |    0.013  |\n",
      "|    policy_loss        |    0.014  |\n",
      "|    value_loss         |    0.001  |\n",
      "|    explained_variance |    0.971  |\n",
      "|    n_updates          |     6400  |\n",
      "| rnd_net_dist/         |           |\n",
      "|    mean_rnd_bonus     |    0.000  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        |   64.000  |\n",
      "|    ep_rew_mean        |    1.001  |\n",
      "|    ep_rew_std         |    0.000  |\n",
      "|    policy_entropy     |    0.029  |\n",
      "|    advantage_mean     |   -0.007  |\n",
      "|    advantage_std      |    0.025  |\n",
      "|    aux_loss_mean      |    0.000  |\n",
      "| time/                 |           |\n",
      "|    fps                |      164  |\n",
      "|    episodes           |     6500  |\n",
      "|    time_elapsed       |     2532  |\n",
      "|    total_timesteps    |   416000  |\n",
      "| train/                |           |\n",
      "|    loss               |   -0.014  |\n",
      "|    policy_loss        |   -0.011  |\n",
      "|    value_loss         |    0.001  |\n",
      "|    explained_variance |    0.966  |\n",
      "|    n_updates          |     6500  |\n",
      "| rnd_net_dist/         |           |\n",
      "|    mean_rnd_bonus     |    0.000  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        |   64.000  |\n",
      "|    ep_rew_mean        |    1.001  |\n",
      "|    ep_rew_std         |    0.000  |\n",
      "|    policy_entropy     |    0.011  |\n",
      "|    advantage_mean     |    0.002  |\n",
      "|    advantage_std      |    0.022  |\n",
      "|    aux_loss_mean      |    0.000  |\n",
      "| time/                 |           |\n",
      "|    fps                |      164  |\n",
      "|    episodes           |     6600  |\n",
      "|    time_elapsed       |     2570  |\n",
      "|    total_timesteps    |   422400  |\n",
      "| train/                |           |\n",
      "|    loss               |    0.017  |\n",
      "|    policy_loss        |    0.018  |\n",
      "|    value_loss         |    0.000  |\n",
      "|    explained_variance |    0.975  |\n",
      "|    n_updates          |     6600  |\n",
      "| rnd_net_dist/         |           |\n",
      "|    mean_rnd_bonus     |    0.000  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        |   64.000  |\n",
      "|    ep_rew_mean        |    1.001  |\n",
      "|    ep_rew_std         |    0.000  |\n",
      "|    policy_entropy     |    0.022  |\n",
      "|    advantage_mean     |    0.002  |\n",
      "|    advantage_std      |    0.024  |\n",
      "|    aux_loss_mean      |    0.000  |\n",
      "| time/                 |           |\n",
      "|    fps                |      164  |\n",
      "|    episodes           |     6700  |\n",
      "|    time_elapsed       |     2609  |\n",
      "|    total_timesteps    |   428800  |\n",
      "| train/                |           |\n",
      "|    loss               |    0.000  |\n",
      "|    policy_loss        |    0.002  |\n",
      "|    value_loss         |    0.001  |\n",
      "|    explained_variance |    0.971  |\n",
      "|    n_updates          |     6700  |\n",
      "| rnd_net_dist/         |           |\n",
      "|    mean_rnd_bonus     |    0.000  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        |   64.000  |\n",
      "|    ep_rew_mean        |    1.001  |\n",
      "|    ep_rew_std         |    0.000  |\n",
      "|    policy_entropy     |    0.011  |\n",
      "|    advantage_mean     |   -0.005  |\n",
      "|    advantage_std      |    0.027  |\n",
      "|    aux_loss_mean      |    0.000  |\n",
      "| time/                 |           |\n",
      "|    fps                |      164  |\n",
      "|    episodes           |     6800  |\n",
      "|    time_elapsed       |     2648  |\n",
      "|    total_timesteps    |   435200  |\n",
      "| train/                |           |\n",
      "|    loss               |    0.013  |\n",
      "|    policy_loss        |    0.013  |\n",
      "|    value_loss         |    0.001  |\n",
      "|    explained_variance |    0.959  |\n",
      "|    n_updates          |     6800  |\n",
      "| rnd_net_dist/         |           |\n",
      "|    mean_rnd_bonus     |    0.000  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        |   64.000  |\n",
      "|    ep_rew_mean        |    0.981  |\n",
      "|    ep_rew_std         |    0.199  |\n",
      "|    policy_entropy     |    0.031  |\n",
      "|    advantage_mean     |   -0.049  |\n",
      "|    advantage_std      |    0.024  |\n",
      "|    aux_loss_mean      |    0.000  |\n",
      "| time/                 |           |\n",
      "|    fps                |      164  |\n",
      "|    episodes           |     6900  |\n",
      "|    time_elapsed       |     2687  |\n",
      "|    total_timesteps    |   441600  |\n",
      "| train/                |           |\n",
      "|    loss               |   -0.034  |\n",
      "|    policy_loss        |   -0.033  |\n",
      "|    value_loss         |    0.003  |\n",
      "|    explained_variance |    0.948  |\n",
      "|    n_updates          |     6900  |\n",
      "| rnd_net_dist/         |           |\n",
      "|    mean_rnd_bonus     |    0.000  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        |   64.000  |\n",
      "|    ep_rew_mean        |    1.001  |\n",
      "|    ep_rew_std         |    0.000  |\n",
      "|    policy_entropy     |    0.031  |\n",
      "|    advantage_mean     |    0.001  |\n",
      "|    advantage_std      |    0.022  |\n",
      "|    aux_loss_mean      |    0.000  |\n",
      "| time/                 |           |\n",
      "|    fps                |      164  |\n",
      "|    episodes           |     7000  |\n",
      "|    time_elapsed       |     2726  |\n",
      "|    total_timesteps    |   448000  |\n",
      "| train/                |           |\n",
      "|    loss               |    0.018  |\n",
      "|    policy_loss        |    0.020  |\n",
      "|    value_loss         |    0.000  |\n",
      "|    explained_variance |    0.974  |\n",
      "|    n_updates          |     7000  |\n",
      "| rnd_net_dist/         |           |\n",
      "|    mean_rnd_bonus     |    0.000  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        |   64.000  |\n",
      "|    ep_rew_mean        |    1.001  |\n",
      "|    ep_rew_std         |    0.000  |\n",
      "|    policy_entropy     |    0.020  |\n",
      "|    advantage_mean     |   -0.002  |\n",
      "|    advantage_std      |    0.027  |\n",
      "|    aux_loss_mean      |    0.000  |\n",
      "| time/                 |           |\n",
      "|    fps                |      164  |\n",
      "|    episodes           |     7100  |\n",
      "|    time_elapsed       |     2765  |\n",
      "|    total_timesteps    |   454400  |\n",
      "| train/                |           |\n",
      "|    loss               |    0.020  |\n",
      "|    policy_loss        |    0.021  |\n",
      "|    value_loss         |    0.001  |\n",
      "|    explained_variance |    0.958  |\n",
      "|    n_updates          |     7100  |\n",
      "| rnd_net_dist/         |           |\n",
      "|    mean_rnd_bonus     |    0.000  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        |   64.000  |\n",
      "|    ep_rew_mean        |    0.981  |\n",
      "|    ep_rew_std         |    0.199  |\n",
      "|    policy_entropy     |    0.017  |\n",
      "|    advantage_mean     |    0.001  |\n",
      "|    advantage_std      |    0.030  |\n",
      "|    aux_loss_mean      |    0.000  |\n",
      "| time/                 |           |\n",
      "|    fps                |      164  |\n",
      "|    episodes           |     7200  |\n",
      "|    time_elapsed       |     2804  |\n",
      "|    total_timesteps    |   460800  |\n",
      "| train/                |           |\n",
      "|    loss               |   -0.005  |\n",
      "|    policy_loss        |   -0.004  |\n",
      "|    value_loss         |    0.001  |\n",
      "|    explained_variance |    0.951  |\n",
      "|    n_updates          |     7200  |\n",
      "| rnd_net_dist/         |           |\n",
      "|    mean_rnd_bonus     |    0.000  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        |   64.000  |\n",
      "|    ep_rew_mean        |    1.001  |\n",
      "|    ep_rew_std         |    0.000  |\n",
      "|    policy_entropy     |    0.008  |\n",
      "|    advantage_mean     |   -0.001  |\n",
      "|    advantage_std      |    0.029  |\n",
      "|    aux_loss_mean      |    0.000  |\n",
      "| time/                 |           |\n",
      "|    fps                |      164  |\n",
      "|    episodes           |     7300  |\n",
      "|    time_elapsed       |     2843  |\n",
      "|    total_timesteps    |   467200  |\n",
      "| train/                |           |\n",
      "|    loss               |   -0.011  |\n",
      "|    policy_loss        |   -0.010  |\n",
      "|    value_loss         |    0.001  |\n",
      "|    explained_variance |    0.953  |\n",
      "|    n_updates          |     7300  |\n",
      "| rnd_net_dist/         |           |\n",
      "|    mean_rnd_bonus     |    0.000  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        |   64.000  |\n",
      "|    ep_rew_mean        |    1.001  |\n",
      "|    ep_rew_std         |    0.000  |\n",
      "|    policy_entropy     |    0.017  |\n",
      "|    advantage_mean     |   -0.000  |\n",
      "|    advantage_std      |    0.020  |\n",
      "|    aux_loss_mean      |    0.000  |\n",
      "| time/                 |           |\n",
      "|    fps                |      164  |\n",
      "|    episodes           |     7400  |\n",
      "|    time_elapsed       |     2882  |\n",
      "|    total_timesteps    |   473600  |\n",
      "| train/                |           |\n",
      "|    loss               |    0.004  |\n",
      "|    policy_loss        |    0.005  |\n",
      "|    value_loss         |    0.000  |\n",
      "|    explained_variance |    0.978  |\n",
      "|    n_updates          |     7400  |\n",
      "| rnd_net_dist/         |           |\n",
      "|    mean_rnd_bonus     |    0.000  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        |   64.000  |\n",
      "|    ep_rew_mean        |    1.001  |\n",
      "|    ep_rew_std         |    0.000  |\n",
      "|    policy_entropy     |    0.017  |\n",
      "|    advantage_mean     |   -0.001  |\n",
      "|    advantage_std      |    0.021  |\n",
      "|    aux_loss_mean      |    0.000  |\n",
      "| time/                 |           |\n",
      "|    fps                |      164  |\n",
      "|    episodes           |     7500  |\n",
      "|    time_elapsed       |     2921  |\n",
      "|    total_timesteps    |   480000  |\n",
      "| train/                |           |\n",
      "|    loss               |    0.004  |\n",
      "|    policy_loss        |    0.006  |\n",
      "|    value_loss         |    0.000  |\n",
      "|    explained_variance |    0.975  |\n",
      "|    n_updates          |     7500  |\n",
      "| rnd_net_dist/         |           |\n",
      "|    mean_rnd_bonus     |    0.000  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        |   64.000  |\n",
      "|    ep_rew_mean        |    0.981  |\n",
      "|    ep_rew_std         |    0.199  |\n",
      "|    policy_entropy     |    0.012  |\n",
      "|    advantage_mean     |   -0.000  |\n",
      "|    advantage_std      |    0.024  |\n",
      "|    aux_loss_mean      |    0.000  |\n",
      "| time/                 |           |\n",
      "|    fps                |      164  |\n",
      "|    episodes           |     7600  |\n",
      "|    time_elapsed       |     2960  |\n",
      "|    total_timesteps    |   486400  |\n",
      "| train/                |           |\n",
      "|    loss               |    0.014  |\n",
      "|    policy_loss        |    0.015  |\n",
      "|    value_loss         |    0.001  |\n",
      "|    explained_variance |    0.970  |\n",
      "|    n_updates          |     7600  |\n",
      "| rnd_net_dist/         |           |\n",
      "|    mean_rnd_bonus     |    0.000  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        |   64.000  |\n",
      "|    ep_rew_mean        |    1.001  |\n",
      "|    ep_rew_std         |    0.000  |\n",
      "|    policy_entropy     |    0.011  |\n",
      "|    advantage_mean     |   -0.002  |\n",
      "|    advantage_std      |    0.021  |\n",
      "|    aux_loss_mean      |    0.000  |\n",
      "| time/                 |           |\n",
      "|    fps                |      164  |\n",
      "|    episodes           |     7700  |\n",
      "|    time_elapsed       |     2999  |\n",
      "|    total_timesteps    |   492800  |\n",
      "| train/                |           |\n",
      "|    loss               |   -0.003  |\n",
      "|    policy_loss        |   -0.002  |\n",
      "|    value_loss         |    0.000  |\n",
      "|    explained_variance |    0.976  |\n",
      "|    n_updates          |     7700  |\n",
      "| rnd_net_dist/         |           |\n",
      "|    mean_rnd_bonus     |    0.000  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        |   64.000  |\n",
      "|    ep_rew_mean        |    1.001  |\n",
      "|    ep_rew_std         |    0.000  |\n",
      "|    policy_entropy     |    0.011  |\n",
      "|    advantage_mean     |   -0.000  |\n",
      "|    advantage_std      |    0.024  |\n",
      "|    aux_loss_mean      |    0.000  |\n",
      "| time/                 |           |\n",
      "|    fps                |      164  |\n",
      "|    episodes           |     7800  |\n",
      "|    time_elapsed       |     3037  |\n",
      "|    total_timesteps    |   499200  |\n",
      "| train/                |           |\n",
      "|    loss               |    0.018  |\n",
      "|    policy_loss        |    0.019  |\n",
      "|    value_loss         |    0.001  |\n",
      "|    explained_variance |    0.967  |\n",
      "|    n_updates          |     7800  |\n",
      "| rnd_net_dist/         |           |\n",
      "|    mean_rnd_bonus     |    0.000  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        |   64.000  |\n",
      "|    ep_rew_mean        |    1.001  |\n",
      "|    ep_rew_std         |    0.000  |\n",
      "|    policy_entropy     |    0.011  |\n",
      "|    advantage_mean     |    0.001  |\n",
      "|    advantage_std      |    0.021  |\n",
      "|    aux_loss_mean      |    0.000  |\n",
      "| time/                 |           |\n",
      "|    fps                |      164  |\n",
      "|    episodes           |     7900  |\n",
      "|    time_elapsed       |     3076  |\n",
      "|    total_timesteps    |   505600  |\n",
      "| train/                |           |\n",
      "|    loss               |    0.009  |\n",
      "|    policy_loss        |    0.010  |\n",
      "|    value_loss         |    0.000  |\n",
      "|    explained_variance |    0.976  |\n",
      "|    n_updates          |     7900  |\n",
      "| rnd_net_dist/         |           |\n",
      "|    mean_rnd_bonus     |    0.000  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        |   64.000  |\n",
      "|    ep_rew_mean        |    1.001  |\n",
      "|    ep_rew_std         |    0.000  |\n",
      "|    policy_entropy     |    0.011  |\n",
      "|    advantage_mean     |   -0.001  |\n",
      "|    advantage_std      |    0.020  |\n",
      "|    aux_loss_mean      |    0.000  |\n",
      "| time/                 |           |\n",
      "|    fps                |      164  |\n",
      "|    episodes           |     8000  |\n",
      "|    time_elapsed       |     3115  |\n",
      "|    total_timesteps    |   512000  |\n",
      "| train/                |           |\n",
      "|    loss               |   -0.013  |\n",
      "|    policy_loss        |   -0.012  |\n",
      "|    value_loss         |    0.000  |\n",
      "|    explained_variance |    0.980  |\n",
      "|    n_updates          |     8000  |\n",
      "| rnd_net_dist/         |           |\n",
      "|    mean_rnd_bonus     |    0.000  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        |   64.000  |\n",
      "|    ep_rew_mean        |    1.001  |\n",
      "|    ep_rew_std         |    0.000  |\n",
      "|    policy_entropy     |    0.012  |\n",
      "|    advantage_mean     |    0.001  |\n",
      "|    advantage_std      |    0.020  |\n",
      "|    aux_loss_mean      |    0.000  |\n",
      "| time/                 |           |\n",
      "|    fps                |      164  |\n",
      "|    episodes           |     8100  |\n",
      "|    time_elapsed       |     3154  |\n",
      "|    total_timesteps    |   518400  |\n",
      "| train/                |           |\n",
      "|    loss               |    0.012  |\n",
      "|    policy_loss        |    0.013  |\n",
      "|    value_loss         |    0.000  |\n",
      "|    explained_variance |    0.979  |\n",
      "|    n_updates          |     8100  |\n",
      "| rnd_net_dist/         |           |\n",
      "|    mean_rnd_bonus     |    0.000  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        |   64.000  |\n",
      "|    ep_rew_mean        |    1.001  |\n",
      "|    ep_rew_std         |    0.000  |\n",
      "|    policy_entropy     |    0.011  |\n",
      "|    advantage_mean     |   -0.002  |\n",
      "|    advantage_std      |    0.020  |\n",
      "|    aux_loss_mean      |    0.000  |\n",
      "| time/                 |           |\n",
      "|    fps                |      164  |\n",
      "|    episodes           |     8200  |\n",
      "|    time_elapsed       |     3193  |\n",
      "|    total_timesteps    |   524800  |\n",
      "| train/                |           |\n",
      "|    loss               |   -0.015  |\n",
      "|    policy_loss        |   -0.014  |\n",
      "|    value_loss         |    0.000  |\n",
      "|    explained_variance |    0.979  |\n",
      "|    n_updates          |     8200  |\n",
      "| rnd_net_dist/         |           |\n",
      "|    mean_rnd_bonus     |    0.000  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        |   64.000  |\n",
      "|    ep_rew_mean        |    1.001  |\n",
      "|    ep_rew_std         |    0.000  |\n",
      "|    policy_entropy     |    0.013  |\n",
      "|    advantage_mean     |   -0.002  |\n",
      "|    advantage_std      |    0.024  |\n",
      "|    aux_loss_mean      |    0.000  |\n",
      "| time/                 |           |\n",
      "|    fps                |      164  |\n",
      "|    episodes           |     8300  |\n",
      "|    time_elapsed       |     3232  |\n",
      "|    total_timesteps    |   531200  |\n",
      "| train/                |           |\n",
      "|    loss               |    0.039  |\n",
      "|    policy_loss        |    0.040  |\n",
      "|    value_loss         |    0.001  |\n",
      "|    explained_variance |    0.969  |\n",
      "|    n_updates          |     8300  |\n",
      "| rnd_net_dist/         |           |\n",
      "|    mean_rnd_bonus     |    0.000  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        |   64.000  |\n",
      "|    ep_rew_mean        |    1.001  |\n",
      "|    ep_rew_std         |    0.000  |\n",
      "|    policy_entropy     |    0.011  |\n",
      "|    advantage_mean     |   -0.002  |\n",
      "|    advantage_std      |    0.021  |\n",
      "|    aux_loss_mean      |    0.000  |\n",
      "| time/                 |           |\n",
      "|    fps                |      164  |\n",
      "|    episodes           |     8400  |\n",
      "|    time_elapsed       |     3271  |\n",
      "|    total_timesteps    |   537600  |\n",
      "| train/                |           |\n",
      "|    loss               |    0.064  |\n",
      "|    policy_loss        |    0.065  |\n",
      "|    value_loss         |    0.000  |\n",
      "|    explained_variance |    0.975  |\n",
      "|    n_updates          |     8400  |\n",
      "| rnd_net_dist/         |           |\n",
      "|    mean_rnd_bonus     |    0.000  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        |   64.000  |\n",
      "|    ep_rew_mean        |    1.001  |\n",
      "|    ep_rew_std         |    0.000  |\n",
      "|    policy_entropy     |    0.012  |\n",
      "|    advantage_mean     |   -0.002  |\n",
      "|    advantage_std      |    0.019  |\n",
      "|    aux_loss_mean      |    0.000  |\n",
      "| time/                 |           |\n",
      "|    fps                |      164  |\n",
      "|    episodes           |     8500  |\n",
      "|    time_elapsed       |     3310  |\n",
      "|    total_timesteps    |   544000  |\n",
      "| train/                |           |\n",
      "|    loss               |    0.009  |\n",
      "|    policy_loss        |    0.010  |\n",
      "|    value_loss         |    0.000  |\n",
      "|    explained_variance |    0.980  |\n",
      "|    n_updates          |     8500  |\n",
      "| rnd_net_dist/         |           |\n",
      "|    mean_rnd_bonus     |    0.000  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        |   64.000  |\n",
      "|    ep_rew_mean        |    1.001  |\n",
      "|    ep_rew_std         |    0.000  |\n",
      "|    policy_entropy     |    0.011  |\n",
      "|    advantage_mean     |   -0.001  |\n",
      "|    advantage_std      |    0.020  |\n",
      "|    aux_loss_mean      |    0.000  |\n",
      "| time/                 |           |\n",
      "|    fps                |      164  |\n",
      "|    episodes           |     8600  |\n",
      "|    time_elapsed       |     3349  |\n",
      "|    total_timesteps    |   550400  |\n",
      "| train/                |           |\n",
      "|    loss               |   -0.002  |\n",
      "|    policy_loss        |   -0.001  |\n",
      "|    value_loss         |    0.000  |\n",
      "|    explained_variance |    0.978  |\n",
      "|    n_updates          |     8600  |\n",
      "| rnd_net_dist/         |           |\n",
      "|    mean_rnd_bonus     |    0.000  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        |   64.000  |\n",
      "|    ep_rew_mean        |    1.001  |\n",
      "|    ep_rew_std         |    0.000  |\n",
      "|    policy_entropy     |    0.011  |\n",
      "|    advantage_mean     |   -0.001  |\n",
      "|    advantage_std      |    0.019  |\n",
      "|    aux_loss_mean      |    0.000  |\n",
      "| time/                 |           |\n",
      "|    fps                |      164  |\n",
      "|    episodes           |     8700  |\n",
      "|    time_elapsed       |     3387  |\n",
      "|    total_timesteps    |   556800  |\n",
      "| train/                |           |\n",
      "|    loss               |    0.010  |\n",
      "|    policy_loss        |    0.011  |\n",
      "|    value_loss         |    0.000  |\n",
      "|    explained_variance |    0.980  |\n",
      "|    n_updates          |     8700  |\n",
      "| rnd_net_dist/         |           |\n",
      "|    mean_rnd_bonus     |    0.000  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        |   64.000  |\n",
      "|    ep_rew_mean        |    1.001  |\n",
      "|    ep_rew_std         |    0.000  |\n",
      "|    policy_entropy     |    0.011  |\n",
      "|    advantage_mean     |   -0.001  |\n",
      "|    advantage_std      |    0.019  |\n",
      "|    aux_loss_mean      |    0.000  |\n",
      "| time/                 |           |\n",
      "|    fps                |      164  |\n",
      "|    episodes           |     8800  |\n",
      "|    time_elapsed       |     3426  |\n",
      "|    total_timesteps    |   563200  |\n",
      "| train/                |           |\n",
      "|    loss               |   -0.017  |\n",
      "|    policy_loss        |   -0.016  |\n",
      "|    value_loss         |    0.000  |\n",
      "|    explained_variance |    0.982  |\n",
      "|    n_updates          |     8800  |\n",
      "| rnd_net_dist/         |           |\n",
      "|    mean_rnd_bonus     |    0.000  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        |   64.000  |\n",
      "|    ep_rew_mean        |    1.001  |\n",
      "|    ep_rew_std         |    0.000  |\n",
      "|    policy_entropy     |    0.011  |\n",
      "|    advantage_mean     |   -0.005  |\n",
      "|    advantage_std      |    0.020  |\n",
      "|    aux_loss_mean      |    0.000  |\n",
      "| time/                 |           |\n",
      "|    fps                |      164  |\n",
      "|    episodes           |     8900  |\n",
      "|    time_elapsed       |     3465  |\n",
      "|    total_timesteps    |   569600  |\n",
      "| train/                |           |\n",
      "|    loss               |   -0.034  |\n",
      "|    policy_loss        |   -0.033  |\n",
      "|    value_loss         |    0.000  |\n",
      "|    explained_variance |    0.978  |\n",
      "|    n_updates          |     8900  |\n",
      "| rnd_net_dist/         |           |\n",
      "|    mean_rnd_bonus     |    0.000  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        |   64.000  |\n",
      "|    ep_rew_mean        |    1.001  |\n",
      "|    ep_rew_std         |    0.000  |\n",
      "|    policy_entropy     |    0.011  |\n",
      "|    advantage_mean     |    0.001  |\n",
      "|    advantage_std      |    0.017  |\n",
      "|    aux_loss_mean      |    0.000  |\n",
      "| time/                 |           |\n",
      "|    fps                |      164  |\n",
      "|    episodes           |     9000  |\n",
      "|    time_elapsed       |     3504  |\n",
      "|    total_timesteps    |   576000  |\n",
      "| train/                |           |\n",
      "|    loss               |    0.005  |\n",
      "|    policy_loss        |    0.006  |\n",
      "|    value_loss         |    0.000  |\n",
      "|    explained_variance |    0.984  |\n",
      "|    n_updates          |     9000  |\n",
      "| rnd_net_dist/         |           |\n",
      "|    mean_rnd_bonus     |    0.000  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        |   64.000  |\n",
      "|    ep_rew_mean        |    1.001  |\n",
      "|    ep_rew_std         |    0.000  |\n",
      "|    policy_entropy     |    0.011  |\n",
      "|    advantage_mean     |   -0.002  |\n",
      "|    advantage_std      |    0.015  |\n",
      "|    aux_loss_mean      |    0.000  |\n",
      "| time/                 |           |\n",
      "|    fps                |      164  |\n",
      "|    episodes           |     9100  |\n",
      "|    time_elapsed       |     3543  |\n",
      "|    total_timesteps    |   582400  |\n",
      "| train/                |           |\n",
      "|    loss               |    0.003  |\n",
      "|    policy_loss        |    0.004  |\n",
      "|    value_loss         |    0.000  |\n",
      "|    explained_variance |    0.988  |\n",
      "|    n_updates          |     9100  |\n",
      "| rnd_net_dist/         |           |\n",
      "|    mean_rnd_bonus     |    0.000  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        |   64.000  |\n",
      "|    ep_rew_mean        |    1.001  |\n",
      "|    ep_rew_std         |    0.000  |\n",
      "|    policy_entropy     |    0.011  |\n",
      "|    advantage_mean     |   -0.003  |\n",
      "|    advantage_std      |    0.022  |\n",
      "|    aux_loss_mean      |    0.000  |\n",
      "| time/                 |           |\n",
      "|    fps                |      164  |\n",
      "|    episodes           |     9200  |\n",
      "|    time_elapsed       |     3582  |\n",
      "|    total_timesteps    |   588800  |\n",
      "| train/                |           |\n",
      "|    loss               |    0.004  |\n",
      "|    policy_loss        |    0.004  |\n",
      "|    value_loss         |    0.000  |\n",
      "|    explained_variance |    0.974  |\n",
      "|    n_updates          |     9200  |\n",
      "| rnd_net_dist/         |           |\n",
      "|    mean_rnd_bonus     |    0.000  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        |   64.000  |\n",
      "|    ep_rew_mean        |    1.001  |\n",
      "|    ep_rew_std         |    0.000  |\n",
      "|    policy_entropy     |    0.012  |\n",
      "|    advantage_mean     |   -0.001  |\n",
      "|    advantage_std      |    0.016  |\n",
      "|    aux_loss_mean      |    0.000  |\n",
      "| time/                 |           |\n",
      "|    fps                |      164  |\n",
      "|    episodes           |     9300  |\n",
      "|    time_elapsed       |     3621  |\n",
      "|    total_timesteps    |   595200  |\n",
      "| train/                |           |\n",
      "|    loss               |   -0.009  |\n",
      "|    policy_loss        |   -0.008  |\n",
      "|    value_loss         |    0.000  |\n",
      "|    explained_variance |    0.987  |\n",
      "|    n_updates          |     9300  |\n",
      "| rnd_net_dist/         |           |\n",
      "|    mean_rnd_bonus     |    0.000  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        |   64.000  |\n",
      "|    ep_rew_mean        |    1.001  |\n",
      "|    ep_rew_std         |    0.000  |\n",
      "|    policy_entropy     |    0.012  |\n",
      "|    advantage_mean     |    0.001  |\n",
      "|    advantage_std      |    0.018  |\n",
      "|    aux_loss_mean      |    0.000  |\n",
      "| time/                 |           |\n",
      "|    fps                |      164  |\n",
      "|    episodes           |     9400  |\n",
      "|    time_elapsed       |     3660  |\n",
      "|    total_timesteps    |   601600  |\n",
      "| train/                |           |\n",
      "|    loss               |    0.012  |\n",
      "|    policy_loss        |    0.013  |\n",
      "|    value_loss         |    0.000  |\n",
      "|    explained_variance |    0.983  |\n",
      "|    n_updates          |     9400  |\n",
      "| rnd_net_dist/         |           |\n",
      "|    mean_rnd_bonus     |    0.000  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        |   64.000  |\n",
      "|    ep_rew_mean        |    1.001  |\n",
      "|    ep_rew_std         |    0.000  |\n",
      "|    policy_entropy     |    0.011  |\n",
      "|    advantage_mean     |   -0.006  |\n",
      "|    advantage_std      |    0.022  |\n",
      "|    aux_loss_mean      |    0.000  |\n",
      "| time/                 |           |\n",
      "|    fps                |      164  |\n",
      "|    episodes           |     9500  |\n",
      "|    time_elapsed       |     3699  |\n",
      "|    total_timesteps    |   608000  |\n",
      "| train/                |           |\n",
      "|    loss               |    0.027  |\n",
      "|    policy_loss        |    0.028  |\n",
      "|    value_loss         |    0.001  |\n",
      "|    explained_variance |    0.972  |\n",
      "|    n_updates          |     9500  |\n",
      "| rnd_net_dist/         |           |\n",
      "|    mean_rnd_bonus     |    0.000  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        |   64.000  |\n",
      "|    ep_rew_mean        |    1.001  |\n",
      "|    ep_rew_std         |    0.000  |\n",
      "|    policy_entropy     |    0.011  |\n",
      "|    advantage_mean     |   -0.002  |\n",
      "|    advantage_std      |    0.018  |\n",
      "|    aux_loss_mean      |    0.000  |\n",
      "| time/                 |           |\n",
      "|    fps                |      164  |\n",
      "|    episodes           |     9600  |\n",
      "|    time_elapsed       |     3738  |\n",
      "|    total_timesteps    |   614400  |\n",
      "| train/                |           |\n",
      "|    loss               |    0.000  |\n",
      "|    policy_loss        |    0.001  |\n",
      "|    value_loss         |    0.000  |\n",
      "|    explained_variance |    0.982  |\n",
      "|    n_updates          |     9600  |\n",
      "| rnd_net_dist/         |           |\n",
      "|    mean_rnd_bonus     |    0.000  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        |   64.000  |\n",
      "|    ep_rew_mean        |    1.001  |\n",
      "|    ep_rew_std         |    0.000  |\n",
      "|    policy_entropy     |    0.010  |\n",
      "|    advantage_mean     |   -0.003  |\n",
      "|    advantage_std      |    0.018  |\n",
      "|    aux_loss_mean      |    0.000  |\n",
      "| time/                 |           |\n",
      "|    fps                |      164  |\n",
      "|    episodes           |     9700  |\n",
      "|    time_elapsed       |     3777  |\n",
      "|    total_timesteps    |   620800  |\n",
      "| train/                |           |\n",
      "|    loss               |   -0.008  |\n",
      "|    policy_loss        |   -0.007  |\n",
      "|    value_loss         |    0.000  |\n",
      "|    explained_variance |    0.982  |\n",
      "|    n_updates          |     9700  |\n",
      "| rnd_net_dist/         |           |\n",
      "|    mean_rnd_bonus     |    0.000  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        |   64.000  |\n",
      "|    ep_rew_mean        |    1.001  |\n",
      "|    ep_rew_std         |    0.000  |\n",
      "|    policy_entropy     |    0.011  |\n",
      "|    advantage_mean     |    0.001  |\n",
      "|    advantage_std      |    0.020  |\n",
      "|    aux_loss_mean      |    0.000  |\n",
      "| time/                 |           |\n",
      "|    fps                |      164  |\n",
      "|    episodes           |     9800  |\n",
      "|    time_elapsed       |     3816  |\n",
      "|    total_timesteps    |   627200  |\n",
      "| train/                |           |\n",
      "|    loss               |    0.013  |\n",
      "|    policy_loss        |    0.014  |\n",
      "|    value_loss         |    0.000  |\n",
      "|    explained_variance |    0.979  |\n",
      "|    n_updates          |     9800  |\n",
      "| rnd_net_dist/         |           |\n",
      "|    mean_rnd_bonus     |    0.000  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        |   64.000  |\n",
      "|    ep_rew_mean        |    1.001  |\n",
      "|    ep_rew_std         |    0.000  |\n",
      "|    policy_entropy     |    0.011  |\n",
      "|    advantage_mean     |   -0.001  |\n",
      "|    advantage_std      |    0.017  |\n",
      "|    aux_loss_mean      |    0.000  |\n",
      "| time/                 |           |\n",
      "|    fps                |      164  |\n",
      "|    episodes           |     9900  |\n",
      "|    time_elapsed       |     3854  |\n",
      "|    total_timesteps    |   633600  |\n",
      "| train/                |           |\n",
      "|    loss               |   -0.014  |\n",
      "|    policy_loss        |   -0.013  |\n",
      "|    value_loss         |    0.000  |\n",
      "|    explained_variance |    0.984  |\n",
      "|    n_updates          |     9900  |\n",
      "| rnd_net_dist/         |           |\n",
      "|    mean_rnd_bonus     |    0.000  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        |   64.000  |\n",
      "|    ep_rew_mean        |    1.001  |\n",
      "|    ep_rew_std         |    0.000  |\n",
      "|    policy_entropy     |    0.010  |\n",
      "|    advantage_mean     |    0.001  |\n",
      "|    advantage_std      |    0.019  |\n",
      "|    aux_loss_mean      |    0.000  |\n",
      "| time/                 |           |\n",
      "|    fps                |      164  |\n",
      "|    episodes           |    10000  |\n",
      "|    time_elapsed       |     3894  |\n",
      "|    total_timesteps    |   640000  |\n",
      "| train/                |           |\n",
      "|    loss               |   -0.002  |\n",
      "|    policy_loss        |   -0.001  |\n",
      "|    value_loss         |    0.000  |\n",
      "|    explained_variance |    0.982  |\n",
      "|    n_updates          |    10000  |\n",
      "| rnd_net_dist/         |           |\n",
      "|    mean_rnd_bonus     |    0.000  |\n",
      "-------------------------------------\n",
      "Training complete. Total episodes: 10000, total steps: 640000\n"
     ]
    }
   ],
   "source": [
    "delay = 64\n",
    "env = MemoryTaskEnv(delay=delay, difficulty=0)\n",
    "mem_dim = 32 #64*1.5\n",
    "\n",
    "aux_modules = [\n",
    "    CueAuxModule(feat_dim=mem_dim*2, n_classes=2),\n",
    "    ConfidenceAuxModule(feat_dim=mem_dim*2)\n",
    "]\n",
    "\n",
    "policy = MemoryTransformerPolicy  # Or your patched ExternalMemoryTransformerPolicy\n",
    "\n",
    "memory = StrategicMemoryBuffer(\n",
    "    obs_dim=env.observation_space.shape[0],\n",
    "    action_dim=1,        # Discrete(2)\n",
    "    mem_dim=mem_dim,\n",
    "    max_entries=16,\n",
    "    device=\"cpu\"\n",
    ")\n",
    "\n",
    "policy = MemoryTransformerPolicy  # Or your patched ExternalMemoryTransformerPolicy\n",
    "\n",
    "\n",
    "\n",
    "agent = ExternalMemoryPPO(\n",
    "    policy_class=policy,\n",
    "    use_rnd=True,\n",
    "    env=env,\n",
    "    memory=memory,\n",
    "    #aux_modules=aux_modules,\n",
    "    device=\"cpu\",\n",
    "    #learning_rate=1e-4,\n",
    "    her=False,\n",
    "    verbose=1,\n",
    "    reward_norm=False,\n",
    "    ent_coef=0.1\n",
    ")\n",
    "\n",
    "agent.learn(total_timesteps=delay*10_000, log_interval=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2231e50-71fc-4e60-91d4-8131ad107179",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e34381f1-8083-4fc2-a46d-da0e63208b6e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d75a6741-2324-4682-b9e2-eb094f5c39e6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aec2e0ee-49d1-4bf9-80c6-21261428608e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2107cdd1-0ebd-420f-a199-dbf24f57d118",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ccdf85b-f939-4213-a5e4-0962c1380a25",
   "metadata": {},
   "outputs": [],
   "source": [
    "xxxxxxxxxxxxx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b52fee7-70c0-4105-997d-4fed045f73db",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d354fdc0-b3a4-4830-93ac-26deb1e10718",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "\n",
    "class MarketHiddenRegimeMemoryEnv(gym.Env):\n",
    "    \"\"\"\n",
    "    Custom Gymnasium environment:\n",
    "    - At reset, samples a hidden regime {0: bull, 1: bear, 2: neutral}.\n",
    "    - For N steps, agent observes noisy price changes (regime only weakly affects drift).\n",
    "    - On final step, agent must pick regime; reward=1 if correct, else 0.\n",
    "    - Agent can only win if it remembers early cues.\n",
    "    \"\"\"\n",
    "    metadata = {\"render_modes\": [\"human\"]}\n",
    "    def __init__(self, n_steps=40, obs_noise=1.0, drift_strength=0.05, seed=None):\n",
    "        super().__init__()\n",
    "        self.n_steps = n_steps\n",
    "        self.obs_noise = obs_noise\n",
    "        self.drift_strength = drift_strength\n",
    "        self.rng = np.random.default_rng(seed)\n",
    "        # Observations: [price_change] (can add more features)\n",
    "        self.observation_space = gym.spaces.Box(low=-5, high=5, shape=(1,), dtype=np.float32)\n",
    "        # Actions: Only on last step: 0=bull, 1=bear, 2=neutral\n",
    "        self.action_space = gym.spaces.Discrete(3)\n",
    "        self.current_step = 0\n",
    "        self.regime = None\n",
    "        self.price = 0.0\n",
    "\n",
    "    def reset(self, *, seed=None, options=None):\n",
    "        super().reset(seed=seed)\n",
    "        self.current_step = 0\n",
    "        self.regime = int(self.rng.integers(0, 3))  # 0,1,2\n",
    "        self.price = 0.0\n",
    "        # Optionally: stronger initial drift for \"hint\"\n",
    "        self.drift = {0: +self.drift_strength, 1: -self.drift_strength, 2: 0.0}[self.regime]\n",
    "        obs = np.array([self._next_price()], dtype=np.float32)\n",
    "        return obs, {}\n",
    "\n",
    "    def _next_price(self):\n",
    "        # Regime determines drift, but noise dominates\n",
    "        noise = self.rng.normal(0, self.obs_noise)\n",
    "        drift = self.drift\n",
    "        price_change = drift + noise\n",
    "        self.price += price_change\n",
    "        return price_change\n",
    "\n",
    "    def step(self, action):\n",
    "        self.current_step += 1\n",
    "        done = (self.current_step >= self.n_steps)\n",
    "        reward = 0.0\n",
    "        # Only give reward at the end, based on regime guess\n",
    "        if done:\n",
    "            reward = 1.0 if int(action) == self.regime else 0.0\n",
    "        obs = np.array([self._next_price()], dtype=np.float32) if not done else np.zeros((1,), dtype=np.float32)\n",
    "        info = {'regime': self.regime, 'step': self.current_step}\n",
    "        return obs, reward, done, False, info\n",
    "\n",
    "    def render(self, mode=\"human\"):\n",
    "        print(f\"Step {self.current_step}, price={self.price:.2f}, regime={self.regime}\")\n",
    "\n",
    "    def close(self):\n",
    "        pass\n",
    "\n",
    "# Register with Gymnasium for convenience\n",
    "from gymnasium.envs.registration import register\n",
    "register(\n",
    "    id=\"MarketHiddenRegimeMemory-v0\",\n",
    "    entry_point=MarketHiddenRegimeMemoryEnv,\n",
    "    max_episode_steps=40,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "f38af66c-93be-4ba6-9bfe-8ecf6f0d01c1",
   "metadata": {},
   "source": [
    "env = gym.make(\"MarketHiddenRegimeMemory-v0\", n_steps=40)\n",
    "obs, _ = env.reset()\n",
    "done = False\n",
    "while not done:\n",
    "    # action = agent.predict(obs)    # Use your model here!\n",
    "    action = env.action_space.sample()  # Random for test\n",
    "    obs, reward, done, _, info = env.step(action)\n",
    "    env.render()\n",
    "print(\"Final reward:\", reward, \"True regime:\", info[\"regime\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ee20fd1b-59cd-40d8-b033-45887c5fcf60",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        |   40.000  |\n",
      "|    ep_rew_mean        |    0.414  |\n",
      "|    ep_rew_std         |    0.444  |\n",
      "|    policy_entropy     |    0.576  |\n",
      "|    advantage_mean     |   -0.109  |\n",
      "|    advantage_std      |    0.139  |\n",
      "|    aux_loss_mean      |    0.024  |\n",
      "| time/                 |           |\n",
      "|    fps                |      199  |\n",
      "|    episodes           |      100  |\n",
      "|    time_elapsed       |       20  |\n",
      "|    total_timesteps    |     4000  |\n",
      "| train/                |           |\n",
      "|    loss               |   -2.757  |\n",
      "|    policy_loss        |   -2.716  |\n",
      "|    value_loss         |    0.031  |\n",
      "|    explained_variance |   -5.454  |\n",
      "|    n_updates          |      100  |\n",
      "| aux_train/            |           |\n",
      "|    aux_confidence_mse |    0.016  |\n",
      "| rnd_net_dist/         |           |\n",
      "|    mean_rnd_bonus     |    0.015  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        |   40.000  |\n",
      "|    ep_rew_mean        |    0.352  |\n",
      "|    ep_rew_std         |    0.473  |\n",
      "|    policy_entropy     |    0.530  |\n",
      "|    advantage_mean     |    0.200  |\n",
      "|    advantage_std      |    0.208  |\n",
      "|    aux_loss_mean      |    0.021  |\n",
      "| time/                 |           |\n",
      "|    fps                |      199  |\n",
      "|    episodes           |      200  |\n",
      "|    time_elapsed       |       40  |\n",
      "|    total_timesteps    |     8000  |\n",
      "| train/                |           |\n",
      "|    loss               |    4.518  |\n",
      "|    policy_loss        |    4.528  |\n",
      "|    value_loss         |    0.082  |\n",
      "|    explained_variance |   -0.281  |\n",
      "|    n_updates          |      200  |\n",
      "| aux_train/            |           |\n",
      "|    aux_confidence_mse |    0.014  |\n",
      "| rnd_net_dist/         |           |\n",
      "|    mean_rnd_bonus     |    0.003  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        |   40.000  |\n",
      "|    ep_rew_mean        |    0.383  |\n",
      "|    ep_rew_std         |    0.485  |\n",
      "|    policy_entropy     |    0.532  |\n",
      "|    advantage_mean     |    0.254  |\n",
      "|    advantage_std      |    0.207  |\n",
      "|    aux_loss_mean      |    0.021  |\n",
      "| time/                 |           |\n",
      "|    fps                |      196  |\n",
      "|    episodes           |      300  |\n",
      "|    time_elapsed       |       61  |\n",
      "|    total_timesteps    |    12000  |\n",
      "| train/                |           |\n",
      "|    loss               |    5.604  |\n",
      "|    policy_loss        |    5.602  |\n",
      "|    value_loss         |    0.106  |\n",
      "|    explained_variance |   -0.019  |\n",
      "|    n_updates          |      300  |\n",
      "| aux_train/            |           |\n",
      "|    aux_confidence_mse |    0.013  |\n",
      "| rnd_net_dist/         |           |\n",
      "|    mean_rnd_bonus     |    0.001  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        |   40.000  |\n",
      "|    ep_rew_mean        |    0.371  |\n",
      "|    ep_rew_std         |    0.483  |\n",
      "|    policy_entropy     |    0.482  |\n",
      "|    advantage_mean     |    0.273  |\n",
      "|    advantage_std      |    0.204  |\n",
      "|    aux_loss_mean      |    0.024  |\n",
      "| time/                 |           |\n",
      "|    fps                |      195  |\n",
      "|    episodes           |      400  |\n",
      "|    time_elapsed       |       82  |\n",
      "|    total_timesteps    |    16000  |\n",
      "| train/                |           |\n",
      "|    loss               |    4.577  |\n",
      "|    policy_loss        |    4.565  |\n",
      "|    value_loss         |    0.115  |\n",
      "|    explained_variance |    0.098  |\n",
      "|    n_updates          |      400  |\n",
      "| aux_train/            |           |\n",
      "|    aux_confidence_mse |    0.021  |\n",
      "| rnd_net_dist/         |           |\n",
      "|    mean_rnd_bonus     |    0.000  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        |   40.000  |\n",
      "|    ep_rew_mean        |    0.271  |\n",
      "|    ep_rew_std         |    0.444  |\n",
      "|    policy_entropy     |    0.466  |\n",
      "|    advantage_mean     |   -0.151  |\n",
      "|    advantage_std      |    0.115  |\n",
      "|    aux_loss_mean      |    0.026  |\n",
      "| time/                 |           |\n",
      "|    fps                |      194  |\n",
      "|    episodes           |      500  |\n",
      "|    time_elapsed       |      103  |\n",
      "|    total_timesteps    |    20000  |\n",
      "| train/                |           |\n",
      "|    loss               |   -1.889  |\n",
      "|    policy_loss        |   -1.862  |\n",
      "|    value_loss         |    0.036  |\n",
      "|    explained_variance |   -2.090  |\n",
      "|    n_updates          |      500  |\n",
      "| aux_train/            |           |\n",
      "|    aux_confidence_mse |    0.017  |\n",
      "| rnd_net_dist/         |           |\n",
      "|    mean_rnd_bonus     |    0.000  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        |   40.000  |\n",
      "|    ep_rew_mean        |    0.271  |\n",
      "|    ep_rew_std         |    0.444  |\n",
      "|    policy_entropy     |    0.434  |\n",
      "|    advantage_mean     |   -0.082  |\n",
      "|    advantage_std      |    0.111  |\n",
      "|    aux_loss_mean      |    0.022  |\n",
      "| time/                 |           |\n",
      "|    fps                |      195  |\n",
      "|    episodes           |      600  |\n",
      "|    time_elapsed       |      123  |\n",
      "|    total_timesteps    |    24000  |\n",
      "| train/                |           |\n",
      "|    loss               |   -1.078  |\n",
      "|    policy_loss        |   -1.046  |\n",
      "|    value_loss         |    0.019  |\n",
      "|    explained_variance |   -6.254  |\n",
      "|    n_updates          |      600  |\n",
      "| aux_train/            |           |\n",
      "|    aux_confidence_mse |    0.026  |\n",
      "| rnd_net_dist/         |           |\n",
      "|    mean_rnd_bonus     |    0.000  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        |   40.000  |\n",
      "|    ep_rew_mean        |    0.400  |\n",
      "|    ep_rew_std         |    0.490  |\n",
      "|    policy_entropy     |    0.525  |\n",
      "|    advantage_mean     |   -0.192  |\n",
      "|    advantage_std      |    0.112  |\n",
      "|    aux_loss_mean      |    0.020  |\n",
      "| time/                 |           |\n",
      "|    fps                |      194  |\n",
      "|    episodes           |      700  |\n",
      "|    time_elapsed       |      144  |\n",
      "|    total_timesteps    |    28000  |\n",
      "| train/                |           |\n",
      "|    loss               |   -3.361  |\n",
      "|    policy_loss        |   -3.336  |\n",
      "|    value_loss         |    0.049  |\n",
      "|    explained_variance |   -0.695  |\n",
      "|    n_updates          |      700  |\n",
      "| aux_train/            |           |\n",
      "|    aux_confidence_mse |    0.028  |\n",
      "| rnd_net_dist/         |           |\n",
      "|    mean_rnd_bonus     |    0.000  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        |   40.000  |\n",
      "|    ep_rew_mean        |    0.360  |\n",
      "|    ep_rew_std         |    0.480  |\n",
      "|    policy_entropy     |    0.540  |\n",
      "|    advantage_mean     |    0.295  |\n",
      "|    advantage_std      |    0.214  |\n",
      "|    aux_loss_mean      |    0.019  |\n",
      "| time/                 |           |\n",
      "|    fps                |      193  |\n",
      "|    episodes           |      800  |\n",
      "|    time_elapsed       |      165  |\n",
      "|    total_timesteps    |    32000  |\n",
      "| train/                |           |\n",
      "|    loss               |    5.014  |\n",
      "|    policy_loss        |    5.000  |\n",
      "|    value_loss         |    0.132  |\n",
      "|    explained_variance |    0.085  |\n",
      "|    n_updates          |      800  |\n",
      "| aux_train/            |           |\n",
      "|    aux_confidence_mse |    0.017  |\n",
      "| rnd_net_dist/         |           |\n",
      "|    mean_rnd_bonus     |    0.000  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        |   40.000  |\n",
      "|    ep_rew_mean        |    0.350  |\n",
      "|    ep_rew_std         |    0.477  |\n",
      "|    policy_entropy     |    0.583  |\n",
      "|    advantage_mean     |    0.242  |\n",
      "|    advantage_std      |    0.188  |\n",
      "|    aux_loss_mean      |    0.021  |\n",
      "| time/                 |           |\n",
      "|    fps                |      193  |\n",
      "|    episodes           |      900  |\n",
      "|    time_elapsed       |      186  |\n",
      "|    total_timesteps    |    36000  |\n",
      "| train/                |           |\n",
      "|    loss               |    5.308  |\n",
      "|    policy_loss        |    5.318  |\n",
      "|    value_loss         |    0.093  |\n",
      "|    explained_variance |    0.132  |\n",
      "|    n_updates          |      900  |\n",
      "| aux_train/            |           |\n",
      "|    aux_confidence_mse |    0.020  |\n",
      "| rnd_net_dist/         |           |\n",
      "|    mean_rnd_bonus     |    0.000  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        |   40.000  |\n",
      "|    ep_rew_mean        |    0.260  |\n",
      "|    ep_rew_std         |    0.439  |\n",
      "|    policy_entropy     |    0.496  |\n",
      "|    advantage_mean     |   -0.047  |\n",
      "|    advantage_std      |    0.090  |\n",
      "|    aux_loss_mean      |    0.021  |\n",
      "| time/                 |           |\n",
      "|    fps                |      193  |\n",
      "|    episodes           |     1000  |\n",
      "|    time_elapsed       |      207  |\n",
      "|    total_timesteps    |    40000  |\n",
      "| train/                |           |\n",
      "|    loss               |   -0.591  |\n",
      "|    policy_loss        |   -0.548  |\n",
      "|    value_loss         |    0.010  |\n",
      "|    explained_variance |  -19.527  |\n",
      "|    n_updates          |     1000  |\n",
      "| aux_train/            |           |\n",
      "|    aux_confidence_mse |    0.016  |\n",
      "| rnd_net_dist/         |           |\n",
      "|    mean_rnd_bonus     |    0.000  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        |   40.000  |\n",
      "|    ep_rew_mean        |    0.340  |\n",
      "|    ep_rew_std         |    0.474  |\n",
      "|    policy_entropy     |    0.440  |\n",
      "|    advantage_mean     |   -0.149  |\n",
      "|    advantage_std      |    0.119  |\n",
      "|    aux_loss_mean      |    0.028  |\n",
      "| time/                 |           |\n",
      "|    fps                |      192  |\n",
      "|    episodes           |     1100  |\n",
      "|    time_elapsed       |      229  |\n",
      "|    total_timesteps    |    44000  |\n",
      "| train/                |           |\n",
      "|    loss               |   -3.735  |\n",
      "|    policy_loss        |   -3.712  |\n",
      "|    value_loss         |    0.036  |\n",
      "|    explained_variance |   -1.857  |\n",
      "|    n_updates          |     1100  |\n",
      "| aux_train/            |           |\n",
      "|    aux_confidence_mse |    0.023  |\n",
      "| rnd_net_dist/         |           |\n",
      "|    mean_rnd_bonus     |    0.000  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        |   40.000  |\n",
      "|    ep_rew_mean        |    0.300  |\n",
      "|    ep_rew_std         |    0.458  |\n",
      "|    policy_entropy     |    0.402  |\n",
      "|    advantage_mean     |    0.289  |\n",
      "|    advantage_std      |    0.222  |\n",
      "|    aux_loss_mean      |    0.025  |\n",
      "| time/                 |           |\n",
      "|    fps                |      191  |\n",
      "|    episodes           |     1200  |\n",
      "|    time_elapsed       |      250  |\n",
      "|    total_timesteps    |    48000  |\n",
      "| train/                |           |\n",
      "|    loss               |    5.515  |\n",
      "|    policy_loss        |    5.486  |\n",
      "|    value_loss         |    0.132  |\n",
      "|    explained_variance |   -0.076  |\n",
      "|    n_updates          |     1200  |\n",
      "| aux_train/            |           |\n",
      "|    aux_confidence_mse |    0.031  |\n",
      "| rnd_net_dist/         |           |\n",
      "|    mean_rnd_bonus     |    0.000  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        |   40.000  |\n",
      "|    ep_rew_mean        |    0.330  |\n",
      "|    ep_rew_std         |    0.470  |\n",
      "|    policy_entropy     |    0.462  |\n",
      "|    advantage_mean     |    0.267  |\n",
      "|    advantage_std      |    0.166  |\n",
      "|    aux_loss_mean      |    0.026  |\n",
      "| time/                 |           |\n",
      "|    fps                |      191  |\n",
      "|    episodes           |     1300  |\n",
      "|    time_elapsed       |      271  |\n",
      "|    total_timesteps    |    52000  |\n",
      "| train/                |           |\n",
      "|    loss               |    3.508  |\n",
      "|    policy_loss        |    3.502  |\n",
      "|    value_loss         |    0.098  |\n",
      "|    explained_variance |    0.380  |\n",
      "|    n_updates          |     1300  |\n",
      "| aux_train/            |           |\n",
      "|    aux_confidence_mse |    0.024  |\n",
      "| rnd_net_dist/         |           |\n",
      "|    mean_rnd_bonus     |    0.000  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        |   40.000  |\n",
      "|    ep_rew_mean        |    0.400  |\n",
      "|    ep_rew_std         |    0.490  |\n",
      "|    policy_entropy     |    0.418  |\n",
      "|    advantage_mean     |   -0.163  |\n",
      "|    advantage_std      |    0.129  |\n",
      "|    aux_loss_mean      |    0.020  |\n",
      "| time/                 |           |\n",
      "|    fps                |      191  |\n",
      "|    episodes           |     1400  |\n",
      "|    time_elapsed       |      292  |\n",
      "|    total_timesteps    |    56000  |\n",
      "| train/                |           |\n",
      "|    loss               |   -3.863  |\n",
      "|    policy_loss        |   -3.845  |\n",
      "|    value_loss         |    0.043  |\n",
      "|    explained_variance |   -1.989  |\n",
      "|    n_updates          |     1400  |\n",
      "| aux_train/            |           |\n",
      "|    aux_confidence_mse |    0.024  |\n",
      "| rnd_net_dist/         |           |\n",
      "|    mean_rnd_bonus     |    0.000  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        |   40.000  |\n",
      "|    ep_rew_mean        |    0.330  |\n",
      "|    ep_rew_std         |    0.470  |\n",
      "|    policy_entropy     |    0.500  |\n",
      "|    advantage_mean     |   -0.168  |\n",
      "|    advantage_std      |    0.118  |\n",
      "|    aux_loss_mean      |    0.018  |\n",
      "| time/                 |           |\n",
      "|    fps                |      191  |\n",
      "|    episodes           |     1500  |\n",
      "|    time_elapsed       |      313  |\n",
      "|    total_timesteps    |    60000  |\n",
      "| train/                |           |\n",
      "|    loss               |   -3.602  |\n",
      "|    policy_loss        |   -3.575  |\n",
      "|    value_loss         |    0.042  |\n",
      "|    explained_variance |   -1.274  |\n",
      "|    n_updates          |     1500  |\n",
      "| aux_train/            |           |\n",
      "|    aux_confidence_mse |    0.015  |\n",
      "| rnd_net_dist/         |           |\n",
      "|    mean_rnd_bonus     |    0.000  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        |   40.000  |\n",
      "|    ep_rew_mean        |    0.260  |\n",
      "|    ep_rew_std         |    0.439  |\n",
      "|    policy_entropy     |    0.472  |\n",
      "|    advantage_mean     |   -0.121  |\n",
      "|    advantage_std      |    0.105  |\n",
      "|    aux_loss_mean      |    0.016  |\n",
      "| time/                 |           |\n",
      "|    fps                |      191  |\n",
      "|    episodes           |     1600  |\n",
      "|    time_elapsed       |      334  |\n",
      "|    total_timesteps    |    64000  |\n",
      "| train/                |           |\n",
      "|    loss               |   -2.069  |\n",
      "|    policy_loss        |   -2.037  |\n",
      "|    value_loss         |    0.025  |\n",
      "|    explained_variance |   -2.741  |\n",
      "|    n_updates          |     1600  |\n",
      "| aux_train/            |           |\n",
      "|    aux_confidence_mse |    0.018  |\n",
      "| rnd_net_dist/         |           |\n",
      "|    mean_rnd_bonus     |    0.000  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        |   40.000  |\n",
      "|    ep_rew_mean        |    0.270  |\n",
      "|    ep_rew_std         |    0.444  |\n",
      "|    policy_entropy     |    0.442  |\n",
      "|    advantage_mean     |   -0.089  |\n",
      "|    advantage_std      |    0.083  |\n",
      "|    aux_loss_mean      |    0.018  |\n",
      "| time/                 |           |\n",
      "|    fps                |      191  |\n",
      "|    episodes           |     1700  |\n",
      "|    time_elapsed       |      356  |\n",
      "|    total_timesteps    |    68000  |\n",
      "| train/                |           |\n",
      "|    loss               |   -1.661  |\n",
      "|    policy_loss        |   -1.626  |\n",
      "|    value_loss         |    0.015  |\n",
      "|    explained_variance |   -3.688  |\n",
      "|    n_updates          |     1700  |\n",
      "| aux_train/            |           |\n",
      "|    aux_confidence_mse |    0.019  |\n",
      "| rnd_net_dist/         |           |\n",
      "|    mean_rnd_bonus     |    0.000  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        |   40.000  |\n",
      "|    ep_rew_mean        |    0.370  |\n",
      "|    ep_rew_std         |    0.483  |\n",
      "|    policy_entropy     |    0.514  |\n",
      "|    advantage_mean     |   -0.136  |\n",
      "|    advantage_std      |    0.118  |\n",
      "|    aux_loss_mean      |    0.017  |\n",
      "| time/                 |           |\n",
      "|    fps                |      190  |\n",
      "|    episodes           |     1800  |\n",
      "|    time_elapsed       |      377  |\n",
      "|    total_timesteps    |    72000  |\n",
      "| train/                |           |\n",
      "|    loss               |   -2.516  |\n",
      "|    policy_loss        |   -2.482  |\n",
      "|    value_loss         |    0.032  |\n",
      "|    explained_variance |   -3.223  |\n",
      "|    n_updates          |     1800  |\n",
      "| aux_train/            |           |\n",
      "|    aux_confidence_mse |    0.015  |\n",
      "| rnd_net_dist/         |           |\n",
      "|    mean_rnd_bonus     |    0.000  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        |   40.000  |\n",
      "|    ep_rew_mean        |    0.380  |\n",
      "|    ep_rew_std         |    0.485  |\n",
      "|    policy_entropy     |    0.539  |\n",
      "|    advantage_mean     |    0.191  |\n",
      "|    advantage_std      |    0.155  |\n",
      "|    aux_loss_mean      |    0.019  |\n",
      "| time/                 |           |\n",
      "|    fps                |      190  |\n",
      "|    episodes           |     1900  |\n",
      "|    time_elapsed       |      398  |\n",
      "|    total_timesteps    |    76000  |\n",
      "| train/                |           |\n",
      "|    loss               |    4.399  |\n",
      "|    policy_loss        |    4.421  |\n",
      "|    value_loss         |    0.060  |\n",
      "|    explained_variance |    0.230  |\n",
      "|    n_updates          |     1900  |\n",
      "| aux_train/            |           |\n",
      "|    aux_confidence_mse |    0.015  |\n",
      "| rnd_net_dist/         |           |\n",
      "|    mean_rnd_bonus     |    0.000  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        |   40.000  |\n",
      "|    ep_rew_mean        |    0.380  |\n",
      "|    ep_rew_std         |    0.485  |\n",
      "|    policy_entropy     |    0.499  |\n",
      "|    advantage_mean     |   -0.161  |\n",
      "|    advantage_std      |    0.116  |\n",
      "|    aux_loss_mean      |    0.016  |\n",
      "| time/                 |           |\n",
      "|    fps                |      190  |\n",
      "|    episodes           |     2000  |\n",
      "|    time_elapsed       |      419  |\n",
      "|    total_timesteps    |    80000  |\n",
      "| train/                |           |\n",
      "|    loss               |   -3.185  |\n",
      "|    policy_loss        |   -3.156  |\n",
      "|    value_loss         |    0.039  |\n",
      "|    explained_variance |   -1.460  |\n",
      "|    n_updates          |     2000  |\n",
      "| aux_train/            |           |\n",
      "|    aux_confidence_mse |    0.018  |\n",
      "| rnd_net_dist/         |           |\n",
      "|    mean_rnd_bonus     |    0.000  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        |   40.000  |\n",
      "|    ep_rew_mean        |    0.380  |\n",
      "|    ep_rew_std         |    0.485  |\n",
      "|    policy_entropy     |    0.538  |\n",
      "|    advantage_mean     |   -0.178  |\n",
      "|    advantage_std      |    0.128  |\n",
      "|    aux_loss_mean      |    0.015  |\n",
      "| time/                 |           |\n",
      "|    fps                |      190  |\n",
      "|    episodes           |     2100  |\n",
      "|    time_elapsed       |      440  |\n",
      "|    total_timesteps    |    84000  |\n",
      "| train/                |           |\n",
      "|    loss               |   -4.320  |\n",
      "|    policy_loss        |   -4.291  |\n",
      "|    value_loss         |    0.048  |\n",
      "|    explained_variance |   -1.657  |\n",
      "|    n_updates          |     2100  |\n",
      "| aux_train/            |           |\n",
      "|    aux_confidence_mse |    0.015  |\n",
      "| rnd_net_dist/         |           |\n",
      "|    mean_rnd_bonus     |    0.000  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        |   40.000  |\n",
      "|    ep_rew_mean        |    0.290  |\n",
      "|    ep_rew_std         |    0.454  |\n",
      "|    policy_entropy     |    0.641  |\n",
      "|    advantage_mean     |   -0.106  |\n",
      "|    advantage_std      |    0.097  |\n",
      "|    aux_loss_mean      |    0.016  |\n",
      "| time/                 |           |\n",
      "|    fps                |      190  |\n",
      "|    episodes           |     2200  |\n",
      "|    time_elapsed       |      461  |\n",
      "|    total_timesteps    |    88000  |\n",
      "| train/                |           |\n",
      "|    loss               |   -2.583  |\n",
      "|    policy_loss        |   -2.531  |\n",
      "|    value_loss         |    0.021  |\n",
      "|    explained_variance |   -3.421  |\n",
      "|    n_updates          |     2200  |\n",
      "| aux_train/            |           |\n",
      "|    aux_confidence_mse |    0.017  |\n",
      "| rnd_net_dist/         |           |\n",
      "|    mean_rnd_bonus     |    0.000  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        |   40.000  |\n",
      "|    ep_rew_mean        |    0.340  |\n",
      "|    ep_rew_std         |    0.474  |\n",
      "|    policy_entropy     |    0.638  |\n",
      "|    advantage_mean     |    0.279  |\n",
      "|    advantage_std      |    0.200  |\n",
      "|    aux_loss_mean      |    0.016  |\n",
      "| time/                 |           |\n",
      "|    fps                |      190  |\n",
      "|    episodes           |     2300  |\n",
      "|    time_elapsed       |      482  |\n",
      "|    total_timesteps    |    92000  |\n",
      "| train/                |           |\n",
      "|    loss               |    7.640  |\n",
      "|    policy_loss        |    7.644  |\n",
      "|    value_loss         |    0.117  |\n",
      "|    explained_variance |    0.128  |\n",
      "|    n_updates          |     2300  |\n",
      "| aux_train/            |           |\n",
      "|    aux_confidence_mse |    0.015  |\n",
      "| rnd_net_dist/         |           |\n",
      "|    mean_rnd_bonus     |    0.000  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        |   40.000  |\n",
      "|    ep_rew_mean        |    0.360  |\n",
      "|    ep_rew_std         |    0.480  |\n",
      "|    policy_entropy     |    0.610  |\n",
      "|    advantage_mean     |   -0.161  |\n",
      "|    advantage_std      |    0.121  |\n",
      "|    aux_loss_mean      |    0.014  |\n",
      "| time/                 |           |\n",
      "|    fps                |      190  |\n",
      "|    episodes           |     2400  |\n",
      "|    time_elapsed       |      503  |\n",
      "|    total_timesteps    |    96000  |\n",
      "| train/                |           |\n",
      "|    loss               |   -4.358  |\n",
      "|    policy_loss        |   -4.318  |\n",
      "|    value_loss         |    0.040  |\n",
      "|    explained_variance |   -2.327  |\n",
      "|    n_updates          |     2400  |\n",
      "| aux_train/            |           |\n",
      "|    aux_confidence_mse |    0.011  |\n",
      "| rnd_net_dist/         |           |\n",
      "|    mean_rnd_bonus     |    0.000  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        |   40.000  |\n",
      "|    ep_rew_mean        |    0.320  |\n",
      "|    ep_rew_std         |    0.466  |\n",
      "|    policy_entropy     |    0.645  |\n",
      "|    advantage_mean     |    0.208  |\n",
      "|    advantage_std      |    0.190  |\n",
      "|    aux_loss_mean      |    0.014  |\n",
      "| time/                 |           |\n",
      "|    fps                |      190  |\n",
      "|    episodes           |     2500  |\n",
      "|    time_elapsed       |      524  |\n",
      "|    total_timesteps    |   100000  |\n",
      "| train/                |           |\n",
      "|    loss               |    6.363  |\n",
      "|    policy_loss        |    6.387  |\n",
      "|    value_loss         |    0.079  |\n",
      "|    explained_variance |   -0.024  |\n",
      "|    n_updates          |     2500  |\n",
      "| aux_train/            |           |\n",
      "|    aux_confidence_mse |    0.011  |\n",
      "| rnd_net_dist/         |           |\n",
      "|    mean_rnd_bonus     |    0.000  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        |   40.000  |\n",
      "|    ep_rew_mean        |    0.300  |\n",
      "|    ep_rew_std         |    0.458  |\n",
      "|    policy_entropy     |    0.650  |\n",
      "|    advantage_mean     |   -0.146  |\n",
      "|    advantage_std      |    0.129  |\n",
      "|    aux_loss_mean      |    0.012  |\n",
      "| time/                 |           |\n",
      "|    fps                |      190  |\n",
      "|    episodes           |     2600  |\n",
      "|    time_elapsed       |      546  |\n",
      "|    total_timesteps    |   104000  |\n",
      "| train/                |           |\n",
      "|    loss               |   -3.758  |\n",
      "|    policy_loss        |   -3.713  |\n",
      "|    value_loss         |    0.037  |\n",
      "|    explained_variance |   -3.503  |\n",
      "|    n_updates          |     2600  |\n",
      "| aux_train/            |           |\n",
      "|    aux_confidence_mse |    0.012  |\n",
      "| rnd_net_dist/         |           |\n",
      "|    mean_rnd_bonus     |    0.000  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        |   40.000  |\n",
      "|    ep_rew_mean        |    0.320  |\n",
      "|    ep_rew_std         |    0.466  |\n",
      "|    policy_entropy     |    0.655  |\n",
      "|    advantage_mean     |   -0.111  |\n",
      "|    advantage_std      |    0.099  |\n",
      "|    aux_loss_mean      |    0.009  |\n",
      "| time/                 |           |\n",
      "|    fps                |      190  |\n",
      "|    episodes           |     2700  |\n",
      "|    time_elapsed       |      567  |\n",
      "|    total_timesteps    |   108000  |\n",
      "| train/                |           |\n",
      "|    loss               |   -2.872  |\n",
      "|    policy_loss        |   -2.818  |\n",
      "|    value_loss         |    0.022  |\n",
      "|    explained_variance |   -3.838  |\n",
      "|    n_updates          |     2700  |\n",
      "| aux_train/            |           |\n",
      "|    aux_confidence_mse |    0.007  |\n",
      "| rnd_net_dist/         |           |\n",
      "|    mean_rnd_bonus     |    0.000  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        |   40.000  |\n",
      "|    ep_rew_mean        |    0.350  |\n",
      "|    ep_rew_std         |    0.477  |\n",
      "|    policy_entropy     |    0.647  |\n",
      "|    advantage_mean     |   -0.129  |\n",
      "|    advantage_std      |    0.119  |\n",
      "|    aux_loss_mean      |    0.010  |\n",
      "| time/                 |           |\n",
      "|    fps                |      190  |\n",
      "|    episodes           |     2800  |\n",
      "|    time_elapsed       |      588  |\n",
      "|    total_timesteps    |   112000  |\n",
      "| train/                |           |\n",
      "|    loss               |   -3.134  |\n",
      "|    policy_loss        |   -3.086  |\n",
      "|    value_loss         |    0.030  |\n",
      "|    explained_variance |   -3.867  |\n",
      "|    n_updates          |     2800  |\n",
      "| aux_train/            |           |\n",
      "|    aux_confidence_mse |    0.014  |\n",
      "| rnd_net_dist/         |           |\n",
      "|    mean_rnd_bonus     |    0.000  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        |   40.000  |\n",
      "|    ep_rew_mean        |    0.320  |\n",
      "|    ep_rew_std         |    0.466  |\n",
      "|    policy_entropy     |    0.647  |\n",
      "|    advantage_mean     |   -0.132  |\n",
      "|    advantage_std      |    0.104  |\n",
      "|    aux_loss_mean      |    0.011  |\n",
      "| time/                 |           |\n",
      "|    fps                |      190  |\n",
      "|    episodes           |     2900  |\n",
      "|    time_elapsed       |      609  |\n",
      "|    total_timesteps    |   116000  |\n",
      "| train/                |           |\n",
      "|    loss               |   -3.513  |\n",
      "|    policy_loss        |   -3.463  |\n",
      "|    value_loss         |    0.028  |\n",
      "|    explained_variance |   -2.153  |\n",
      "|    n_updates          |     2900  |\n",
      "| aux_train/            |           |\n",
      "|    aux_confidence_mse |    0.010  |\n",
      "| rnd_net_dist/         |           |\n",
      "|    mean_rnd_bonus     |    0.000  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        |   40.000  |\n",
      "|    ep_rew_mean        |    0.310  |\n",
      "|    ep_rew_std         |    0.462  |\n",
      "|    policy_entropy     |    0.657  |\n",
      "|    advantage_mean     |   -0.119  |\n",
      "|    advantage_std      |    0.104  |\n",
      "|    aux_loss_mean      |    0.008  |\n",
      "| time/                 |           |\n",
      "|    fps                |      190  |\n",
      "|    episodes           |     3000  |\n",
      "|    time_elapsed       |      630  |\n",
      "|    total_timesteps    |   120000  |\n",
      "| train/                |           |\n",
      "|    loss               |   -3.198  |\n",
      "|    policy_loss        |   -3.145  |\n",
      "|    value_loss         |    0.025  |\n",
      "|    explained_variance |   -2.726  |\n",
      "|    n_updates          |     3000  |\n",
      "| aux_train/            |           |\n",
      "|    aux_confidence_mse |    0.007  |\n",
      "| rnd_net_dist/         |           |\n",
      "|    mean_rnd_bonus     |    0.000  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        |   40.000  |\n",
      "|    ep_rew_mean        |    0.330  |\n",
      "|    ep_rew_std         |    0.470  |\n",
      "|    policy_entropy     |    0.651  |\n",
      "|    advantage_mean     |    0.237  |\n",
      "|    advantage_std      |    0.207  |\n",
      "|    aux_loss_mean      |    0.006  |\n",
      "| time/                 |           |\n",
      "|    fps                |      190  |\n",
      "|    episodes           |     3100  |\n",
      "|    time_elapsed       |      651  |\n",
      "|    total_timesteps    |   124000  |\n",
      "| train/                |           |\n",
      "|    loss               |    5.789  |\n",
      "|    policy_loss        |    5.805  |\n",
      "|    value_loss         |    0.098  |\n",
      "|    explained_variance |   -0.131  |\n",
      "|    n_updates          |     3100  |\n",
      "| aux_train/            |           |\n",
      "|    aux_confidence_mse |    0.004  |\n",
      "| rnd_net_dist/         |           |\n",
      "|    mean_rnd_bonus     |    0.000  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        |   40.000  |\n",
      "|    ep_rew_mean        |    0.340  |\n",
      "|    ep_rew_std         |    0.474  |\n",
      "|    policy_entropy     |    0.585  |\n",
      "|    advantage_mean     |   -0.140  |\n",
      "|    advantage_std      |    0.122  |\n",
      "|    aux_loss_mean      |    0.004  |\n",
      "| time/                 |           |\n",
      "|    fps                |      190  |\n",
      "|    episodes           |     3200  |\n",
      "|    time_elapsed       |      673  |\n",
      "|    total_timesteps    |   128000  |\n",
      "| train/                |           |\n",
      "|    loss               |   -3.635  |\n",
      "|    policy_loss        |   -3.594  |\n",
      "|    value_loss         |    0.034  |\n",
      "|    explained_variance |   -2.682  |\n",
      "|    n_updates          |     3200  |\n",
      "| aux_train/            |           |\n",
      "|    aux_confidence_mse |    0.004  |\n",
      "| rnd_net_dist/         |           |\n",
      "|    mean_rnd_bonus     |    0.000  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        |   40.000  |\n",
      "|    ep_rew_mean        |    0.400  |\n",
      "|    ep_rew_std         |    0.490  |\n",
      "|    policy_entropy     |    0.634  |\n",
      "|    advantage_mean     |    0.192  |\n",
      "|    advantage_std      |    0.150  |\n",
      "|    aux_loss_mean      |    0.004  |\n",
      "| time/                 |           |\n",
      "|    fps                |      190  |\n",
      "|    episodes           |     3300  |\n",
      "|    time_elapsed       |      694  |\n",
      "|    total_timesteps    |   132000  |\n",
      "| train/                |           |\n",
      "|    loss               |    4.200  |\n",
      "|    policy_loss        |    4.234  |\n",
      "|    value_loss         |    0.059  |\n",
      "|    explained_variance |    0.320  |\n",
      "|    n_updates          |     3300  |\n",
      "| aux_train/            |           |\n",
      "|    aux_confidence_mse |    0.005  |\n",
      "| rnd_net_dist/         |           |\n",
      "|    mean_rnd_bonus     |    0.000  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        |   40.000  |\n",
      "|    ep_rew_mean        |    0.320  |\n",
      "|    ep_rew_std         |    0.466  |\n",
      "|    policy_entropy     |    0.645  |\n",
      "|    advantage_mean     |   -0.090  |\n",
      "|    advantage_std      |    0.083  |\n",
      "|    aux_loss_mean      |    0.005  |\n",
      "| time/                 |           |\n",
      "|    fps                |      190  |\n",
      "|    episodes           |     3400  |\n",
      "|    time_elapsed       |      715  |\n",
      "|    total_timesteps    |   136000  |\n",
      "| train/                |           |\n",
      "|    loss               |   -2.217  |\n",
      "|    policy_loss        |   -2.161  |\n",
      "|    value_loss         |    0.015  |\n",
      "|    explained_variance |   -3.408  |\n",
      "|    n_updates          |     3400  |\n",
      "| aux_train/            |           |\n",
      "|    aux_confidence_mse |    0.007  |\n",
      "| rnd_net_dist/         |           |\n",
      "|    mean_rnd_bonus     |    0.000  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        |   40.000  |\n",
      "|    ep_rew_mean        |    0.360  |\n",
      "|    ep_rew_std         |    0.480  |\n",
      "|    policy_entropy     |    0.658  |\n",
      "|    advantage_mean     |   -0.118  |\n",
      "|    advantage_std      |    0.100  |\n",
      "|    aux_loss_mean      |    0.007  |\n",
      "| time/                 |           |\n",
      "|    fps                |      189  |\n",
      "|    episodes           |     3500  |\n",
      "|    time_elapsed       |      737  |\n",
      "|    total_timesteps    |   140000  |\n",
      "| train/                |           |\n",
      "|    loss               |   -3.097  |\n",
      "|    policy_loss        |   -3.043  |\n",
      "|    value_loss         |    0.024  |\n",
      "|    explained_variance |   -2.181  |\n",
      "|    n_updates          |     3500  |\n",
      "| aux_train/            |           |\n",
      "|    aux_confidence_mse |    0.006  |\n",
      "| rnd_net_dist/         |           |\n",
      "|    mean_rnd_bonus     |    0.000  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        |   40.000  |\n",
      "|    ep_rew_mean        |    0.410  |\n",
      "|    ep_rew_std         |    0.492  |\n",
      "|    policy_entropy     |    0.633  |\n",
      "|    advantage_mean     |   -0.205  |\n",
      "|    advantage_std      |    0.142  |\n",
      "|    aux_loss_mean      |    0.006  |\n",
      "| time/                 |           |\n",
      "|    fps                |      189  |\n",
      "|    episodes           |     3600  |\n",
      "|    time_elapsed       |      758  |\n",
      "|    total_timesteps    |   144000  |\n",
      "| train/                |           |\n",
      "|    loss               |   -4.558  |\n",
      "|    policy_loss        |   -4.526  |\n",
      "|    value_loss         |    0.062  |\n",
      "|    explained_variance |   -1.159  |\n",
      "|    n_updates          |     3600  |\n",
      "| aux_train/            |           |\n",
      "|    aux_confidence_mse |    0.011  |\n",
      "| rnd_net_dist/         |           |\n",
      "|    mean_rnd_bonus     |    0.000  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        |   40.000  |\n",
      "|    ep_rew_mean        |    0.220  |\n",
      "|    ep_rew_std         |    0.414  |\n",
      "|    policy_entropy     |    0.636  |\n",
      "|    advantage_mean     |    0.324  |\n",
      "|    advantage_std      |    0.212  |\n",
      "|    aux_loss_mean      |    0.006  |\n",
      "| time/                 |           |\n",
      "|    fps                |      189  |\n",
      "|    episodes           |     3700  |\n",
      "|    time_elapsed       |      779  |\n",
      "|    total_timesteps    |   148000  |\n",
      "| train/                |           |\n",
      "|    loss               |    7.921  |\n",
      "|    policy_loss        |    7.909  |\n",
      "|    value_loss         |    0.149  |\n",
      "|    explained_variance |    0.193  |\n",
      "|    n_updates          |     3700  |\n",
      "| aux_train/            |           |\n",
      "|    aux_confidence_mse |    0.005  |\n",
      "| rnd_net_dist/         |           |\n",
      "|    mean_rnd_bonus     |    0.000  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        |   40.000  |\n",
      "|    ep_rew_mean        |    0.320  |\n",
      "|    ep_rew_std         |    0.466  |\n",
      "|    policy_entropy     |    0.637  |\n",
      "|    advantage_mean     |   -0.171  |\n",
      "|    advantage_std      |    0.118  |\n",
      "|    aux_loss_mean      |    0.007  |\n",
      "| time/                 |           |\n",
      "|    fps                |      189  |\n",
      "|    episodes           |     3800  |\n",
      "|    time_elapsed       |      800  |\n",
      "|    total_timesteps    |   152000  |\n",
      "| train/                |           |\n",
      "|    loss               |   -4.126  |\n",
      "|    policy_loss        |   -4.085  |\n",
      "|    value_loss         |    0.043  |\n",
      "|    explained_variance |   -1.556  |\n",
      "|    n_updates          |     3800  |\n",
      "| aux_train/            |           |\n",
      "|    aux_confidence_mse |    0.008  |\n",
      "| rnd_net_dist/         |           |\n",
      "|    mean_rnd_bonus     |    0.000  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        |   40.000  |\n",
      "|    ep_rew_mean        |    0.380  |\n",
      "|    ep_rew_std         |    0.485  |\n",
      "|    policy_entropy     |    0.631  |\n",
      "|    advantage_mean     |   -0.133  |\n",
      "|    advantage_std      |    0.130  |\n",
      "|    aux_loss_mean      |    0.008  |\n",
      "| time/                 |           |\n",
      "|    fps                |      189  |\n",
      "|    episodes           |     3900  |\n",
      "|    time_elapsed       |      822  |\n",
      "|    total_timesteps    |   156000  |\n",
      "| train/                |           |\n",
      "|    loss               |   -3.633  |\n",
      "|    policy_loss        |   -3.588  |\n",
      "|    value_loss         |    0.034  |\n",
      "|    explained_variance |   -3.777  |\n",
      "|    n_updates          |     3900  |\n",
      "| aux_train/            |           |\n",
      "|    aux_confidence_mse |    0.009  |\n",
      "| rnd_net_dist/         |           |\n",
      "|    mean_rnd_bonus     |    0.000  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        |   40.000  |\n",
      "|    ep_rew_mean        |    0.290  |\n",
      "|    ep_rew_std         |    0.454  |\n",
      "|    policy_entropy     |    0.605  |\n",
      "|    advantage_mean     |   -0.112  |\n",
      "|    advantage_std      |    0.097  |\n",
      "|    aux_loss_mean      |    0.006  |\n",
      "| time/                 |           |\n",
      "|    fps                |      189  |\n",
      "|    episodes           |     4000  |\n",
      "|    time_elapsed       |      843  |\n",
      "|    total_timesteps    |   160000  |\n",
      "| train/                |           |\n",
      "|    loss               |   -2.611  |\n",
      "|    policy_loss        |   -2.562  |\n",
      "|    value_loss         |    0.022  |\n",
      "|    explained_variance |   -2.394  |\n",
      "|    n_updates          |     4000  |\n",
      "| aux_train/            |           |\n",
      "|    aux_confidence_mse |    0.006  |\n",
      "| rnd_net_dist/         |           |\n",
      "|    mean_rnd_bonus     |    0.000  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        |   40.000  |\n",
      "|    ep_rew_mean        |    0.410  |\n",
      "|    ep_rew_std         |    0.492  |\n",
      "|    policy_entropy     |    0.511  |\n",
      "|    advantage_mean     |   -0.152  |\n",
      "|    advantage_std      |    0.100  |\n",
      "|    aux_loss_mean      |    0.010  |\n",
      "| time/                 |           |\n",
      "|    fps                |      189  |\n",
      "|    episodes           |     4100  |\n",
      "|    time_elapsed       |      864  |\n",
      "|    total_timesteps    |   164000  |\n",
      "| train/                |           |\n",
      "|    loss               |   -3.485  |\n",
      "|    policy_loss        |   -3.451  |\n",
      "|    value_loss         |    0.033  |\n",
      "|    explained_variance |   -1.055  |\n",
      "|    n_updates          |     4100  |\n",
      "| aux_train/            |           |\n",
      "|    aux_confidence_mse |    0.008  |\n",
      "| rnd_net_dist/         |           |\n",
      "|    mean_rnd_bonus     |    0.000  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        |   40.000  |\n",
      "|    ep_rew_mean        |    0.260  |\n",
      "|    ep_rew_std         |    0.439  |\n",
      "|    policy_entropy     |    0.486  |\n",
      "|    advantage_mean     |   -0.101  |\n",
      "|    advantage_std      |    0.086  |\n",
      "|    aux_loss_mean      |    0.012  |\n",
      "| time/                 |           |\n",
      "|    fps                |      189  |\n",
      "|    episodes           |     4200  |\n",
      "|    time_elapsed       |      885  |\n",
      "|    total_timesteps    |   168000  |\n",
      "| train/                |           |\n",
      "|    loss               |   -2.339  |\n",
      "|    policy_loss        |   -2.301  |\n",
      "|    value_loss         |    0.017  |\n",
      "|    explained_variance |   -2.910  |\n",
      "|    n_updates          |     4200  |\n",
      "| aux_train/            |           |\n",
      "|    aux_confidence_mse |    0.014  |\n",
      "| rnd_net_dist/         |           |\n",
      "|    mean_rnd_bonus     |    0.000  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        |   40.000  |\n",
      "|    ep_rew_mean        |    0.340  |\n",
      "|    ep_rew_std         |    0.474  |\n",
      "|    policy_entropy     |    0.494  |\n",
      "|    advantage_mean     |   -0.112  |\n",
      "|    advantage_std      |    0.110  |\n",
      "|    aux_loss_mean      |    0.013  |\n",
      "| time/                 |           |\n",
      "|    fps                |      189  |\n",
      "|    episodes           |     4300  |\n",
      "|    time_elapsed       |      906  |\n",
      "|    total_timesteps    |   172000  |\n",
      "| train/                |           |\n",
      "|    loss               |   -1.583  |\n",
      "|    policy_loss        |   -1.547  |\n",
      "|    value_loss         |    0.024  |\n",
      "|    explained_variance |   -4.367  |\n",
      "|    n_updates          |     4300  |\n",
      "| aux_train/            |           |\n",
      "|    aux_confidence_mse |    0.013  |\n",
      "| rnd_net_dist/         |           |\n",
      "|    mean_rnd_bonus     |    0.000  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        |   40.000  |\n",
      "|    ep_rew_mean        |    0.400  |\n",
      "|    ep_rew_std         |    0.490  |\n",
      "|    policy_entropy     |    0.446  |\n",
      "|    advantage_mean     |    0.195  |\n",
      "|    advantage_std      |    0.185  |\n",
      "|    aux_loss_mean      |    0.018  |\n",
      "| time/                 |           |\n",
      "|    fps                |      189  |\n",
      "|    episodes           |     4400  |\n",
      "|    time_elapsed       |      927  |\n",
      "|    total_timesteps    |   176000  |\n",
      "| train/                |           |\n",
      "|    loss               |    2.504  |\n",
      "|    policy_loss        |    2.511  |\n",
      "|    value_loss         |    0.071  |\n",
      "|    explained_variance |   -0.051  |\n",
      "|    n_updates          |     4400  |\n",
      "| aux_train/            |           |\n",
      "|    aux_confidence_mse |    0.014  |\n",
      "| rnd_net_dist/         |           |\n",
      "|    mean_rnd_bonus     |    0.000  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        |   40.000  |\n",
      "|    ep_rew_mean        |    0.370  |\n",
      "|    ep_rew_std         |    0.483  |\n",
      "|    policy_entropy     |    0.534  |\n",
      "|    advantage_mean     |   -0.145  |\n",
      "|    advantage_std      |    0.114  |\n",
      "|    aux_loss_mean      |    0.015  |\n",
      "| time/                 |           |\n",
      "|    fps                |      189  |\n",
      "|    episodes           |     4500  |\n",
      "|    time_elapsed       |      949  |\n",
      "|    total_timesteps    |   180000  |\n",
      "| train/                |           |\n",
      "|    loss               |   -2.760  |\n",
      "|    policy_loss        |   -2.724  |\n",
      "|    value_loss         |    0.034  |\n",
      "|    explained_variance |   -2.269  |\n",
      "|    n_updates          |     4500  |\n",
      "| aux_train/            |           |\n",
      "|    aux_confidence_mse |    0.007  |\n",
      "| rnd_net_dist/         |           |\n",
      "|    mean_rnd_bonus     |    0.000  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        |   40.000  |\n",
      "|    ep_rew_mean        |    0.340  |\n",
      "|    ep_rew_std         |    0.474  |\n",
      "|    policy_entropy     |    0.504  |\n",
      "|    advantage_mean     |   -0.131  |\n",
      "|    advantage_std      |    0.100  |\n",
      "|    aux_loss_mean      |    0.010  |\n",
      "| time/                 |           |\n",
      "|    fps                |      189  |\n",
      "|    episodes           |     4600  |\n",
      "|    time_elapsed       |      970  |\n",
      "|    total_timesteps    |   184000  |\n",
      "| train/                |           |\n",
      "|    loss               |   -2.297  |\n",
      "|    policy_loss        |   -2.262  |\n",
      "|    value_loss         |    0.027  |\n",
      "|    explained_variance |   -1.851  |\n",
      "|    n_updates          |     4600  |\n",
      "| aux_train/            |           |\n",
      "|    aux_confidence_mse |    0.011  |\n",
      "| rnd_net_dist/         |           |\n",
      "|    mean_rnd_bonus     |    0.000  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        |   40.000  |\n",
      "|    ep_rew_mean        |    0.290  |\n",
      "|    ep_rew_std         |    0.454  |\n",
      "|    policy_entropy     |    0.524  |\n",
      "|    advantage_mean     |   -0.116  |\n",
      "|    advantage_std      |    0.106  |\n",
      "|    aux_loss_mean      |    0.010  |\n",
      "| time/                 |           |\n",
      "|    fps                |      189  |\n",
      "|    episodes           |     4700  |\n",
      "|    time_elapsed       |      991  |\n",
      "|    total_timesteps    |   188000  |\n",
      "| train/                |           |\n",
      "|    loss               |   -2.001  |\n",
      "|    policy_loss        |   -1.962  |\n",
      "|    value_loss         |    0.024  |\n",
      "|    explained_variance |   -3.038  |\n",
      "|    n_updates          |     4700  |\n",
      "| aux_train/            |           |\n",
      "|    aux_confidence_mse |    0.009  |\n",
      "| rnd_net_dist/         |           |\n",
      "|    mean_rnd_bonus     |    0.000  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        |   40.000  |\n",
      "|    ep_rew_mean        |    0.370  |\n",
      "|    ep_rew_std         |    0.483  |\n",
      "|    policy_entropy     |    0.627  |\n",
      "|    advantage_mean     |   -0.171  |\n",
      "|    advantage_std      |    0.125  |\n",
      "|    aux_loss_mean      |    0.009  |\n",
      "| time/                 |           |\n",
      "|    fps                |      189  |\n",
      "|    episodes           |     4800  |\n",
      "|    time_elapsed       |     1012  |\n",
      "|    total_timesteps    |   192000  |\n",
      "| train/                |           |\n",
      "|    loss               |   -4.142  |\n",
      "|    policy_loss        |   -4.103  |\n",
      "|    value_loss         |    0.044  |\n",
      "|    explained_variance |   -1.677  |\n",
      "|    n_updates          |     4800  |\n",
      "| aux_train/            |           |\n",
      "|    aux_confidence_mse |    0.015  |\n",
      "| rnd_net_dist/         |           |\n",
      "|    mean_rnd_bonus     |    0.000  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        |   40.000  |\n",
      "|    ep_rew_mean        |    0.320  |\n",
      "|    ep_rew_std         |    0.466  |\n",
      "|    policy_entropy     |    0.650  |\n",
      "|    advantage_mean     |   -0.078  |\n",
      "|    advantage_std      |    0.100  |\n",
      "|    aux_loss_mean      |    0.013  |\n",
      "| time/                 |           |\n",
      "|    fps                |      189  |\n",
      "|    episodes           |     4900  |\n",
      "|    time_elapsed       |     1033  |\n",
      "|    total_timesteps    |   196000  |\n",
      "| train/                |           |\n",
      "|    loss               |   -2.138  |\n",
      "|    policy_loss        |   -2.082  |\n",
      "|    value_loss         |    0.016  |\n",
      "|    explained_variance |   -5.844  |\n",
      "|    n_updates          |     4900  |\n",
      "| aux_train/            |           |\n",
      "|    aux_confidence_mse |    0.010  |\n",
      "| rnd_net_dist/         |           |\n",
      "|    mean_rnd_bonus     |    0.000  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        |   40.000  |\n",
      "|    ep_rew_mean        |    0.310  |\n",
      "|    ep_rew_std         |    0.462  |\n",
      "|    policy_entropy     |    0.654  |\n",
      "|    advantage_mean     |   -0.116  |\n",
      "|    advantage_std      |    0.092  |\n",
      "|    aux_loss_mean      |    0.009  |\n",
      "| time/                 |           |\n",
      "|    fps                |      189  |\n",
      "|    episodes           |     5000  |\n",
      "|    time_elapsed       |     1053  |\n",
      "|    total_timesteps    |   200000  |\n",
      "| train/                |           |\n",
      "|    loss               |   -3.355  |\n",
      "|    policy_loss        |   -3.302  |\n",
      "|    value_loss         |    0.022  |\n",
      "|    explained_variance |   -2.302  |\n",
      "|    n_updates          |     5000  |\n",
      "| aux_train/            |           |\n",
      "|    aux_confidence_mse |    0.015  |\n",
      "| rnd_net_dist/         |           |\n",
      "|    mean_rnd_bonus     |    0.000  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        |   40.000  |\n",
      "|    ep_rew_mean        |    0.340  |\n",
      "|    ep_rew_std         |    0.474  |\n",
      "|    policy_entropy     |    0.652  |\n",
      "|    advantage_mean     |   -0.141  |\n",
      "|    advantage_std      |    0.133  |\n",
      "|    aux_loss_mean      |    0.009  |\n",
      "| time/                 |           |\n",
      "|    fps                |      189  |\n",
      "|    episodes           |     5100  |\n",
      "|    time_elapsed       |     1075  |\n",
      "|    total_timesteps    |   204000  |\n",
      "| train/                |           |\n",
      "|    loss               |   -3.994  |\n",
      "|    policy_loss        |   -3.949  |\n",
      "|    value_loss         |    0.037  |\n",
      "|    explained_variance |   -2.708  |\n",
      "|    n_updates          |     5100  |\n",
      "| aux_train/            |           |\n",
      "|    aux_confidence_mse |    0.010  |\n",
      "| rnd_net_dist/         |           |\n",
      "|    mean_rnd_bonus     |    0.000  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        |   40.000  |\n",
      "|    ep_rew_mean        |    0.420  |\n",
      "|    ep_rew_std         |    0.494  |\n",
      "|    policy_entropy     |    0.659  |\n",
      "|    advantage_mean     |   -0.123  |\n",
      "|    advantage_std      |    0.101  |\n",
      "|    aux_loss_mean      |    0.011  |\n",
      "| time/                 |           |\n",
      "|    fps                |      190  |\n",
      "|    episodes           |     5200  |\n",
      "|    time_elapsed       |     1093  |\n",
      "|    total_timesteps    |   208000  |\n",
      "| train/                |           |\n",
      "|    loss               |   -3.435  |\n",
      "|    policy_loss        |   -3.382  |\n",
      "|    value_loss         |    0.025  |\n",
      "|    explained_variance |   -2.460  |\n",
      "|    n_updates          |     5200  |\n",
      "| aux_train/            |           |\n",
      "|    aux_confidence_mse |    0.007  |\n",
      "| rnd_net_dist/         |           |\n",
      "|    mean_rnd_bonus     |    0.000  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        |   40.000  |\n",
      "|    ep_rew_mean        |    0.360  |\n",
      "|    ep_rew_std         |    0.480  |\n",
      "|    policy_entropy     |    0.658  |\n",
      "|    advantage_mean     |   -0.161  |\n",
      "|    advantage_std      |    0.121  |\n",
      "|    aux_loss_mean      |    0.008  |\n",
      "| time/                 |           |\n",
      "|    fps                |      191  |\n",
      "|    episodes           |     5300  |\n",
      "|    time_elapsed       |     1109  |\n",
      "|    total_timesteps    |   212000  |\n",
      "| train/                |           |\n",
      "|    loss               |   -4.730  |\n",
      "|    policy_loss        |   -4.685  |\n",
      "|    value_loss         |    0.040  |\n",
      "|    explained_variance |   -1.874  |\n",
      "|    n_updates          |     5300  |\n",
      "| aux_train/            |           |\n",
      "|    aux_confidence_mse |    0.007  |\n",
      "| rnd_net_dist/         |           |\n",
      "|    mean_rnd_bonus     |    0.000  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        |   40.000  |\n",
      "|    ep_rew_mean        |    0.310  |\n",
      "|    ep_rew_std         |    0.462  |\n",
      "|    policy_entropy     |    0.652  |\n",
      "|    advantage_mean     |    0.262  |\n",
      "|    advantage_std      |    0.189  |\n",
      "|    aux_loss_mean      |    0.006  |\n",
      "| time/                 |           |\n",
      "|    fps                |      192  |\n",
      "|    episodes           |     5400  |\n",
      "|    time_elapsed       |     1124  |\n",
      "|    total_timesteps    |   216000  |\n",
      "| train/                |           |\n",
      "|    loss               |    6.476  |\n",
      "|    policy_loss        |    6.489  |\n",
      "|    value_loss         |    0.103  |\n",
      "|    explained_variance |    0.194  |\n",
      "|    n_updates          |     5400  |\n",
      "| aux_train/            |           |\n",
      "|    aux_confidence_mse |    0.005  |\n",
      "| rnd_net_dist/         |           |\n",
      "|    mean_rnd_bonus     |    0.000  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        |   40.000  |\n",
      "|    ep_rew_mean        |    0.300  |\n",
      "|    ep_rew_std         |    0.458  |\n",
      "|    policy_entropy     |    0.636  |\n",
      "|    advantage_mean     |   -0.106  |\n",
      "|    advantage_std      |    0.122  |\n",
      "|    aux_loss_mean      |    0.005  |\n",
      "| time/                 |           |\n",
      "|    fps                |      192  |\n",
      "|    episodes           |     5500  |\n",
      "|    time_elapsed       |     1140  |\n",
      "|    total_timesteps    |   220000  |\n",
      "| train/                |           |\n",
      "|    loss               |   -2.644  |\n",
      "|    policy_loss        |   -2.594  |\n",
      "|    value_loss         |    0.026  |\n",
      "|    explained_variance |   -6.286  |\n",
      "|    n_updates          |     5500  |\n",
      "| aux_train/            |           |\n",
      "|    aux_confidence_mse |    0.004  |\n",
      "| rnd_net_dist/         |           |\n",
      "|    mean_rnd_bonus     |    0.000  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        |   40.000  |\n",
      "|    ep_rew_mean        |    0.270  |\n",
      "|    ep_rew_std         |    0.444  |\n",
      "|    policy_entropy     |    0.659  |\n",
      "|    advantage_mean     |   -0.124  |\n",
      "|    advantage_std      |    0.104  |\n",
      "|    aux_loss_mean      |    0.005  |\n",
      "| time/                 |           |\n",
      "|    fps                |      193  |\n",
      "|    episodes           |     5600  |\n",
      "|    time_elapsed       |     1155  |\n",
      "|    total_timesteps    |   224000  |\n",
      "| train/                |           |\n",
      "|    loss               |   -3.289  |\n",
      "|    policy_loss        |   -3.237  |\n",
      "|    value_loss         |    0.026  |\n",
      "|    explained_variance |   -2.360  |\n",
      "|    n_updates          |     5600  |\n",
      "| aux_train/            |           |\n",
      "|    aux_confidence_mse |    0.006  |\n",
      "| rnd_net_dist/         |           |\n",
      "|    mean_rnd_bonus     |    0.000  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        |   40.000  |\n",
      "|    ep_rew_mean        |    0.320  |\n",
      "|    ep_rew_std         |    0.466  |\n",
      "|    policy_entropy     |    0.636  |\n",
      "|    advantage_mean     |   -0.101  |\n",
      "|    advantage_std      |    0.084  |\n",
      "|    aux_loss_mean      |    0.005  |\n",
      "| time/                 |           |\n",
      "|    fps                |      194  |\n",
      "|    episodes           |     5700  |\n",
      "|    time_elapsed       |     1171  |\n",
      "|    total_timesteps    |   228000  |\n",
      "| train/                |           |\n",
      "|    loss               |   -2.841  |\n",
      "|    policy_loss        |   -2.786  |\n",
      "|    value_loss         |    0.017  |\n",
      "|    explained_variance |   -2.809  |\n",
      "|    n_updates          |     5700  |\n",
      "| aux_train/            |           |\n",
      "|    aux_confidence_mse |    0.003  |\n",
      "| rnd_net_dist/         |           |\n",
      "|    mean_rnd_bonus     |    0.000  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        |   40.000  |\n",
      "|    ep_rew_mean        |    0.340  |\n",
      "|    ep_rew_std         |    0.474  |\n",
      "|    policy_entropy     |    0.627  |\n",
      "|    advantage_mean     |   -0.138  |\n",
      "|    advantage_std      |    0.112  |\n",
      "|    aux_loss_mean      |    0.005  |\n",
      "| time/                 |           |\n",
      "|    fps                |      195  |\n",
      "|    episodes           |     5800  |\n",
      "|    time_elapsed       |     1186  |\n",
      "|    total_timesteps    |   232000  |\n",
      "| train/                |           |\n",
      "|    loss               |   -3.835  |\n",
      "|    policy_loss        |   -3.789  |\n",
      "|    value_loss         |    0.031  |\n",
      "|    explained_variance |   -2.478  |\n",
      "|    n_updates          |     5800  |\n",
      "| aux_train/            |           |\n",
      "|    aux_confidence_mse |    0.005  |\n",
      "| rnd_net_dist/         |           |\n",
      "|    mean_rnd_bonus     |    0.000  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        |   40.000  |\n",
      "|    ep_rew_mean        |    0.300  |\n",
      "|    ep_rew_std         |    0.458  |\n",
      "|    policy_entropy     |    0.653  |\n",
      "|    advantage_mean     |    0.269  |\n",
      "|    advantage_std      |    0.181  |\n",
      "|    aux_loss_mean      |    0.005  |\n",
      "| time/                 |           |\n",
      "|    fps                |      196  |\n",
      "|    episodes           |     5900  |\n",
      "|    time_elapsed       |     1202  |\n",
      "|    total_timesteps    |   236000  |\n",
      "| train/                |           |\n",
      "|    loss               |    6.475  |\n",
      "|    policy_loss        |    6.488  |\n",
      "|    value_loss         |    0.104  |\n",
      "|    explained_variance |    0.272  |\n",
      "|    n_updates          |     5900  |\n",
      "| aux_train/            |           |\n",
      "|    aux_confidence_mse |    0.005  |\n",
      "| rnd_net_dist/         |           |\n",
      "|    mean_rnd_bonus     |    0.000  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        |   40.000  |\n",
      "|    ep_rew_mean        |    0.280  |\n",
      "|    ep_rew_std         |    0.449  |\n",
      "|    policy_entropy     |    0.643  |\n",
      "|    advantage_mean     |    0.276  |\n",
      "|    advantage_std      |    0.178  |\n",
      "|    aux_loss_mean      |    0.005  |\n",
      "| time/                 |           |\n",
      "|    fps                |      197  |\n",
      "|    episodes           |     6000  |\n",
      "|    time_elapsed       |     1217  |\n",
      "|    total_timesteps    |   240000  |\n",
      "| train/                |           |\n",
      "|    loss               |    6.583  |\n",
      "|    policy_loss        |    6.593  |\n",
      "|    value_loss         |    0.107  |\n",
      "|    explained_variance |    0.337  |\n",
      "|    n_updates          |     6000  |\n",
      "| aux_train/            |           |\n",
      "|    aux_confidence_mse |    0.007  |\n",
      "| rnd_net_dist/         |           |\n",
      "|    mean_rnd_bonus     |    0.000  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        |   40.000  |\n",
      "|    ep_rew_mean        |    0.340  |\n",
      "|    ep_rew_std         |    0.474  |\n",
      "|    policy_entropy     |    0.630  |\n",
      "|    advantage_mean     |   -0.127  |\n",
      "|    advantage_std      |    0.092  |\n",
      "|    aux_loss_mean      |    0.006  |\n",
      "| time/                 |           |\n",
      "|    fps                |      197  |\n",
      "|    episodes           |     6100  |\n",
      "|    time_elapsed       |     1233  |\n",
      "|    total_timesteps    |   244000  |\n",
      "| train/                |           |\n",
      "|    loss               |   -3.109  |\n",
      "|    policy_loss        |   -3.058  |\n",
      "|    value_loss         |    0.024  |\n",
      "|    explained_variance |   -1.532  |\n",
      "|    n_updates          |     6100  |\n",
      "| aux_train/            |           |\n",
      "|    aux_confidence_mse |    0.003  |\n",
      "| rnd_net_dist/         |           |\n",
      "|    mean_rnd_bonus     |    0.000  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        |   40.000  |\n",
      "|    ep_rew_mean        |    0.410  |\n",
      "|    ep_rew_std         |    0.492  |\n",
      "|    policy_entropy     |    0.599  |\n",
      "|    advantage_mean     |   -0.149  |\n",
      "|    advantage_std      |    0.110  |\n",
      "|    aux_loss_mean      |    0.006  |\n",
      "| time/                 |           |\n",
      "|    fps                |      198  |\n",
      "|    episodes           |     6200  |\n",
      "|    time_elapsed       |     1248  |\n",
      "|    total_timesteps    |   248000  |\n",
      "| train/                |           |\n",
      "|    loss               |   -3.211  |\n",
      "|    policy_loss        |   -3.169  |\n",
      "|    value_loss         |    0.034  |\n",
      "|    explained_variance |   -1.602  |\n",
      "|    n_updates          |     6200  |\n",
      "| aux_train/            |           |\n",
      "|    aux_confidence_mse |    0.004  |\n",
      "| rnd_net_dist/         |           |\n",
      "|    mean_rnd_bonus     |    0.000  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        |   40.000  |\n",
      "|    ep_rew_mean        |    0.300  |\n",
      "|    ep_rew_std         |    0.458  |\n",
      "|    policy_entropy     |    0.618  |\n",
      "|    advantage_mean     |   -0.140  |\n",
      "|    advantage_std      |    0.088  |\n",
      "|    aux_loss_mean      |    0.005  |\n",
      "| time/                 |           |\n",
      "|    fps                |      199  |\n",
      "|    episodes           |     6300  |\n",
      "|    time_elapsed       |     1264  |\n",
      "|    total_timesteps    |   252000  |\n",
      "| train/                |           |\n",
      "|    loss               |   -3.306  |\n",
      "|    policy_loss        |   -3.258  |\n",
      "|    value_loss         |    0.027  |\n",
      "|    explained_variance |   -0.799  |\n",
      "|    n_updates          |     6300  |\n",
      "| aux_train/            |           |\n",
      "|    aux_confidence_mse |    0.004  |\n",
      "| rnd_net_dist/         |           |\n",
      "|    mean_rnd_bonus     |    0.000  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        |   40.000  |\n",
      "|    ep_rew_mean        |    0.450  |\n",
      "|    ep_rew_std         |    0.497  |\n",
      "|    policy_entropy     |    0.651  |\n",
      "|    advantage_mean     |   -0.164  |\n",
      "|    advantage_std      |    0.120  |\n",
      "|    aux_loss_mean      |    0.005  |\n",
      "| time/                 |           |\n",
      "|    fps                |      200  |\n",
      "|    episodes           |     6400  |\n",
      "|    time_elapsed       |     1279  |\n",
      "|    total_timesteps    |   256000  |\n",
      "| train/                |           |\n",
      "|    loss               |   -4.403  |\n",
      "|    policy_loss        |   -4.359  |\n",
      "|    value_loss         |    0.041  |\n",
      "|    explained_variance |   -1.649  |\n",
      "|    n_updates          |     6400  |\n",
      "| aux_train/            |           |\n",
      "|    aux_confidence_mse |    0.006  |\n",
      "| rnd_net_dist/         |           |\n",
      "|    mean_rnd_bonus     |    0.000  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        |   40.000  |\n",
      "|    ep_rew_mean        |    0.340  |\n",
      "|    ep_rew_std         |    0.474  |\n",
      "|    policy_entropy     |    0.639  |\n",
      "|    advantage_mean     |    0.253  |\n",
      "|    advantage_std      |    0.175  |\n",
      "|    aux_loss_mean      |    0.005  |\n",
      "| time/                 |           |\n",
      "|    fps                |      200  |\n",
      "|    episodes           |     6500  |\n",
      "|    time_elapsed       |     1295  |\n",
      "|    total_timesteps    |   260000  |\n",
      "| train/                |           |\n",
      "|    loss               |    6.916  |\n",
      "|    policy_loss        |    6.933  |\n",
      "|    value_loss         |    0.094  |\n",
      "|    explained_variance |    0.274  |\n",
      "|    n_updates          |     6500  |\n",
      "| aux_train/            |           |\n",
      "|    aux_confidence_mse |    0.003  |\n",
      "| rnd_net_dist/         |           |\n",
      "|    mean_rnd_bonus     |    0.000  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        |   40.000  |\n",
      "|    ep_rew_mean        |    0.280  |\n",
      "|    ep_rew_std         |    0.449  |\n",
      "|    policy_entropy     |    0.625  |\n",
      "|    advantage_mean     |   -0.096  |\n",
      "|    advantage_std      |    0.083  |\n",
      "|    aux_loss_mean      |    0.005  |\n",
      "| time/                 |           |\n",
      "|    fps                |      201  |\n",
      "|    episodes           |     6600  |\n",
      "|    time_elapsed       |     1310  |\n",
      "|    total_timesteps    |   264000  |\n",
      "| train/                |           |\n",
      "|    loss               |   -2.522  |\n",
      "|    policy_loss        |   -2.468  |\n",
      "|    value_loss         |    0.016  |\n",
      "|    explained_variance |   -3.458  |\n",
      "|    n_updates          |     6600  |\n",
      "| aux_train/            |           |\n",
      "|    aux_confidence_mse |    0.004  |\n",
      "| rnd_net_dist/         |           |\n",
      "|    mean_rnd_bonus     |    0.000  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        |   40.000  |\n",
      "|    ep_rew_mean        |    0.340  |\n",
      "|    ep_rew_std         |    0.474  |\n",
      "|    policy_entropy     |    0.613  |\n",
      "|    advantage_mean     |   -0.139  |\n",
      "|    advantage_std      |    0.099  |\n",
      "|    aux_loss_mean      |    0.005  |\n",
      "| time/                 |           |\n",
      "|    fps                |      202  |\n",
      "|    episodes           |     6700  |\n",
      "|    time_elapsed       |     1326  |\n",
      "|    total_timesteps    |   268000  |\n",
      "| train/                |           |\n",
      "|    loss               |   -4.100  |\n",
      "|    policy_loss        |   -4.054  |\n",
      "|    value_loss         |    0.029  |\n",
      "|    explained_variance |   -1.813  |\n",
      "|    n_updates          |     6700  |\n",
      "| aux_train/            |           |\n",
      "|    aux_confidence_mse |    0.005  |\n",
      "| rnd_net_dist/         |           |\n",
      "|    mean_rnd_bonus     |    0.000  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        |   40.000  |\n",
      "|    ep_rew_mean        |    0.330  |\n",
      "|    ep_rew_std         |    0.470  |\n",
      "|    policy_entropy     |    0.589  |\n",
      "|    advantage_mean     |   -0.127  |\n",
      "|    advantage_std      |    0.100  |\n",
      "|    aux_loss_mean      |    0.005  |\n",
      "| time/                 |           |\n",
      "|    fps                |      202  |\n",
      "|    episodes           |     6800  |\n",
      "|    time_elapsed       |     1341  |\n",
      "|    total_timesteps    |   272000  |\n",
      "| train/                |           |\n",
      "|    loss               |   -3.821  |\n",
      "|    policy_loss        |   -3.775  |\n",
      "|    value_loss         |    0.026  |\n",
      "|    explained_variance |   -2.426  |\n",
      "|    n_updates          |     6800  |\n",
      "| aux_train/            |           |\n",
      "|    aux_confidence_mse |    0.003  |\n",
      "| rnd_net_dist/         |           |\n",
      "|    mean_rnd_bonus     |    0.000  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        |   40.000  |\n",
      "|    ep_rew_mean        |    0.340  |\n",
      "|    ep_rew_std         |    0.474  |\n",
      "|    policy_entropy     |    0.631  |\n",
      "|    advantage_mean     |    0.246  |\n",
      "|    advantage_std      |    0.190  |\n",
      "|    aux_loss_mean      |    0.005  |\n",
      "| time/                 |           |\n",
      "|    fps                |      203  |\n",
      "|    episodes           |     6900  |\n",
      "|    time_elapsed       |     1356  |\n",
      "|    total_timesteps    |   276000  |\n",
      "| train/                |           |\n",
      "|    loss               |    5.817  |\n",
      "|    policy_loss        |    5.832  |\n",
      "|    value_loss         |    0.096  |\n",
      "|    explained_variance |    0.122  |\n",
      "|    n_updates          |     6900  |\n",
      "| aux_train/            |           |\n",
      "|    aux_confidence_mse |    0.005  |\n",
      "| rnd_net_dist/         |           |\n",
      "|    mean_rnd_bonus     |    0.000  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        |   40.000  |\n",
      "|    ep_rew_mean        |    0.350  |\n",
      "|    ep_rew_std         |    0.477  |\n",
      "|    policy_entropy     |    0.586  |\n",
      "|    advantage_mean     |    0.250  |\n",
      "|    advantage_std      |    0.176  |\n",
      "|    aux_loss_mean      |    0.006  |\n",
      "| time/                 |           |\n",
      "|    fps                |      204  |\n",
      "|    episodes           |     7000  |\n",
      "|    time_elapsed       |     1372  |\n",
      "|    total_timesteps    |   280000  |\n",
      "| train/                |           |\n",
      "|    loss               |    5.490  |\n",
      "|    policy_loss        |    5.501  |\n",
      "|    value_loss         |    0.093  |\n",
      "|    explained_variance |    0.252  |\n",
      "|    n_updates          |     7000  |\n",
      "| aux_train/            |           |\n",
      "|    aux_confidence_mse |    0.005  |\n",
      "| rnd_net_dist/         |           |\n",
      "|    mean_rnd_bonus     |    0.000  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        |   40.000  |\n",
      "|    ep_rew_mean        |    0.370  |\n",
      "|    ep_rew_std         |    0.483  |\n",
      "|    policy_entropy     |    0.643  |\n",
      "|    advantage_mean     |   -0.141  |\n",
      "|    advantage_std      |    0.097  |\n",
      "|    aux_loss_mean      |    0.006  |\n",
      "| time/                 |           |\n",
      "|    fps                |      204  |\n",
      "|    episodes           |     7100  |\n",
      "|    time_elapsed       |     1387  |\n",
      "|    total_timesteps    |   284000  |\n",
      "| train/                |           |\n",
      "|    loss               |   -3.230  |\n",
      "|    policy_loss        |   -3.181  |\n",
      "|    value_loss         |    0.029  |\n",
      "|    explained_variance |   -1.549  |\n",
      "|    n_updates          |     7100  |\n",
      "| aux_train/            |           |\n",
      "|    aux_confidence_mse |    0.009  |\n",
      "| rnd_net_dist/         |           |\n",
      "|    mean_rnd_bonus     |    0.000  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        |   40.000  |\n",
      "|    ep_rew_mean        |    0.390  |\n",
      "|    ep_rew_std         |    0.488  |\n",
      "|    policy_entropy     |    0.654  |\n",
      "|    advantage_mean     |   -0.154  |\n",
      "|    advantage_std      |    0.114  |\n",
      "|    aux_loss_mean      |    0.008  |\n",
      "| time/                 |           |\n",
      "|    fps                |      205  |\n",
      "|    episodes           |     7200  |\n",
      "|    time_elapsed       |     1403  |\n",
      "|    total_timesteps    |   288000  |\n",
      "| train/                |           |\n",
      "|    loss               |   -3.817  |\n",
      "|    policy_loss        |   -3.770  |\n",
      "|    value_loss         |    0.036  |\n",
      "|    explained_variance |   -1.916  |\n",
      "|    n_updates          |     7200  |\n",
      "| aux_train/            |           |\n",
      "|    aux_confidence_mse |    0.010  |\n",
      "| rnd_net_dist/         |           |\n",
      "|    mean_rnd_bonus     |    0.000  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        |   40.000  |\n",
      "|    ep_rew_mean        |    0.360  |\n",
      "|    ep_rew_std         |    0.480  |\n",
      "|    policy_entropy     |    0.672  |\n",
      "|    advantage_mean     |   -0.141  |\n",
      "|    advantage_std      |    0.108  |\n",
      "|    aux_loss_mean      |    0.008  |\n",
      "| time/                 |           |\n",
      "|    fps                |      205  |\n",
      "|    episodes           |     7300  |\n",
      "|    time_elapsed       |     1418  |\n",
      "|    total_timesteps    |   292000  |\n",
      "| train/                |           |\n",
      "|    loss               |   -3.561  |\n",
      "|    policy_loss        |   -3.511  |\n",
      "|    value_loss         |    0.031  |\n",
      "|    explained_variance |   -2.311  |\n",
      "|    n_updates          |     7300  |\n",
      "| aux_train/            |           |\n",
      "|    aux_confidence_mse |    0.009  |\n",
      "| rnd_net_dist/         |           |\n",
      "|    mean_rnd_bonus     |    0.000  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        |   40.000  |\n",
      "|    ep_rew_mean        |    0.350  |\n",
      "|    ep_rew_std         |    0.477  |\n",
      "|    policy_entropy     |    0.651  |\n",
      "|    advantage_mean     |   -0.151  |\n",
      "|    advantage_std      |    0.121  |\n",
      "|    aux_loss_mean      |    0.007  |\n",
      "| time/                 |           |\n",
      "|    fps                |      206  |\n",
      "|    episodes           |     7400  |\n",
      "|    time_elapsed       |     1434  |\n",
      "|    total_timesteps    |   296000  |\n",
      "| train/                |           |\n",
      "|    loss               |   -3.408  |\n",
      "|    policy_loss        |   -3.362  |\n",
      "|    value_loss         |    0.037  |\n",
      "|    explained_variance |   -2.135  |\n",
      "|    n_updates          |     7400  |\n",
      "| aux_train/            |           |\n",
      "|    aux_confidence_mse |    0.007  |\n",
      "| rnd_net_dist/         |           |\n",
      "|    mean_rnd_bonus     |    0.000  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        |   40.000  |\n",
      "|    ep_rew_mean        |    0.280  |\n",
      "|    ep_rew_std         |    0.449  |\n",
      "|    policy_entropy     |    0.659  |\n",
      "|    advantage_mean     |   -0.120  |\n",
      "|    advantage_std      |    0.089  |\n",
      "|    aux_loss_mean      |    0.007  |\n",
      "| time/                 |           |\n",
      "|    fps                |      207  |\n",
      "|    episodes           |     7500  |\n",
      "|    time_elapsed       |     1449  |\n",
      "|    total_timesteps    |   300000  |\n",
      "| train/                |           |\n",
      "|    loss               |   -2.931  |\n",
      "|    policy_loss        |   -2.877  |\n",
      "|    value_loss         |    0.022  |\n",
      "|    explained_variance |   -1.806  |\n",
      "|    n_updates          |     7500  |\n",
      "| aux_train/            |           |\n",
      "|    aux_confidence_mse |    0.006  |\n",
      "| rnd_net_dist/         |           |\n",
      "|    mean_rnd_bonus     |    0.000  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        |   40.000  |\n",
      "|    ep_rew_mean        |    0.400  |\n",
      "|    ep_rew_std         |    0.490  |\n",
      "|    policy_entropy     |    0.647  |\n",
      "|    advantage_mean     |    0.211  |\n",
      "|    advantage_std      |    0.176  |\n",
      "|    aux_loss_mean      |    0.007  |\n",
      "| time/                 |           |\n",
      "|    fps                |      207  |\n",
      "|    episodes           |     7600  |\n",
      "|    time_elapsed       |     1465  |\n",
      "|    total_timesteps    |   304000  |\n",
      "| train/                |           |\n",
      "|    loss               |    5.227  |\n",
      "|    policy_loss        |    5.254  |\n",
      "|    value_loss         |    0.075  |\n",
      "|    explained_variance |    0.103  |\n",
      "|    n_updates          |     7600  |\n",
      "| aux_train/            |           |\n",
      "|    aux_confidence_mse |    0.006  |\n",
      "| rnd_net_dist/         |           |\n",
      "|    mean_rnd_bonus     |    0.000  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        |   40.000  |\n",
      "|    ep_rew_mean        |    0.330  |\n",
      "|    ep_rew_std         |    0.470  |\n",
      "|    policy_entropy     |    0.651  |\n",
      "|    advantage_mean     |   -0.122  |\n",
      "|    advantage_std      |    0.089  |\n",
      "|    aux_loss_mean      |    0.007  |\n",
      "| time/                 |           |\n",
      "|    fps                |      207  |\n",
      "|    episodes           |     7700  |\n",
      "|    time_elapsed       |     1481  |\n",
      "|    total_timesteps    |   308000  |\n",
      "| train/                |           |\n",
      "|    loss               |   -2.900  |\n",
      "|    policy_loss        |   -2.847  |\n",
      "|    value_loss         |    0.023  |\n",
      "|    explained_variance |   -1.776  |\n",
      "|    n_updates          |     7700  |\n",
      "| aux_train/            |           |\n",
      "|    aux_confidence_mse |    0.008  |\n",
      "| rnd_net_dist/         |           |\n",
      "|    mean_rnd_bonus     |    0.000  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        |   40.000  |\n",
      "|    ep_rew_mean        |    0.280  |\n",
      "|    ep_rew_std         |    0.449  |\n",
      "|    policy_entropy     |    0.574  |\n",
      "|    advantage_mean     |    0.273  |\n",
      "|    advantage_std      |    0.185  |\n",
      "|    aux_loss_mean      |    0.007  |\n",
      "| time/                 |           |\n",
      "|    fps                |      208  |\n",
      "|    episodes           |     7800  |\n",
      "|    time_elapsed       |     1496  |\n",
      "|    total_timesteps    |   312000  |\n",
      "| train/                |           |\n",
      "|    loss               |    5.751  |\n",
      "|    policy_loss        |    5.754  |\n",
      "|    value_loss         |    0.108  |\n",
      "|    explained_variance |    0.265  |\n",
      "|    n_updates          |     7800  |\n",
      "| aux_train/            |           |\n",
      "|    aux_confidence_mse |    0.009  |\n",
      "| rnd_net_dist/         |           |\n",
      "|    mean_rnd_bonus     |    0.000  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        |   40.000  |\n",
      "|    ep_rew_mean        |    0.440  |\n",
      "|    ep_rew_std         |    0.496  |\n",
      "|    policy_entropy     |    0.573  |\n",
      "|    advantage_mean     |   -0.181  |\n",
      "|    advantage_std      |    0.117  |\n",
      "|    aux_loss_mean      |    0.009  |\n",
      "| time/                 |           |\n",
      "|    fps                |      209  |\n",
      "|    episodes           |     7900  |\n",
      "|    time_elapsed       |     1511  |\n",
      "|    total_timesteps    |   316000  |\n",
      "| train/                |           |\n",
      "|    loss               |   -4.170  |\n",
      "|    policy_loss        |   -4.136  |\n",
      "|    value_loss         |    0.046  |\n",
      "|    explained_variance |   -1.219  |\n",
      "|    n_updates          |     7900  |\n",
      "| aux_train/            |           |\n",
      "|    aux_confidence_mse |    0.008  |\n",
      "| rnd_net_dist/         |           |\n",
      "|    mean_rnd_bonus     |    0.000  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        |   40.000  |\n",
      "|    ep_rew_mean        |    0.380  |\n",
      "|    ep_rew_std         |    0.485  |\n",
      "|    policy_entropy     |    0.495  |\n",
      "|    advantage_mean     |   -0.134  |\n",
      "|    advantage_std      |    0.104  |\n",
      "|    aux_loss_mean      |    0.010  |\n",
      "| time/                 |           |\n",
      "|    fps                |      209  |\n",
      "|    episodes           |     8000  |\n",
      "|    time_elapsed       |     1527  |\n",
      "|    total_timesteps    |   320000  |\n",
      "| train/                |           |\n",
      "|    loss               |   -2.493  |\n",
      "|    policy_loss        |   -2.460  |\n",
      "|    value_loss         |    0.028  |\n",
      "|    explained_variance |   -1.799  |\n",
      "|    n_updates          |     8000  |\n",
      "| aux_train/            |           |\n",
      "|    aux_confidence_mse |    0.029  |\n",
      "| rnd_net_dist/         |           |\n",
      "|    mean_rnd_bonus     |    0.000  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        |   40.000  |\n",
      "|    ep_rew_mean        |    0.300  |\n",
      "|    ep_rew_std         |    0.458  |\n",
      "|    policy_entropy     |    0.578  |\n",
      "|    advantage_mean     |   -0.119  |\n",
      "|    advantage_std      |    0.089  |\n",
      "|    aux_loss_mean      |    0.010  |\n",
      "| time/                 |           |\n",
      "|    fps                |      210  |\n",
      "|    episodes           |     8100  |\n",
      "|    time_elapsed       |     1542  |\n",
      "|    total_timesteps    |   324000  |\n",
      "| train/                |           |\n",
      "|    loss               |   -2.916  |\n",
      "|    policy_loss        |   -2.870  |\n",
      "|    value_loss         |    0.022  |\n",
      "|    explained_variance |   -1.689  |\n",
      "|    n_updates          |     8100  |\n",
      "| aux_train/            |           |\n",
      "|    aux_confidence_mse |    0.013  |\n",
      "| rnd_net_dist/         |           |\n",
      "|    mean_rnd_bonus     |    0.000  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        |   40.000  |\n",
      "|    ep_rew_mean        |    0.400  |\n",
      "|    ep_rew_std         |    0.490  |\n",
      "|    policy_entropy     |    0.651  |\n",
      "|    advantage_mean     |    0.237  |\n",
      "|    advantage_std      |    0.175  |\n",
      "|    aux_loss_mean      |    0.008  |\n",
      "| time/                 |           |\n",
      "|    fps                |      210  |\n",
      "|    episodes           |     8200  |\n",
      "|    time_elapsed       |     1558  |\n",
      "|    total_timesteps    |   328000  |\n",
      "| train/                |           |\n",
      "|    loss               |    6.881  |\n",
      "|    policy_loss        |    6.903  |\n",
      "|    value_loss         |    0.086  |\n",
      "|    explained_variance |    0.214  |\n",
      "|    n_updates          |     8200  |\n",
      "| aux_train/            |           |\n",
      "|    aux_confidence_mse |    0.006  |\n",
      "| rnd_net_dist/         |           |\n",
      "|    mean_rnd_bonus     |    0.000  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        |   40.000  |\n",
      "|    ep_rew_mean        |    0.350  |\n",
      "|    ep_rew_std         |    0.477  |\n",
      "|    policy_entropy     |    0.610  |\n",
      "|    advantage_mean     |   -0.146  |\n",
      "|    advantage_std      |    0.093  |\n",
      "|    aux_loss_mean      |    0.007  |\n",
      "| time/                 |           |\n",
      "|    fps                |      211  |\n",
      "|    episodes           |     8300  |\n",
      "|    time_elapsed       |     1573  |\n",
      "|    total_timesteps    |   332000  |\n",
      "| train/                |           |\n",
      "|    loss               |   -3.591  |\n",
      "|    policy_loss        |   -3.546  |\n",
      "|    value_loss         |    0.030  |\n",
      "|    explained_variance |   -1.170  |\n",
      "|    n_updates          |     8300  |\n",
      "| aux_train/            |           |\n",
      "|    aux_confidence_mse |    0.009  |\n",
      "| rnd_net_dist/         |           |\n",
      "|    mean_rnd_bonus     |    0.000  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        |   40.000  |\n",
      "|    ep_rew_mean        |    0.380  |\n",
      "|    ep_rew_std         |    0.485  |\n",
      "|    policy_entropy     |    0.661  |\n",
      "|    advantage_mean     |   -0.151  |\n",
      "|    advantage_std      |    0.100  |\n",
      "|    aux_loss_mean      |    0.006  |\n",
      "| time/                 |           |\n",
      "|    fps                |      211  |\n",
      "|    episodes           |     8400  |\n",
      "|    time_elapsed       |     1589  |\n",
      "|    total_timesteps    |   336000  |\n",
      "| train/                |           |\n",
      "|    loss               |   -4.133  |\n",
      "|    policy_loss        |   -4.083  |\n",
      "|    value_loss         |    0.032  |\n",
      "|    explained_variance |   -1.349  |\n",
      "|    n_updates          |     8400  |\n",
      "| aux_train/            |           |\n",
      "|    aux_confidence_mse |    0.005  |\n",
      "| rnd_net_dist/         |           |\n",
      "|    mean_rnd_bonus     |    0.000  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        |   40.000  |\n",
      "|    ep_rew_mean        |    0.300  |\n",
      "|    ep_rew_std         |    0.458  |\n",
      "|    policy_entropy     |    0.622  |\n",
      "|    advantage_mean     |   -0.094  |\n",
      "|    advantage_std      |    0.091  |\n",
      "|    aux_loss_mean      |    0.007  |\n",
      "| time/                 |           |\n",
      "|    fps                |      211  |\n",
      "|    episodes           |     8500  |\n",
      "|    time_elapsed       |     1604  |\n",
      "|    total_timesteps    |   340000  |\n",
      "| train/                |           |\n",
      "|    loss               |   -2.580  |\n",
      "|    policy_loss        |   -2.527  |\n",
      "|    value_loss         |    0.017  |\n",
      "|    explained_variance |   -4.087  |\n",
      "|    n_updates          |     8500  |\n",
      "| aux_train/            |           |\n",
      "|    aux_confidence_mse |    0.006  |\n",
      "| rnd_net_dist/         |           |\n",
      "|    mean_rnd_bonus     |    0.000  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        |   40.000  |\n",
      "|    ep_rew_mean        |    0.310  |\n",
      "|    ep_rew_std         |    0.462  |\n",
      "|    policy_entropy     |    0.588  |\n",
      "|    advantage_mean     |   -0.133  |\n",
      "|    advantage_std      |    0.084  |\n",
      "|    aux_loss_mean      |    0.011  |\n",
      "| time/                 |           |\n",
      "|    fps                |      212  |\n",
      "|    episodes           |     8600  |\n",
      "|    time_elapsed       |     1620  |\n",
      "|    total_timesteps    |   344000  |\n",
      "| train/                |           |\n",
      "|    loss               |   -3.188  |\n",
      "|    policy_loss        |   -3.143  |\n",
      "|    value_loss         |    0.025  |\n",
      "|    explained_variance |   -0.894  |\n",
      "|    n_updates          |     8600  |\n",
      "| aux_train/            |           |\n",
      "|    aux_confidence_mse |    0.009  |\n",
      "| rnd_net_dist/         |           |\n",
      "|    mean_rnd_bonus     |    0.000  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        |   40.000  |\n",
      "|    ep_rew_mean        |    0.380  |\n",
      "|    ep_rew_std         |    0.485  |\n",
      "|    policy_entropy     |    0.488  |\n",
      "|    advantage_mean     |    0.232  |\n",
      "|    advantage_std      |    0.187  |\n",
      "|    aux_loss_mean      |    0.012  |\n",
      "| time/                 |           |\n",
      "|    fps                |      212  |\n",
      "|    episodes           |     8700  |\n",
      "|    time_elapsed       |     1635  |\n",
      "|    total_timesteps    |   348000  |\n",
      "| train/                |           |\n",
      "|    loss               |    4.302  |\n",
      "|    policy_loss        |    4.304  |\n",
      "|    value_loss         |    0.088  |\n",
      "|    explained_variance |    0.033  |\n",
      "|    n_updates          |     8700  |\n",
      "| aux_train/            |           |\n",
      "|    aux_confidence_mse |    0.027  |\n",
      "| rnd_net_dist/         |           |\n",
      "|    mean_rnd_bonus     |    0.000  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        |   40.000  |\n",
      "|    ep_rew_mean        |    0.350  |\n",
      "|    ep_rew_std         |    0.477  |\n",
      "|    policy_entropy     |    0.610  |\n",
      "|    advantage_mean     |    0.254  |\n",
      "|    advantage_std      |    0.187  |\n",
      "|    aux_loss_mean      |    0.012  |\n",
      "| time/                 |           |\n",
      "|    fps                |      213  |\n",
      "|    episodes           |     8800  |\n",
      "|    time_elapsed       |     1651  |\n",
      "|    total_timesteps    |   352000  |\n",
      "| train/                |           |\n",
      "|    loss               |    6.510  |\n",
      "|    policy_loss        |    6.521  |\n",
      "|    value_loss         |    0.098  |\n",
      "|    explained_variance |    0.187  |\n",
      "|    n_updates          |     8800  |\n",
      "| aux_train/            |           |\n",
      "|    aux_confidence_mse |    0.006  |\n",
      "| rnd_net_dist/         |           |\n",
      "|    mean_rnd_bonus     |    0.000  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        |   40.000  |\n",
      "|    ep_rew_mean        |    0.280  |\n",
      "|    ep_rew_std         |    0.449  |\n",
      "|    policy_entropy     |    0.649  |\n",
      "|    advantage_mean     |   -0.078  |\n",
      "|    advantage_std      |    0.089  |\n",
      "|    aux_loss_mean      |    0.007  |\n",
      "| time/                 |           |\n",
      "|    fps                |      213  |\n",
      "|    episodes           |     8900  |\n",
      "|    time_elapsed       |     1666  |\n",
      "|    total_timesteps    |   356000  |\n",
      "| train/                |           |\n",
      "|    loss               |   -2.041  |\n",
      "|    policy_loss        |   -1.984  |\n",
      "|    value_loss         |    0.014  |\n",
      "|    explained_variance |   -4.928  |\n",
      "|    n_updates          |     8900  |\n",
      "| aux_train/            |           |\n",
      "|    aux_confidence_mse |    0.007  |\n",
      "| rnd_net_dist/         |           |\n",
      "|    mean_rnd_bonus     |    0.000  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        |   40.000  |\n",
      "|    ep_rew_mean        |    0.310  |\n",
      "|    ep_rew_std         |    0.462  |\n",
      "|    policy_entropy     |    0.654  |\n",
      "|    advantage_mean     |    0.264  |\n",
      "|    advantage_std      |    0.187  |\n",
      "|    aux_loss_mean      |    0.007  |\n",
      "| time/                 |           |\n",
      "|    fps                |      214  |\n",
      "|    episodes           |     9000  |\n",
      "|    time_elapsed       |     1682  |\n",
      "|    total_timesteps    |   360000  |\n",
      "| train/                |           |\n",
      "|    loss               |    6.772  |\n",
      "|    policy_loss        |    6.785  |\n",
      "|    value_loss         |    0.104  |\n",
      "|    explained_variance |    0.192  |\n",
      "|    n_updates          |     9000  |\n",
      "| aux_train/            |           |\n",
      "|    aux_confidence_mse |    0.004  |\n",
      "| rnd_net_dist/         |           |\n",
      "|    mean_rnd_bonus     |    0.000  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        |   40.000  |\n",
      "|    ep_rew_mean        |    0.250  |\n",
      "|    ep_rew_std         |    0.433  |\n",
      "|    policy_entropy     |    0.633  |\n",
      "|    advantage_mean     |   -0.104  |\n",
      "|    advantage_std      |    0.084  |\n",
      "|    aux_loss_mean      |    0.008  |\n",
      "| time/                 |           |\n",
      "|    fps                |      214  |\n",
      "|    episodes           |     9100  |\n",
      "|    time_elapsed       |     1697  |\n",
      "|    total_timesteps    |   364000  |\n",
      "| train/                |           |\n",
      "|    loss               |   -2.900  |\n",
      "|    policy_loss        |   -2.846  |\n",
      "|    value_loss         |    0.018  |\n",
      "|    explained_variance |   -2.324  |\n",
      "|    n_updates          |     9100  |\n",
      "| aux_train/            |           |\n",
      "|    aux_confidence_mse |    0.006  |\n",
      "| rnd_net_dist/         |           |\n",
      "|    mean_rnd_bonus     |    0.000  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        |   40.000  |\n",
      "|    ep_rew_mean        |    0.350  |\n",
      "|    ep_rew_std         |    0.477  |\n",
      "|    policy_entropy     |    0.660  |\n",
      "|    advantage_mean     |   -0.131  |\n",
      "|    advantage_std      |    0.093  |\n",
      "|    aux_loss_mean      |    0.007  |\n",
      "| time/                 |           |\n",
      "|    fps                |      214  |\n",
      "|    episodes           |     9200  |\n",
      "|    time_elapsed       |     1713  |\n",
      "|    total_timesteps    |   368000  |\n",
      "| train/                |           |\n",
      "|    loss               |   -3.560  |\n",
      "|    policy_loss        |   -3.507  |\n",
      "|    value_loss         |    0.026  |\n",
      "|    explained_variance |   -1.491  |\n",
      "|    n_updates          |     9200  |\n",
      "| aux_train/            |           |\n",
      "|    aux_confidence_mse |    0.005  |\n",
      "| rnd_net_dist/         |           |\n",
      "|    mean_rnd_bonus     |    0.000  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        |   40.000  |\n",
      "|    ep_rew_mean        |    0.290  |\n",
      "|    ep_rew_std         |    0.454  |\n",
      "|    policy_entropy     |    0.628  |\n",
      "|    advantage_mean     |    0.278  |\n",
      "|    advantage_std      |    0.189  |\n",
      "|    aux_loss_mean      |    0.006  |\n",
      "| time/                 |           |\n",
      "|    fps                |      215  |\n",
      "|    episodes           |     9300  |\n",
      "|    time_elapsed       |     1728  |\n",
      "|    total_timesteps    |   372000  |\n",
      "| train/                |           |\n",
      "|    loss               |    6.901  |\n",
      "|    policy_loss        |    6.907  |\n",
      "|    value_loss         |    0.112  |\n",
      "|    explained_variance |    0.257  |\n",
      "|    n_updates          |     9300  |\n",
      "| aux_train/            |           |\n",
      "|    aux_confidence_mse |    0.007  |\n",
      "| rnd_net_dist/         |           |\n",
      "|    mean_rnd_bonus     |    0.000  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        |   40.000  |\n",
      "|    ep_rew_mean        |    0.290  |\n",
      "|    ep_rew_std         |    0.454  |\n",
      "|    policy_entropy     |    0.656  |\n",
      "|    advantage_mean     |    0.281  |\n",
      "|    advantage_std      |    0.196  |\n",
      "|    aux_loss_mean      |    0.006  |\n",
      "| time/                 |           |\n",
      "|    fps                |      215  |\n",
      "|    episodes           |     9400  |\n",
      "|    time_elapsed       |     1744  |\n",
      "|    total_timesteps    |   376000  |\n",
      "| train/                |           |\n",
      "|    loss               |    8.299  |\n",
      "|    policy_loss        |    8.305  |\n",
      "|    value_loss         |    0.116  |\n",
      "|    explained_variance |    0.214  |\n",
      "|    n_updates          |     9400  |\n",
      "| aux_train/            |           |\n",
      "|    aux_confidence_mse |    0.005  |\n",
      "| rnd_net_dist/         |           |\n",
      "|    mean_rnd_bonus     |    0.000  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        |   40.000  |\n",
      "|    ep_rew_mean        |    0.300  |\n",
      "|    ep_rew_std         |    0.458  |\n",
      "|    policy_entropy     |    0.632  |\n",
      "|    advantage_mean     |    0.285  |\n",
      "|    advantage_std      |    0.195  |\n",
      "|    aux_loss_mean      |    0.005  |\n",
      "| time/                 |           |\n",
      "|    fps                |      215  |\n",
      "|    episodes           |     9500  |\n",
      "|    time_elapsed       |     1760  |\n",
      "|    total_timesteps    |   380000  |\n",
      "| train/                |           |\n",
      "|    loss               |    6.735  |\n",
      "|    policy_loss        |    6.738  |\n",
      "|    value_loss         |    0.118  |\n",
      "|    explained_variance |    0.215  |\n",
      "|    n_updates          |     9500  |\n",
      "| aux_train/            |           |\n",
      "|    aux_confidence_mse |    0.005  |\n",
      "| rnd_net_dist/         |           |\n",
      "|    mean_rnd_bonus     |    0.000  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        |   40.000  |\n",
      "|    ep_rew_mean        |    0.300  |\n",
      "|    ep_rew_std         |    0.458  |\n",
      "|    policy_entropy     |    0.612  |\n",
      "|    advantage_mean     |    0.286  |\n",
      "|    advantage_std      |    0.214  |\n",
      "|    aux_loss_mean      |    0.006  |\n",
      "| time/                 |           |\n",
      "|    fps                |      215  |\n",
      "|    episodes           |     9600  |\n",
      "|    time_elapsed       |     1780  |\n",
      "|    total_timesteps    |   384000  |\n",
      "| train/                |           |\n",
      "|    loss               |    7.549  |\n",
      "|    policy_loss        |    7.546  |\n",
      "|    value_loss         |    0.127  |\n",
      "|    explained_variance |    0.100  |\n",
      "|    n_updates          |     9600  |\n",
      "| aux_train/            |           |\n",
      "|    aux_confidence_mse |    0.012  |\n",
      "| rnd_net_dist/         |           |\n",
      "|    mean_rnd_bonus     |    0.000  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        |   40.000  |\n",
      "|    ep_rew_mean        |    0.280  |\n",
      "|    ep_rew_std         |    0.449  |\n",
      "|    policy_entropy     |    0.637  |\n",
      "|    advantage_mean     |   -0.131  |\n",
      "|    advantage_std      |    0.079  |\n",
      "|    aux_loss_mean      |    0.006  |\n",
      "| time/                 |           |\n",
      "|    fps                |      215  |\n",
      "|    episodes           |     9700  |\n",
      "|    time_elapsed       |     1802  |\n",
      "|    total_timesteps    |   388000  |\n",
      "| train/                |           |\n",
      "|    loss               |   -3.670  |\n",
      "|    policy_loss        |   -3.618  |\n",
      "|    value_loss         |    0.023  |\n",
      "|    explained_variance |   -0.992  |\n",
      "|    n_updates          |     9700  |\n",
      "| aux_train/            |           |\n",
      "|    aux_confidence_mse |    0.005  |\n",
      "| rnd_net_dist/         |           |\n",
      "|    mean_rnd_bonus     |    0.000  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        |   40.000  |\n",
      "|    ep_rew_mean        |    0.390  |\n",
      "|    ep_rew_std         |    0.488  |\n",
      "|    policy_entropy     |    0.668  |\n",
      "|    advantage_mean     |    0.231  |\n",
      "|    advantage_std      |    0.177  |\n",
      "|    aux_loss_mean      |    0.005  |\n",
      "| time/                 |           |\n",
      "|    fps                |      214  |\n",
      "|    episodes           |     9800  |\n",
      "|    time_elapsed       |     1825  |\n",
      "|    total_timesteps    |   392000  |\n",
      "| train/                |           |\n",
      "|    loss               |    5.503  |\n",
      "|    policy_loss        |    5.528  |\n",
      "|    value_loss         |    0.084  |\n",
      "|    explained_variance |    0.178  |\n",
      "|    n_updates          |     9800  |\n",
      "| aux_train/            |           |\n",
      "|    aux_confidence_mse |    0.004  |\n",
      "| rnd_net_dist/         |           |\n",
      "|    mean_rnd_bonus     |    0.000  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        |   40.000  |\n",
      "|    ep_rew_mean        |    0.330  |\n",
      "|    ep_rew_std         |    0.470  |\n",
      "|    policy_entropy     |    0.644  |\n",
      "|    advantage_mean     |    0.259  |\n",
      "|    advantage_std      |    0.184  |\n",
      "|    aux_loss_mean      |    0.005  |\n",
      "| time/                 |           |\n",
      "|    fps                |      214  |\n",
      "|    episodes           |     9900  |\n",
      "|    time_elapsed       |     1847  |\n",
      "|    total_timesteps    |   396000  |\n",
      "| train/                |           |\n",
      "|    loss               |    7.597  |\n",
      "|    policy_loss        |    7.611  |\n",
      "|    value_loss         |    0.100  |\n",
      "|    explained_variance |    0.204  |\n",
      "|    n_updates          |     9900  |\n",
      "| aux_train/            |           |\n",
      "|    aux_confidence_mse |    0.007  |\n",
      "| rnd_net_dist/         |           |\n",
      "|    mean_rnd_bonus     |    0.000  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        |   40.000  |\n",
      "|    ep_rew_mean        |    0.380  |\n",
      "|    ep_rew_std         |    0.485  |\n",
      "|    policy_entropy     |    0.614  |\n",
      "|    advantage_mean     |    0.230  |\n",
      "|    advantage_std      |    0.182  |\n",
      "|    aux_loss_mean      |    0.005  |\n",
      "| time/                 |           |\n",
      "|    fps                |      214  |\n",
      "|    episodes           |    10000  |\n",
      "|    time_elapsed       |     1869  |\n",
      "|    total_timesteps    |   400000  |\n",
      "| train/                |           |\n",
      "|    loss               |    6.711  |\n",
      "|    policy_loss        |    6.729  |\n",
      "|    value_loss         |    0.085  |\n",
      "|    explained_variance |    0.140  |\n",
      "|    n_updates          |    10000  |\n",
      "| aux_train/            |           |\n",
      "|    aux_confidence_mse |    0.008  |\n",
      "| rnd_net_dist/         |           |\n",
      "|    mean_rnd_bonus     |    0.000  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        |   40.000  |\n",
      "|    ep_rew_mean        |    0.350  |\n",
      "|    ep_rew_std         |    0.477  |\n",
      "|    policy_entropy     |    0.659  |\n",
      "|    advantage_mean     |   -0.132  |\n",
      "|    advantage_std      |    0.100  |\n",
      "|    aux_loss_mean      |    0.004  |\n",
      "| time/                 |           |\n",
      "|    fps                |      213  |\n",
      "|    episodes           |    10100  |\n",
      "|    time_elapsed       |     1892  |\n",
      "|    total_timesteps    |   404000  |\n",
      "| train/                |           |\n",
      "|    loss               |   -3.039  |\n",
      "|    policy_loss        |   -2.987  |\n",
      "|    value_loss         |    0.027  |\n",
      "|    explained_variance |   -2.047  |\n",
      "|    n_updates          |    10100  |\n",
      "| aux_train/            |           |\n",
      "|    aux_confidence_mse |    0.005  |\n",
      "| rnd_net_dist/         |           |\n",
      "|    mean_rnd_bonus     |    0.000  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        |   40.000  |\n",
      "|    ep_rew_mean        |    0.370  |\n",
      "|    ep_rew_std         |    0.483  |\n",
      "|    policy_entropy     |    0.658  |\n",
      "|    advantage_mean     |   -0.127  |\n",
      "|    advantage_std      |    0.091  |\n",
      "|    aux_loss_mean      |    0.004  |\n",
      "| time/                 |           |\n",
      "|    fps                |      213  |\n",
      "|    episodes           |    10200  |\n",
      "|    time_elapsed       |     1915  |\n",
      "|    total_timesteps    |   408000  |\n",
      "| train/                |           |\n",
      "|    loss               |   -3.330  |\n",
      "|    policy_loss        |   -3.277  |\n",
      "|    value_loss         |    0.024  |\n",
      "|    explained_variance |   -1.480  |\n",
      "|    n_updates          |    10200  |\n",
      "| aux_train/            |           |\n",
      "|    aux_confidence_mse |    0.004  |\n",
      "| rnd_net_dist/         |           |\n",
      "|    mean_rnd_bonus     |    0.000  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        |   40.000  |\n",
      "|    ep_rew_mean        |    0.390  |\n",
      "|    ep_rew_std         |    0.488  |\n",
      "|    policy_entropy     |    0.668  |\n",
      "|    advantage_mean     |   -0.137  |\n",
      "|    advantage_std      |    0.084  |\n",
      "|    aux_loss_mean      |    0.004  |\n",
      "| time/                 |           |\n",
      "|    fps                |      212  |\n",
      "|    episodes           |    10300  |\n",
      "|    time_elapsed       |     1939  |\n",
      "|    total_timesteps    |   412000  |\n",
      "| train/                |           |\n",
      "|    loss               |   -3.757  |\n",
      "|    policy_loss        |   -3.703  |\n",
      "|    value_loss         |    0.026  |\n",
      "|    explained_variance |   -0.940  |\n",
      "|    n_updates          |    10300  |\n",
      "| aux_train/            |           |\n",
      "|    aux_confidence_mse |    0.004  |\n",
      "| rnd_net_dist/         |           |\n",
      "|    mean_rnd_bonus     |    0.000  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        |   40.000  |\n",
      "|    ep_rew_mean        |    0.420  |\n",
      "|    ep_rew_std         |    0.494  |\n",
      "|    policy_entropy     |    0.671  |\n",
      "|    advantage_mean     |    0.242  |\n",
      "|    advantage_std      |    0.163  |\n",
      "|    aux_loss_mean      |    0.004  |\n",
      "| time/                 |           |\n",
      "|    fps                |      212  |\n",
      "|    episodes           |    10400  |\n",
      "|    time_elapsed       |     1961  |\n",
      "|    total_timesteps    |   416000  |\n",
      "| train/                |           |\n",
      "|    loss               |    6.604  |\n",
      "|    policy_loss        |    6.628  |\n",
      "|    value_loss         |    0.085  |\n",
      "|    explained_variance |    0.345  |\n",
      "|    n_updates          |    10400  |\n",
      "| aux_train/            |           |\n",
      "|    aux_confidence_mse |    0.005  |\n",
      "| rnd_net_dist/         |           |\n",
      "|    mean_rnd_bonus     |    0.000  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        |   40.000  |\n",
      "|    ep_rew_mean        |    0.330  |\n",
      "|    ep_rew_std         |    0.470  |\n",
      "|    policy_entropy     |    0.673  |\n",
      "|    advantage_mean     |    0.282  |\n",
      "|    advantage_std      |    0.194  |\n",
      "|    aux_loss_mean      |    0.003  |\n",
      "| time/                 |           |\n",
      "|    fps                |      211  |\n",
      "|    episodes           |    10500  |\n",
      "|    time_elapsed       |     1987  |\n",
      "|    total_timesteps    |   420000  |\n",
      "| train/                |           |\n",
      "|    loss               |    6.924  |\n",
      "|    policy_loss        |    6.933  |\n",
      "|    value_loss         |    0.116  |\n",
      "|    explained_variance |    0.218  |\n",
      "|    n_updates          |    10500  |\n",
      "| aux_train/            |           |\n",
      "|    aux_confidence_mse |    0.003  |\n",
      "| rnd_net_dist/         |           |\n",
      "|    mean_rnd_bonus     |    0.000  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        |   40.000  |\n",
      "|    ep_rew_mean        |    0.360  |\n",
      "|    ep_rew_std         |    0.480  |\n",
      "|    policy_entropy     |    0.600  |\n",
      "|    advantage_mean     |    0.273  |\n",
      "|    advantage_std      |    0.192  |\n",
      "|    aux_loss_mean      |    0.004  |\n",
      "| time/                 |           |\n",
      "|    fps                |      210  |\n",
      "|    episodes           |    10600  |\n",
      "|    time_elapsed       |     2011  |\n",
      "|    total_timesteps    |   424000  |\n",
      "| train/                |           |\n",
      "|    loss               |    5.507  |\n",
      "|    policy_loss        |    5.512  |\n",
      "|    value_loss         |    0.110  |\n",
      "|    explained_variance |    0.196  |\n",
      "|    n_updates          |    10600  |\n",
      "| aux_train/            |           |\n",
      "|    aux_confidence_mse |    0.006  |\n",
      "| rnd_net_dist/         |           |\n",
      "|    mean_rnd_bonus     |    0.000  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        |   40.000  |\n",
      "|    ep_rew_mean        |    0.360  |\n",
      "|    ep_rew_std         |    0.480  |\n",
      "|    policy_entropy     |    0.644  |\n",
      "|    advantage_mean     |   -0.137  |\n",
      "|    advantage_std      |    0.095  |\n",
      "|    aux_loss_mean      |    0.004  |\n",
      "| time/                 |           |\n",
      "|    fps                |      210  |\n",
      "|    episodes           |    10700  |\n",
      "|    time_elapsed       |     2035  |\n",
      "|    total_timesteps    |   428000  |\n",
      "| train/                |           |\n",
      "|    loss               |   -3.751  |\n",
      "|    policy_loss        |   -3.700  |\n",
      "|    value_loss         |    0.028  |\n",
      "|    explained_variance |   -1.093  |\n",
      "|    n_updates          |    10700  |\n",
      "| aux_train/            |           |\n",
      "|    aux_confidence_mse |    0.002  |\n",
      "| rnd_net_dist/         |           |\n",
      "|    mean_rnd_bonus     |    0.000  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        |   40.000  |\n",
      "|    ep_rew_mean        |    0.390  |\n",
      "|    ep_rew_std         |    0.488  |\n",
      "|    policy_entropy     |    0.556  |\n",
      "|    advantage_mean     |   -0.133  |\n",
      "|    advantage_std      |    0.109  |\n",
      "|    aux_loss_mean      |    0.005  |\n",
      "| time/                 |           |\n",
      "|    fps                |      209  |\n",
      "|    episodes           |    10800  |\n",
      "|    time_elapsed       |     2059  |\n",
      "|    total_timesteps    |   432000  |\n",
      "| train/                |           |\n",
      "|    loss               |   -2.339  |\n",
      "|    policy_loss        |   -2.299  |\n",
      "|    value_loss         |    0.029  |\n",
      "|    explained_variance |   -2.263  |\n",
      "|    n_updates          |    10800  |\n",
      "| aux_train/            |           |\n",
      "|    aux_confidence_mse |    0.013  |\n",
      "| rnd_net_dist/         |           |\n",
      "|    mean_rnd_bonus     |    0.000  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        |   40.000  |\n",
      "|    ep_rew_mean        |    0.390  |\n",
      "|    ep_rew_std         |    0.488  |\n",
      "|    policy_entropy     |    0.559  |\n",
      "|    advantage_mean     |    0.255  |\n",
      "|    advantage_std      |    0.187  |\n",
      "|    aux_loss_mean      |    0.008  |\n",
      "| time/                 |           |\n",
      "|    fps                |      209  |\n",
      "|    episodes           |    10900  |\n",
      "|    time_elapsed       |     2083  |\n",
      "|    total_timesteps    |   436000  |\n",
      "| train/                |           |\n",
      "|    loss               |    5.579  |\n",
      "|    policy_loss        |    5.584  |\n",
      "|    value_loss         |    0.099  |\n",
      "|    explained_variance |    0.190  |\n",
      "|    n_updates          |    10900  |\n",
      "| aux_train/            |           |\n",
      "|    aux_confidence_mse |    0.012  |\n",
      "| rnd_net_dist/         |           |\n",
      "|    mean_rnd_bonus     |    0.000  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        |   40.000  |\n",
      "|    ep_rew_mean        |    0.340  |\n",
      "|    ep_rew_std         |    0.474  |\n",
      "|    policy_entropy     |    0.572  |\n",
      "|    advantage_mean     |    0.265  |\n",
      "|    advantage_std      |    0.197  |\n",
      "|    aux_loss_mean      |    0.010  |\n",
      "| time/                 |           |\n",
      "|    fps                |      208  |\n",
      "|    episodes           |    11000  |\n",
      "|    time_elapsed       |     2107  |\n",
      "|    total_timesteps    |   440000  |\n",
      "| train/                |           |\n",
      "|    loss               |    6.273  |\n",
      "|    policy_loss        |    6.275  |\n",
      "|    value_loss         |    0.108  |\n",
      "|    explained_variance |    0.101  |\n",
      "|    n_updates          |    11000  |\n",
      "| aux_train/            |           |\n",
      "|    aux_confidence_mse |    0.012  |\n",
      "| rnd_net_dist/         |           |\n",
      "|    mean_rnd_bonus     |    0.000  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        |   40.000  |\n",
      "|    ep_rew_mean        |    0.340  |\n",
      "|    ep_rew_std         |    0.474  |\n",
      "|    policy_entropy     |    0.618  |\n",
      "|    advantage_mean     |   -0.132  |\n",
      "|    advantage_std      |    0.098  |\n",
      "|    aux_loss_mean      |    0.006  |\n",
      "| time/                 |           |\n",
      "|    fps                |      208  |\n",
      "|    episodes           |    11100  |\n",
      "|    time_elapsed       |     2132  |\n",
      "|    total_timesteps    |   444000  |\n",
      "| train/                |           |\n",
      "|    loss               |   -3.110  |\n",
      "|    policy_loss        |   -3.063  |\n",
      "|    value_loss         |    0.027  |\n",
      "|    explained_variance |   -1.737  |\n",
      "|    n_updates          |    11100  |\n",
      "| aux_train/            |           |\n",
      "|    aux_confidence_mse |    0.007  |\n",
      "| rnd_net_dist/         |           |\n",
      "|    mean_rnd_bonus     |    0.000  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        |   40.000  |\n",
      "|    ep_rew_mean        |    0.420  |\n",
      "|    ep_rew_std         |    0.494  |\n",
      "|    policy_entropy     |    0.607  |\n",
      "|    advantage_mean     |    0.231  |\n",
      "|    advantage_std      |    0.161  |\n",
      "|    aux_loss_mean      |    0.005  |\n",
      "| time/                 |           |\n",
      "|    fps                |      207  |\n",
      "|    episodes           |    11200  |\n",
      "|    time_elapsed       |     2156  |\n",
      "|    total_timesteps    |   448000  |\n",
      "| train/                |           |\n",
      "|    loss               |    4.771  |\n",
      "|    policy_loss        |    4.791  |\n",
      "|    value_loss         |    0.078  |\n",
      "|    explained_variance |    0.318  |\n",
      "|    n_updates          |    11200  |\n",
      "| aux_train/            |           |\n",
      "|    aux_confidence_mse |    0.007  |\n",
      "| rnd_net_dist/         |           |\n",
      "|    mean_rnd_bonus     |    0.000  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        |   40.000  |\n",
      "|    ep_rew_mean        |    0.310  |\n",
      "|    ep_rew_std         |    0.462  |\n",
      "|    policy_entropy     |    0.625  |\n",
      "|    advantage_mean     |    0.267  |\n",
      "|    advantage_std      |    0.187  |\n",
      "|    aux_loss_mean      |    0.004  |\n",
      "| time/                 |           |\n",
      "|    fps                |      207  |\n",
      "|    episodes           |    11300  |\n",
      "|    time_elapsed       |     2180  |\n",
      "|    total_timesteps    |   452000  |\n",
      "| train/                |           |\n",
      "|    loss               |    5.990  |\n",
      "|    policy_loss        |    5.999  |\n",
      "|    value_loss         |    0.105  |\n",
      "|    explained_variance |    0.220  |\n",
      "|    n_updates          |    11300  |\n",
      "| aux_train/            |           |\n",
      "|    aux_confidence_mse |    0.005  |\n",
      "| rnd_net_dist/         |           |\n",
      "|    mean_rnd_bonus     |    0.000  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        |   40.000  |\n",
      "|    ep_rew_mean        |    0.430  |\n",
      "|    ep_rew_std         |    0.495  |\n",
      "|    policy_entropy     |    0.645  |\n",
      "|    advantage_mean     |    0.232  |\n",
      "|    advantage_std      |    0.174  |\n",
      "|    aux_loss_mean      |    0.004  |\n",
      "| time/                 |           |\n",
      "|    fps                |      206  |\n",
      "|    episodes           |    11400  |\n",
      "|    time_elapsed       |     2203  |\n",
      "|    total_timesteps    |   456000  |\n",
      "| train/                |           |\n",
      "|    loss               |    7.221  |\n",
      "|    policy_loss        |    7.244  |\n",
      "|    value_loss         |    0.083  |\n",
      "|    explained_variance |    0.185  |\n",
      "|    n_updates          |    11400  |\n",
      "| aux_train/            |           |\n",
      "|    aux_confidence_mse |    0.005  |\n",
      "| rnd_net_dist/         |           |\n",
      "|    mean_rnd_bonus     |    0.000  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        |   40.000  |\n",
      "|    ep_rew_mean        |    0.420  |\n",
      "|    ep_rew_std         |    0.494  |\n",
      "|    policy_entropy     |    0.656  |\n",
      "|    advantage_mean     |   -0.166  |\n",
      "|    advantage_std      |    0.109  |\n",
      "|    aux_loss_mean      |    0.004  |\n",
      "| time/                 |           |\n",
      "|    fps                |      206  |\n",
      "|    episodes           |    11500  |\n",
      "|    time_elapsed       |     2228  |\n",
      "|    total_timesteps    |   460000  |\n",
      "| train/                |           |\n",
      "|    loss               |   -3.999  |\n",
      "|    policy_loss        |   -3.954  |\n",
      "|    value_loss         |    0.039  |\n",
      "|    explained_variance |   -1.233  |\n",
      "|    n_updates          |    11500  |\n",
      "| aux_train/            |           |\n",
      "|    aux_confidence_mse |    0.007  |\n",
      "| rnd_net_dist/         |           |\n",
      "|    mean_rnd_bonus     |    0.000  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        |   40.000  |\n",
      "|    ep_rew_mean        |    0.240  |\n",
      "|    ep_rew_std         |    0.427  |\n",
      "|    policy_entropy     |    0.667  |\n",
      "|    advantage_mean     |    0.283  |\n",
      "|    advantage_std      |    0.199  |\n",
      "|    aux_loss_mean      |    0.004  |\n",
      "| time/                 |           |\n",
      "|    fps                |      206  |\n",
      "|    episodes           |    11600  |\n",
      "|    time_elapsed       |     2251  |\n",
      "|    total_timesteps    |   464000  |\n",
      "| train/                |           |\n",
      "|    loss               |    8.276  |\n",
      "|    policy_loss        |    8.283  |\n",
      "|    value_loss         |    0.118  |\n",
      "|    explained_variance |    0.179  |\n",
      "|    n_updates          |    11600  |\n",
      "| aux_train/            |           |\n",
      "|    aux_confidence_mse |    0.003  |\n",
      "| rnd_net_dist/         |           |\n",
      "|    mean_rnd_bonus     |    0.000  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        |   40.000  |\n",
      "|    ep_rew_mean        |    0.340  |\n",
      "|    ep_rew_std         |    0.474  |\n",
      "|    policy_entropy     |    0.659  |\n",
      "|    advantage_mean     |   -0.122  |\n",
      "|    advantage_std      |    0.093  |\n",
      "|    aux_loss_mean      |    0.004  |\n",
      "| time/                 |           |\n",
      "|    fps                |      205  |\n",
      "|    episodes           |    11700  |\n",
      "|    time_elapsed       |     2275  |\n",
      "|    total_timesteps    |   468000  |\n",
      "| train/                |           |\n",
      "|    loss               |   -3.088  |\n",
      "|    policy_loss        |   -3.034  |\n",
      "|    value_loss         |    0.023  |\n",
      "|    explained_variance |   -2.111  |\n",
      "|    n_updates          |    11700  |\n",
      "| aux_train/            |           |\n",
      "|    aux_confidence_mse |    0.005  |\n",
      "| rnd_net_dist/         |           |\n",
      "|    mean_rnd_bonus     |    0.000  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        |   40.000  |\n",
      "|    ep_rew_mean        |    0.350  |\n",
      "|    ep_rew_std         |    0.477  |\n",
      "|    policy_entropy     |    0.660  |\n",
      "|    advantage_mean     |   -0.122  |\n",
      "|    advantage_std      |    0.084  |\n",
      "|    aux_loss_mean      |    0.004  |\n",
      "| time/                 |           |\n",
      "|    fps                |      205  |\n",
      "|    episodes           |    11800  |\n",
      "|    time_elapsed       |     2298  |\n",
      "|    total_timesteps    |   472000  |\n",
      "| train/                |           |\n",
      "|    loss               |   -3.087  |\n",
      "|    policy_loss        |   -3.032  |\n",
      "|    value_loss         |    0.022  |\n",
      "|    explained_variance |   -1.652  |\n",
      "|    n_updates          |    11800  |\n",
      "| aux_train/            |           |\n",
      "|    aux_confidence_mse |    0.004  |\n",
      "| rnd_net_dist/         |           |\n",
      "|    mean_rnd_bonus     |    0.000  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        |   40.000  |\n",
      "|    ep_rew_mean        |    0.360  |\n",
      "|    ep_rew_std         |    0.480  |\n",
      "|    policy_entropy     |    0.663  |\n",
      "|    advantage_mean     |   -0.134  |\n",
      "|    advantage_std      |    0.090  |\n",
      "|    aux_loss_mean      |    0.004  |\n",
      "| time/                 |           |\n",
      "|    fps                |      204  |\n",
      "|    episodes           |    11900  |\n",
      "|    time_elapsed       |     2322  |\n",
      "|    total_timesteps    |   476000  |\n",
      "| train/                |           |\n",
      "|    loss               |   -3.621  |\n",
      "|    policy_loss        |   -3.568  |\n",
      "|    value_loss         |    0.026  |\n",
      "|    explained_variance |   -1.420  |\n",
      "|    n_updates          |    11900  |\n",
      "| aux_train/            |           |\n",
      "|    aux_confidence_mse |    0.003  |\n",
      "| rnd_net_dist/         |           |\n",
      "|    mean_rnd_bonus     |    0.000  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        |   40.000  |\n",
      "|    ep_rew_mean        |    0.330  |\n",
      "|    ep_rew_std         |    0.470  |\n",
      "|    policy_entropy     |    0.670  |\n",
      "|    advantage_mean     |   -0.134  |\n",
      "|    advantage_std      |    0.100  |\n",
      "|    aux_loss_mean      |    0.004  |\n",
      "| time/                 |           |\n",
      "|    fps                |      204  |\n",
      "|    episodes           |    12000  |\n",
      "|    time_elapsed       |     2346  |\n",
      "|    total_timesteps    |   480000  |\n",
      "| train/                |           |\n",
      "|    loss               |   -3.754  |\n",
      "|    policy_loss        |   -3.701  |\n",
      "|    value_loss         |    0.028  |\n",
      "|    explained_variance |   -1.774  |\n",
      "|    n_updates          |    12000  |\n",
      "| aux_train/            |           |\n",
      "|    aux_confidence_mse |    0.002  |\n",
      "| rnd_net_dist/         |           |\n",
      "|    mean_rnd_bonus     |    0.000  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        |   40.000  |\n",
      "|    ep_rew_mean        |    0.310  |\n",
      "|    ep_rew_std         |    0.462  |\n",
      "|    policy_entropy     |    0.643  |\n",
      "|    advantage_mean     |    0.250  |\n",
      "|    advantage_std      |    0.189  |\n",
      "|    aux_loss_mean      |    0.003  |\n",
      "| time/                 |           |\n",
      "|    fps                |      204  |\n",
      "|    episodes           |    12100  |\n",
      "|    time_elapsed       |     2370  |\n",
      "|    total_timesteps    |   484000  |\n",
      "| train/                |           |\n",
      "|    loss               |    5.870  |\n",
      "|    policy_loss        |    5.886  |\n",
      "|    value_loss         |    0.097  |\n",
      "|    explained_variance |    0.169  |\n",
      "|    n_updates          |    12100  |\n",
      "| aux_train/            |           |\n",
      "|    aux_confidence_mse |    0.003  |\n",
      "| rnd_net_dist/         |           |\n",
      "|    mean_rnd_bonus     |    0.000  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        |   40.000  |\n",
      "|    ep_rew_mean        |    0.300  |\n",
      "|    ep_rew_std         |    0.458  |\n",
      "|    policy_entropy     |    0.618  |\n",
      "|    advantage_mean     |   -0.114  |\n",
      "|    advantage_std      |    0.092  |\n",
      "|    aux_loss_mean      |    0.004  |\n",
      "| time/                 |           |\n",
      "|    fps                |      203  |\n",
      "|    episodes           |    12200  |\n",
      "|    time_elapsed       |     2394  |\n",
      "|    total_timesteps    |   488000  |\n",
      "| train/                |           |\n",
      "|    loss               |   -2.918  |\n",
      "|    policy_loss        |   -2.867  |\n",
      "|    value_loss         |    0.021  |\n",
      "|    explained_variance |   -2.634  |\n",
      "|    n_updates          |    12200  |\n",
      "| aux_train/            |           |\n",
      "|    aux_confidence_mse |    0.006  |\n",
      "| rnd_net_dist/         |           |\n",
      "|    mean_rnd_bonus     |    0.000  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        |   40.000  |\n",
      "|    ep_rew_mean        |    0.320  |\n",
      "|    ep_rew_std         |    0.466  |\n",
      "|    policy_entropy     |    0.664  |\n",
      "|    advantage_mean     |   -0.133  |\n",
      "|    advantage_std      |    0.106  |\n",
      "|    aux_loss_mean      |    0.004  |\n",
      "| time/                 |           |\n",
      "|    fps                |      203  |\n",
      "|    episodes           |    12300  |\n",
      "|    time_elapsed       |     2418  |\n",
      "|    total_timesteps    |   492000  |\n",
      "| train/                |           |\n",
      "|    loss               |   -3.516  |\n",
      "|    policy_loss        |   -3.464  |\n",
      "|    value_loss         |    0.028  |\n",
      "|    explained_variance |   -2.127  |\n",
      "|    n_updates          |    12300  |\n",
      "| aux_train/            |           |\n",
      "|    aux_confidence_mse |    0.004  |\n",
      "| rnd_net_dist/         |           |\n",
      "|    mean_rnd_bonus     |    0.000  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        |   40.000  |\n",
      "|    ep_rew_mean        |    0.380  |\n",
      "|    ep_rew_std         |    0.485  |\n",
      "|    policy_entropy     |    0.657  |\n",
      "|    advantage_mean     |    0.246  |\n",
      "|    advantage_std      |    0.168  |\n",
      "|    aux_loss_mean      |    0.004  |\n",
      "| time/                 |           |\n",
      "|    fps                |      203  |\n",
      "|    episodes           |    12400  |\n",
      "|    time_elapsed       |     2442  |\n",
      "|    total_timesteps    |   496000  |\n",
      "| train/                |           |\n",
      "|    loss               |    6.080  |\n",
      "|    policy_loss        |    6.101  |\n",
      "|    value_loss         |    0.088  |\n",
      "|    explained_variance |    0.312  |\n",
      "|    n_updates          |    12400  |\n",
      "| aux_train/            |           |\n",
      "|    aux_confidence_mse |    0.002  |\n",
      "| rnd_net_dist/         |           |\n",
      "|    mean_rnd_bonus     |    0.000  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        |   40.000  |\n",
      "|    ep_rew_mean        |    0.310  |\n",
      "|    ep_rew_std         |    0.462  |\n",
      "|    policy_entropy     |    0.623  |\n",
      "|    advantage_mean     |   -0.110  |\n",
      "|    advantage_std      |    0.086  |\n",
      "|    aux_loss_mean      |    0.004  |\n",
      "| time/                 |           |\n",
      "|    fps                |      202  |\n",
      "|    episodes           |    12500  |\n",
      "|    time_elapsed       |     2466  |\n",
      "|    total_timesteps    |   500000  |\n",
      "| train/                |           |\n",
      "|    loss               |   -2.688  |\n",
      "|    policy_loss        |   -2.636  |\n",
      "|    value_loss         |    0.019  |\n",
      "|    explained_variance |   -2.115  |\n",
      "|    n_updates          |    12500  |\n",
      "| aux_train/            |           |\n",
      "|    aux_confidence_mse |    0.004  |\n",
      "| rnd_net_dist/         |           |\n",
      "|    mean_rnd_bonus     |    0.000  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        |   40.000  |\n",
      "|    ep_rew_mean        |    0.350  |\n",
      "|    ep_rew_std         |    0.477  |\n",
      "|    policy_entropy     |    0.631  |\n",
      "|    advantage_mean     |   -0.125  |\n",
      "|    advantage_std      |    0.072  |\n",
      "|    aux_loss_mean      |    0.004  |\n",
      "| time/                 |           |\n",
      "|    fps                |      202  |\n",
      "|    episodes           |    12600  |\n",
      "|    time_elapsed       |     2489  |\n",
      "|    total_timesteps    |   504000  |\n",
      "| train/                |           |\n",
      "|    loss               |   -3.745  |\n",
      "|    policy_loss        |   -3.692  |\n",
      "|    value_loss         |    0.021  |\n",
      "|    explained_variance |   -0.636  |\n",
      "|    n_updates          |    12600  |\n",
      "| aux_train/            |           |\n",
      "|    aux_confidence_mse |    0.004  |\n",
      "| rnd_net_dist/         |           |\n",
      "|    mean_rnd_bonus     |    0.000  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        |   40.000  |\n",
      "|    ep_rew_mean        |    0.360  |\n",
      "|    ep_rew_std         |    0.480  |\n",
      "|    policy_entropy     |    0.632  |\n",
      "|    advantage_mean     |   -0.142  |\n",
      "|    advantage_std      |    0.096  |\n",
      "|    aux_loss_mean      |    0.004  |\n",
      "| time/                 |           |\n",
      "|    fps                |      202  |\n",
      "|    episodes           |    12700  |\n",
      "|    time_elapsed       |     2513  |\n",
      "|    total_timesteps    |   508000  |\n",
      "| train/                |           |\n",
      "|    loss               |   -3.576  |\n",
      "|    policy_loss        |   -3.528  |\n",
      "|    value_loss         |    0.029  |\n",
      "|    explained_variance |   -1.578  |\n",
      "|    n_updates          |    12700  |\n",
      "| aux_train/            |           |\n",
      "|    aux_confidence_mse |    0.003  |\n",
      "| rnd_net_dist/         |           |\n",
      "|    mean_rnd_bonus     |    0.000  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        |   40.000  |\n",
      "|    ep_rew_mean        |    0.330  |\n",
      "|    ep_rew_std         |    0.470  |\n",
      "|    policy_entropy     |    0.611  |\n",
      "|    advantage_mean     |    0.249  |\n",
      "|    advantage_std      |    0.174  |\n",
      "|    aux_loss_mean      |    0.007  |\n",
      "| time/                 |           |\n",
      "|    fps                |      201  |\n",
      "|    episodes           |    12800  |\n",
      "|    time_elapsed       |     2536  |\n",
      "|    total_timesteps    |   512000  |\n",
      "| train/                |           |\n",
      "|    loss               |    5.959  |\n",
      "|    policy_loss        |    5.974  |\n",
      "|    value_loss         |    0.092  |\n",
      "|    explained_variance |    0.290  |\n",
      "|    n_updates          |    12800  |\n",
      "| aux_train/            |           |\n",
      "|    aux_confidence_mse |    0.005  |\n",
      "| rnd_net_dist/         |           |\n",
      "|    mean_rnd_bonus     |    0.000  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        |   40.000  |\n",
      "|    ep_rew_mean        |    0.280  |\n",
      "|    ep_rew_std         |    0.449  |\n",
      "|    policy_entropy     |    0.622  |\n",
      "|    advantage_mean     |   -0.105  |\n",
      "|    advantage_std      |    0.077  |\n",
      "|    aux_loss_mean      |    0.006  |\n",
      "| time/                 |           |\n",
      "|    fps                |      201  |\n",
      "|    episodes           |    12900  |\n",
      "|    time_elapsed       |     2560  |\n",
      "|    total_timesteps    |   516000  |\n",
      "| train/                |           |\n",
      "|    loss               |   -2.515  |\n",
      "|    policy_loss        |   -2.462  |\n",
      "|    value_loss         |    0.017  |\n",
      "|    explained_variance |   -2.105  |\n",
      "|    n_updates          |    12900  |\n",
      "| aux_train/            |           |\n",
      "|    aux_confidence_mse |    0.005  |\n",
      "| rnd_net_dist/         |           |\n",
      "|    mean_rnd_bonus     |    0.000  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        |   40.000  |\n",
      "|    ep_rew_mean        |    0.350  |\n",
      "|    ep_rew_std         |    0.477  |\n",
      "|    policy_entropy     |    0.610  |\n",
      "|    advantage_mean     |    0.256  |\n",
      "|    advantage_std      |    0.180  |\n",
      "|    aux_loss_mean      |    0.007  |\n",
      "| time/                 |           |\n",
      "|    fps                |      201  |\n",
      "|    episodes           |    13000  |\n",
      "|    time_elapsed       |     2584  |\n",
      "|    total_timesteps    |   520000  |\n",
      "| train/                |           |\n",
      "|    loss               |    6.107  |\n",
      "|    policy_loss        |    6.119  |\n",
      "|    value_loss         |    0.097  |\n",
      "|    explained_variance |    0.221  |\n",
      "|    n_updates          |    13000  |\n",
      "| aux_train/            |           |\n",
      "|    aux_confidence_mse |    0.011  |\n",
      "| rnd_net_dist/         |           |\n",
      "|    mean_rnd_bonus     |    0.000  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        |   40.000  |\n",
      "|    ep_rew_mean        |    0.350  |\n",
      "|    ep_rew_std         |    0.477  |\n",
      "|    policy_entropy     |    0.659  |\n",
      "|    advantage_mean     |   -0.139  |\n",
      "|    advantage_std      |    0.091  |\n",
      "|    aux_loss_mean      |    0.005  |\n",
      "| time/                 |           |\n",
      "|    fps                |      200  |\n",
      "|    episodes           |    13100  |\n",
      "|    time_elapsed       |     2608  |\n",
      "|    total_timesteps    |   524000  |\n",
      "| train/                |           |\n",
      "|    loss               |   -3.264  |\n",
      "|    policy_loss        |   -3.212  |\n",
      "|    value_loss         |    0.027  |\n",
      "|    explained_variance |   -1.264  |\n",
      "|    n_updates          |    13100  |\n",
      "| aux_train/            |           |\n",
      "|    aux_confidence_mse |    0.003  |\n",
      "| rnd_net_dist/         |           |\n",
      "|    mean_rnd_bonus     |    0.000  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        |   40.000  |\n",
      "|    ep_rew_mean        |    0.340  |\n",
      "|    ep_rew_std         |    0.474  |\n",
      "|    policy_entropy     |    0.617  |\n",
      "|    advantage_mean     |   -0.122  |\n",
      "|    advantage_std      |    0.094  |\n",
      "|    aux_loss_mean      |    0.006  |\n",
      "| time/                 |           |\n",
      "|    fps                |      200  |\n",
      "|    episodes           |    13200  |\n",
      "|    time_elapsed       |     2632  |\n",
      "|    total_timesteps    |   528000  |\n",
      "| train/                |           |\n",
      "|    loss               |   -3.205  |\n",
      "|    policy_loss        |   -3.156  |\n",
      "|    value_loss         |    0.023  |\n",
      "|    explained_variance |   -1.974  |\n",
      "|    n_updates          |    13200  |\n",
      "| aux_train/            |           |\n",
      "|    aux_confidence_mse |    0.011  |\n",
      "| rnd_net_dist/         |           |\n",
      "|    mean_rnd_bonus     |    0.000  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        |   40.000  |\n",
      "|    ep_rew_mean        |    0.320  |\n",
      "|    ep_rew_std         |    0.466  |\n",
      "|    policy_entropy     |    0.662  |\n",
      "|    advantage_mean     |   -0.126  |\n",
      "|    advantage_std      |    0.081  |\n",
      "|    aux_loss_mean      |    0.006  |\n",
      "| time/                 |           |\n",
      "|    fps                |      200  |\n",
      "|    episodes           |    13300  |\n",
      "|    time_elapsed       |     2655  |\n",
      "|    total_timesteps    |   532000  |\n",
      "| train/                |           |\n",
      "|    loss               |   -3.388  |\n",
      "|    policy_loss        |   -3.333  |\n",
      "|    value_loss         |    0.022  |\n",
      "|    explained_variance |   -1.095  |\n",
      "|    n_updates          |    13300  |\n",
      "| aux_train/            |           |\n",
      "|    aux_confidence_mse |    0.004  |\n",
      "| rnd_net_dist/         |           |\n",
      "|    mean_rnd_bonus     |    0.000  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        |   40.000  |\n",
      "|    ep_rew_mean        |    0.380  |\n",
      "|    ep_rew_std         |    0.485  |\n",
      "|    policy_entropy     |    0.642  |\n",
      "|    advantage_mean     |    0.242  |\n",
      "|    advantage_std      |    0.184  |\n",
      "|    aux_loss_mean      |    0.005  |\n",
      "| time/                 |           |\n",
      "|    fps                |      200  |\n",
      "|    episodes           |    13400  |\n",
      "|    time_elapsed       |     2679  |\n",
      "|    total_timesteps    |   536000  |\n",
      "| train/                |           |\n",
      "|    loss               |    6.059  |\n",
      "|    policy_loss        |    6.077  |\n",
      "|    value_loss         |    0.091  |\n",
      "|    explained_variance |    0.155  |\n",
      "|    n_updates          |    13400  |\n",
      "| aux_train/            |           |\n",
      "|    aux_confidence_mse |    0.005  |\n",
      "| rnd_net_dist/         |           |\n",
      "|    mean_rnd_bonus     |    0.000  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        |   40.000  |\n",
      "|    ep_rew_mean        |    0.340  |\n",
      "|    ep_rew_std         |    0.474  |\n",
      "|    policy_entropy     |    0.630  |\n",
      "|    advantage_mean     |   -0.132  |\n",
      "|    advantage_std      |    0.084  |\n",
      "|    aux_loss_mean      |    0.004  |\n",
      "| time/                 |           |\n",
      "|    fps                |      199  |\n",
      "|    episodes           |    13500  |\n",
      "|    time_elapsed       |     2703  |\n",
      "|    total_timesteps    |   540000  |\n",
      "| train/                |           |\n",
      "|    loss               |   -3.462  |\n",
      "|    policy_loss        |   -3.412  |\n",
      "|    value_loss         |    0.024  |\n",
      "|    explained_variance |   -1.184  |\n",
      "|    n_updates          |    13500  |\n",
      "| aux_train/            |           |\n",
      "|    aux_confidence_mse |    0.008  |\n",
      "| rnd_net_dist/         |           |\n",
      "|    mean_rnd_bonus     |    0.000  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        |   40.000  |\n",
      "|    ep_rew_mean        |    0.280  |\n",
      "|    ep_rew_std         |    0.449  |\n",
      "|    policy_entropy     |    0.652  |\n",
      "|    advantage_mean     |   -0.100  |\n",
      "|    advantage_std      |    0.070  |\n",
      "|    aux_loss_mean      |    0.005  |\n",
      "| time/                 |           |\n",
      "|    fps                |      199  |\n",
      "|    episodes           |    13600  |\n",
      "|    time_elapsed       |     2727  |\n",
      "|    total_timesteps    |   544000  |\n",
      "| train/                |           |\n",
      "|    loss               |   -2.841  |\n",
      "|    policy_loss        |   -2.783  |\n",
      "|    value_loss         |    0.015  |\n",
      "|    explained_variance |   -1.596  |\n",
      "|    n_updates          |    13600  |\n",
      "| aux_train/            |           |\n",
      "|    aux_confidence_mse |    0.003  |\n",
      "| rnd_net_dist/         |           |\n",
      "|    mean_rnd_bonus     |    0.000  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        |   40.000  |\n",
      "|    ep_rew_mean        |    0.460  |\n",
      "|    ep_rew_std         |    0.498  |\n",
      "|    policy_entropy     |    0.631  |\n",
      "|    advantage_mean     |   -0.178  |\n",
      "|    advantage_std      |    0.110  |\n",
      "|    aux_loss_mean      |    0.005  |\n",
      "| time/                 |           |\n",
      "|    fps                |      199  |\n",
      "|    episodes           |    13700  |\n",
      "|    time_elapsed       |     2750  |\n",
      "|    total_timesteps    |   548000  |\n",
      "| train/                |           |\n",
      "|    loss               |   -5.060  |\n",
      "|    policy_loss        |   -5.019  |\n",
      "|    value_loss         |    0.044  |\n",
      "|    explained_variance |   -0.896  |\n",
      "|    n_updates          |    13700  |\n",
      "| aux_train/            |           |\n",
      "|    aux_confidence_mse |    0.004  |\n",
      "| rnd_net_dist/         |           |\n",
      "|    mean_rnd_bonus     |    0.000  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        |   40.000  |\n",
      "|    ep_rew_mean        |    0.360  |\n",
      "|    ep_rew_std         |    0.480  |\n",
      "|    policy_entropy     |    0.626  |\n",
      "|    advantage_mean     |    0.248  |\n",
      "|    advantage_std      |    0.180  |\n",
      "|    aux_loss_mean      |    0.005  |\n",
      "| time/                 |           |\n",
      "|    fps                |      198  |\n",
      "|    episodes           |    13800  |\n",
      "|    time_elapsed       |     2774  |\n",
      "|    total_timesteps    |   552000  |\n",
      "| train/                |           |\n",
      "|    loss               |    5.107  |\n",
      "|    policy_loss        |    5.123  |\n",
      "|    value_loss         |    0.093  |\n",
      "|    explained_variance |    0.208  |\n",
      "|    n_updates          |    13800  |\n",
      "| aux_train/            |           |\n",
      "|    aux_confidence_mse |    0.004  |\n",
      "| rnd_net_dist/         |           |\n",
      "|    mean_rnd_bonus     |    0.000  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        |   40.000  |\n",
      "|    ep_rew_mean        |    0.400  |\n",
      "|    ep_rew_std         |    0.490  |\n",
      "|    policy_entropy     |    0.650  |\n",
      "|    advantage_mean     |   -0.155  |\n",
      "|    advantage_std      |    0.098  |\n",
      "|    aux_loss_mean      |    0.006  |\n",
      "| time/                 |           |\n",
      "|    fps                |      198  |\n",
      "|    episodes           |    13900  |\n",
      "|    time_elapsed       |     2798  |\n",
      "|    total_timesteps    |   556000  |\n",
      "| train/                |           |\n",
      "|    loss               |   -3.949  |\n",
      "|    policy_loss        |   -3.901  |\n",
      "|    value_loss         |    0.034  |\n",
      "|    explained_variance |   -1.011  |\n",
      "|    n_updates          |    13900  |\n",
      "| aux_train/            |           |\n",
      "|    aux_confidence_mse |    0.004  |\n",
      "| rnd_net_dist/         |           |\n",
      "|    mean_rnd_bonus     |    0.000  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        |   40.000  |\n",
      "|    ep_rew_mean        |    0.320  |\n",
      "|    ep_rew_std         |    0.466  |\n",
      "|    policy_entropy     |    0.665  |\n",
      "|    advantage_mean     |    0.251  |\n",
      "|    advantage_std      |    0.194  |\n",
      "|    aux_loss_mean      |    0.005  |\n",
      "| time/                 |           |\n",
      "|    fps                |      198  |\n",
      "|    episodes           |    14000  |\n",
      "|    time_elapsed       |     2821  |\n",
      "|    total_timesteps    |   560000  |\n",
      "| train/                |           |\n",
      "|    loss               |    7.488  |\n",
      "|    policy_loss        |    7.504  |\n",
      "|    value_loss         |    0.100  |\n",
      "|    explained_variance |    0.094  |\n",
      "|    n_updates          |    14000  |\n",
      "| aux_train/            |           |\n",
      "|    aux_confidence_mse |    0.004  |\n",
      "| rnd_net_dist/         |           |\n",
      "|    mean_rnd_bonus     |    0.000  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        |   40.000  |\n",
      "|    ep_rew_mean        |    0.340  |\n",
      "|    ep_rew_std         |    0.474  |\n",
      "|    policy_entropy     |    0.640  |\n",
      "|    advantage_mean     |   -0.135  |\n",
      "|    advantage_std      |    0.090  |\n",
      "|    aux_loss_mean      |    0.004  |\n",
      "| time/                 |           |\n",
      "|    fps                |      198  |\n",
      "|    episodes           |    14100  |\n",
      "|    time_elapsed       |     2845  |\n",
      "|    total_timesteps    |   564000  |\n",
      "| train/                |           |\n",
      "|    loss               |   -3.898  |\n",
      "|    policy_loss        |   -3.847  |\n",
      "|    value_loss         |    0.026  |\n",
      "|    explained_variance |   -1.070  |\n",
      "|    n_updates          |    14100  |\n",
      "| aux_train/            |           |\n",
      "|    aux_confidence_mse |    0.004  |\n",
      "| rnd_net_dist/         |           |\n",
      "|    mean_rnd_bonus     |    0.000  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        |   40.000  |\n",
      "|    ep_rew_mean        |    0.430  |\n",
      "|    ep_rew_std         |    0.495  |\n",
      "|    policy_entropy     |    0.630  |\n",
      "|    advantage_mean     |    0.241  |\n",
      "|    advantage_std      |    0.170  |\n",
      "|    aux_loss_mean      |    0.004  |\n",
      "| time/                 |           |\n",
      "|    fps                |      197  |\n",
      "|    episodes           |    14200  |\n",
      "|    time_elapsed       |     2869  |\n",
      "|    total_timesteps    |   568000  |\n",
      "| train/                |           |\n",
      "|    loss               |    7.259  |\n",
      "|    policy_loss        |    7.278  |\n",
      "|    value_loss         |    0.086  |\n",
      "|    explained_variance |    0.288  |\n",
      "|    n_updates          |    14200  |\n",
      "| aux_train/            |           |\n",
      "|    aux_confidence_mse |    0.005  |\n",
      "| rnd_net_dist/         |           |\n",
      "|    mean_rnd_bonus     |    0.000  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        |   40.000  |\n",
      "|    ep_rew_mean        |    0.350  |\n",
      "|    ep_rew_std         |    0.477  |\n",
      "|    policy_entropy     |    0.651  |\n",
      "|    advantage_mean     |    0.248  |\n",
      "|    advantage_std      |    0.183  |\n",
      "|    aux_loss_mean      |    0.005  |\n",
      "| time/                 |           |\n",
      "|    fps                |      197  |\n",
      "|    episodes           |    14300  |\n",
      "|    time_elapsed       |     2893  |\n",
      "|    total_timesteps    |   572000  |\n",
      "| train/                |           |\n",
      "|    loss               |    6.390  |\n",
      "|    policy_loss        |    6.407  |\n",
      "|    value_loss         |    0.094  |\n",
      "|    explained_variance |    0.196  |\n",
      "|    n_updates          |    14300  |\n",
      "| aux_train/            |           |\n",
      "|    aux_confidence_mse |    0.004  |\n",
      "| rnd_net_dist/         |           |\n",
      "|    mean_rnd_bonus     |    0.000  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        |   40.000  |\n",
      "|    ep_rew_mean        |    0.390  |\n",
      "|    ep_rew_std         |    0.488  |\n",
      "|    policy_entropy     |    0.637  |\n",
      "|    advantage_mean     |   -0.156  |\n",
      "|    advantage_std      |    0.096  |\n",
      "|    aux_loss_mean      |    0.005  |\n",
      "| time/                 |           |\n",
      "|    fps                |      197  |\n",
      "|    episodes           |    14400  |\n",
      "|    time_elapsed       |     2917  |\n",
      "|    total_timesteps    |   576000  |\n",
      "| train/                |           |\n",
      "|    loss               |   -3.601  |\n",
      "|    policy_loss        |   -3.554  |\n",
      "|    value_loss         |    0.033  |\n",
      "|    explained_variance |   -1.063  |\n",
      "|    n_updates          |    14400  |\n",
      "| aux_train/            |           |\n",
      "|    aux_confidence_mse |    0.006  |\n",
      "| rnd_net_dist/         |           |\n",
      "|    mean_rnd_bonus     |    0.000  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        |   40.000  |\n",
      "|    ep_rew_mean        |    0.330  |\n",
      "|    ep_rew_std         |    0.470  |\n",
      "|    policy_entropy     |    0.619  |\n",
      "|    advantage_mean     |   -0.123  |\n",
      "|    advantage_std      |    0.080  |\n",
      "|    aux_loss_mean      |    0.005  |\n",
      "| time/                 |           |\n",
      "|    fps                |      197  |\n",
      "|    episodes           |    14500  |\n",
      "|    time_elapsed       |     2940  |\n",
      "|    total_timesteps    |   580000  |\n",
      "| train/                |           |\n",
      "|    loss               |   -2.666  |\n",
      "|    policy_loss        |   -2.615  |\n",
      "|    value_loss         |    0.021  |\n",
      "|    explained_variance |   -1.136  |\n",
      "|    n_updates          |    14500  |\n",
      "| aux_train/            |           |\n",
      "|    aux_confidence_mse |    0.009  |\n",
      "| rnd_net_dist/         |           |\n",
      "|    mean_rnd_bonus     |    0.000  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        |   40.000  |\n",
      "|    ep_rew_mean        |    0.270  |\n",
      "|    ep_rew_std         |    0.444  |\n",
      "|    policy_entropy     |    0.647  |\n",
      "|    advantage_mean     |   -0.108  |\n",
      "|    advantage_std      |    0.086  |\n",
      "|    aux_loss_mean      |    0.005  |\n",
      "| time/                 |           |\n",
      "|    fps                |      197  |\n",
      "|    episodes           |    14600  |\n",
      "|    time_elapsed       |     2964  |\n",
      "|    total_timesteps    |   584000  |\n",
      "| train/                |           |\n",
      "|    loss               |   -2.621  |\n",
      "|    policy_loss        |   -2.566  |\n",
      "|    value_loss         |    0.019  |\n",
      "|    explained_variance |   -2.165  |\n",
      "|    n_updates          |    14600  |\n",
      "| aux_train/            |           |\n",
      "|    aux_confidence_mse |    0.003  |\n",
      "| rnd_net_dist/         |           |\n",
      "|    mean_rnd_bonus     |    0.000  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        |   40.000  |\n",
      "|    ep_rew_mean        |    0.280  |\n",
      "|    ep_rew_std         |    0.449  |\n",
      "|    policy_entropy     |    0.664  |\n",
      "|    advantage_mean     |   -0.111  |\n",
      "|    advantage_std      |    0.082  |\n",
      "|    aux_loss_mean      |    0.005  |\n",
      "| time/                 |           |\n",
      "|    fps                |      196  |\n",
      "|    episodes           |    14700  |\n",
      "|    time_elapsed       |     2988  |\n",
      "|    total_timesteps    |   588000  |\n",
      "| train/                |           |\n",
      "|    loss               |   -2.742  |\n",
      "|    policy_loss        |   -2.685  |\n",
      "|    value_loss         |    0.019  |\n",
      "|    explained_variance |   -1.668  |\n",
      "|    n_updates          |    14700  |\n",
      "| aux_train/            |           |\n",
      "|    aux_confidence_mse |    0.002  |\n",
      "| rnd_net_dist/         |           |\n",
      "|    mean_rnd_bonus     |    0.000  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        |   40.000  |\n",
      "|    ep_rew_mean        |    0.410  |\n",
      "|    ep_rew_std         |    0.492  |\n",
      "|    policy_entropy     |    0.658  |\n",
      "|    advantage_mean     |    0.240  |\n",
      "|    advantage_std      |    0.172  |\n",
      "|    aux_loss_mean      |    0.004  |\n",
      "| time/                 |           |\n",
      "|    fps                |      196  |\n",
      "|    episodes           |    14800  |\n",
      "|    time_elapsed       |     3012  |\n",
      "|    total_timesteps    |   592000  |\n",
      "| train/                |           |\n",
      "|    loss               |    6.717  |\n",
      "|    policy_loss        |    6.739  |\n",
      "|    value_loss         |    0.087  |\n",
      "|    explained_variance |    0.261  |\n",
      "|    n_updates          |    14800  |\n",
      "| aux_train/            |           |\n",
      "|    aux_confidence_mse |    0.001  |\n",
      "| rnd_net_dist/         |           |\n",
      "|    mean_rnd_bonus     |    0.000  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        |   40.000  |\n",
      "|    ep_rew_mean        |    0.360  |\n",
      "|    ep_rew_std         |    0.480  |\n",
      "|    policy_entropy     |    0.633  |\n",
      "|    advantage_mean     |    0.251  |\n",
      "|    advantage_std      |    0.189  |\n",
      "|    aux_loss_mean      |    0.004  |\n",
      "| time/                 |           |\n",
      "|    fps                |      196  |\n",
      "|    episodes           |    14900  |\n",
      "|    time_elapsed       |     3035  |\n",
      "|    total_timesteps    |   596000  |\n",
      "| train/                |           |\n",
      "|    loss               |    6.867  |\n",
      "|    policy_loss        |    6.881  |\n",
      "|    value_loss         |    0.098  |\n",
      "|    explained_variance |    0.136  |\n",
      "|    n_updates          |    14900  |\n",
      "| aux_train/            |           |\n",
      "|    aux_confidence_mse |    0.004  |\n",
      "| rnd_net_dist/         |           |\n",
      "|    mean_rnd_bonus     |    0.000  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        |   40.000  |\n",
      "|    ep_rew_mean        |    0.280  |\n",
      "|    ep_rew_std         |    0.449  |\n",
      "|    policy_entropy     |    0.658  |\n",
      "|    advantage_mean     |   -0.114  |\n",
      "|    advantage_std      |    0.079  |\n",
      "|    aux_loss_mean      |    0.003  |\n",
      "| time/                 |           |\n",
      "|    fps                |      196  |\n",
      "|    episodes           |    15000  |\n",
      "|    time_elapsed       |     3057  |\n",
      "|    total_timesteps    |   600000  |\n",
      "| train/                |           |\n",
      "|    loss               |   -3.214  |\n",
      "|    policy_loss        |   -3.158  |\n",
      "|    value_loss         |    0.019  |\n",
      "|    explained_variance |   -1.452  |\n",
      "|    n_updates          |    15000  |\n",
      "| aux_train/            |           |\n",
      "|    aux_confidence_mse |    0.002  |\n",
      "| rnd_net_dist/         |           |\n",
      "|    mean_rnd_bonus     |    0.000  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        |   40.000  |\n",
      "|    ep_rew_mean        |    0.250  |\n",
      "|    ep_rew_std         |    0.433  |\n",
      "|    policy_entropy     |    0.626  |\n",
      "|    advantage_mean     |   -0.098  |\n",
      "|    advantage_std      |    0.069  |\n",
      "|    aux_loss_mean      |    0.003  |\n",
      "| time/                 |           |\n",
      "|    fps                |      196  |\n",
      "|    episodes           |    15100  |\n",
      "|    time_elapsed       |     3079  |\n",
      "|    total_timesteps    |   604000  |\n",
      "| train/                |           |\n",
      "|    loss               |   -2.751  |\n",
      "|    policy_loss        |   -2.696  |\n",
      "|    value_loss         |    0.014  |\n",
      "|    explained_variance |   -1.488  |\n",
      "|    n_updates          |    15100  |\n",
      "| aux_train/            |           |\n",
      "|    aux_confidence_mse |    0.004  |\n",
      "| rnd_net_dist/         |           |\n",
      "|    mean_rnd_bonus     |    0.000  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        |   40.000  |\n",
      "|    ep_rew_mean        |    0.210  |\n",
      "|    ep_rew_std         |    0.407  |\n",
      "|    policy_entropy     |    0.627  |\n",
      "|    advantage_mean     |   -0.086  |\n",
      "|    advantage_std      |    0.070  |\n",
      "|    aux_loss_mean      |    0.004  |\n",
      "| time/                 |           |\n",
      "|    fps                |      196  |\n",
      "|    episodes           |    15200  |\n",
      "|    time_elapsed       |     3102  |\n",
      "|    total_timesteps    |   608000  |\n",
      "| train/                |           |\n",
      "|    loss               |   -2.363  |\n",
      "|    policy_loss        |   -2.306  |\n",
      "|    value_loss         |    0.012  |\n",
      "|    explained_variance |   -2.347  |\n",
      "|    n_updates          |    15200  |\n",
      "| aux_train/            |           |\n",
      "|    aux_confidence_mse |    0.004  |\n",
      "| rnd_net_dist/         |           |\n",
      "|    mean_rnd_bonus     |    0.000  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        |   40.000  |\n",
      "|    ep_rew_mean        |    0.400  |\n",
      "|    ep_rew_std         |    0.490  |\n",
      "|    policy_entropy     |    0.650  |\n",
      "|    advantage_mean     |   -0.157  |\n",
      "|    advantage_std      |    0.102  |\n",
      "|    aux_loss_mean      |    0.004  |\n",
      "| time/                 |           |\n",
      "|    fps                |      195  |\n",
      "|    episodes           |    15300  |\n",
      "|    time_elapsed       |     3124  |\n",
      "|    total_timesteps    |   612000  |\n",
      "| train/                |           |\n",
      "|    loss               |   -4.118  |\n",
      "|    policy_loss        |   -4.071  |\n",
      "|    value_loss         |    0.035  |\n",
      "|    explained_variance |   -1.330  |\n",
      "|    n_updates          |    15300  |\n",
      "| aux_train/            |           |\n",
      "|    aux_confidence_mse |    0.004  |\n",
      "| rnd_net_dist/         |           |\n",
      "|    mean_rnd_bonus     |    0.000  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        |   40.000  |\n",
      "|    ep_rew_mean        |    0.410  |\n",
      "|    ep_rew_std         |    0.492  |\n",
      "|    policy_entropy     |    0.659  |\n",
      "|    advantage_mean     |    0.236  |\n",
      "|    advantage_std      |    0.183  |\n",
      "|    aux_loss_mean      |    0.004  |\n",
      "| time/                 |           |\n",
      "|    fps                |      195  |\n",
      "|    episodes           |    15400  |\n",
      "|    time_elapsed       |     3146  |\n",
      "|    total_timesteps    |   616000  |\n",
      "| train/                |           |\n",
      "|    loss               |    5.998  |\n",
      "|    policy_loss        |    6.019  |\n",
      "|    value_loss         |    0.088  |\n",
      "|    explained_variance |    0.142  |\n",
      "|    n_updates          |    15400  |\n",
      "| aux_train/            |           |\n",
      "|    aux_confidence_mse |    0.003  |\n",
      "| rnd_net_dist/         |           |\n",
      "|    mean_rnd_bonus     |    0.000  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        |   40.000  |\n",
      "|    ep_rew_mean        |    0.430  |\n",
      "|    ep_rew_std         |    0.495  |\n",
      "|    policy_entropy     |    0.641  |\n",
      "|    advantage_mean     |   -0.163  |\n",
      "|    advantage_std      |    0.134  |\n",
      "|    aux_loss_mean      |    0.004  |\n",
      "| time/                 |           |\n",
      "|    fps                |      195  |\n",
      "|    episodes           |    15500  |\n",
      "|    time_elapsed       |     3168  |\n",
      "|    total_timesteps    |   620000  |\n",
      "| train/                |           |\n",
      "|    loss               |   -4.035  |\n",
      "|    policy_loss        |   -3.994  |\n",
      "|    value_loss         |    0.044  |\n",
      "|    explained_variance |   -2.624  |\n",
      "|    n_updates          |    15500  |\n",
      "| aux_train/            |           |\n",
      "|    aux_confidence_mse |    0.005  |\n",
      "| rnd_net_dist/         |           |\n",
      "|    mean_rnd_bonus     |    0.000  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        |   40.000  |\n",
      "|    ep_rew_mean        |    0.350  |\n",
      "|    ep_rew_std         |    0.477  |\n",
      "|    policy_entropy     |    0.657  |\n",
      "|    advantage_mean     |   -0.140  |\n",
      "|    advantage_std      |    0.098  |\n",
      "|    aux_loss_mean      |    0.004  |\n",
      "| time/                 |           |\n",
      "|    fps                |      195  |\n",
      "|    episodes           |    15600  |\n",
      "|    time_elapsed       |     3191  |\n",
      "|    total_timesteps    |   624000  |\n",
      "| train/                |           |\n",
      "|    loss               |   -3.102  |\n",
      "|    policy_loss        |   -3.051  |\n",
      "|    value_loss         |    0.029  |\n",
      "|    explained_variance |   -1.654  |\n",
      "|    n_updates          |    15600  |\n",
      "| aux_train/            |           |\n",
      "|    aux_confidence_mse |    0.002  |\n",
      "| rnd_net_dist/         |           |\n",
      "|    mean_rnd_bonus     |    0.000  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        |   40.000  |\n",
      "|    ep_rew_mean        |    0.290  |\n",
      "|    ep_rew_std         |    0.454  |\n",
      "|    policy_entropy     |    0.623  |\n",
      "|    advantage_mean     |    0.276  |\n",
      "|    advantage_std      |    0.198  |\n",
      "|    aux_loss_mean      |    0.005  |\n",
      "| time/                 |           |\n",
      "|    fps                |      195  |\n",
      "|    episodes           |    15700  |\n",
      "|    time_elapsed       |     3213  |\n",
      "|    total_timesteps    |   628000  |\n",
      "| train/                |           |\n",
      "|    loss               |    7.449  |\n",
      "|    policy_loss        |    7.453  |\n",
      "|    value_loss         |    0.114  |\n",
      "|    explained_variance |    0.130  |\n",
      "|    n_updates          |    15700  |\n",
      "| aux_train/            |           |\n",
      "|    aux_confidence_mse |    0.005  |\n",
      "| rnd_net_dist/         |           |\n",
      "|    mean_rnd_bonus     |    0.000  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        |   40.000  |\n",
      "|    ep_rew_mean        |    0.350  |\n",
      "|    ep_rew_std         |    0.477  |\n",
      "|    policy_entropy     |    0.638  |\n",
      "|    advantage_mean     |   -0.127  |\n",
      "|    advantage_std      |    0.085  |\n",
      "|    aux_loss_mean      |    0.007  |\n",
      "| time/                 |           |\n",
      "|    fps                |      195  |\n",
      "|    episodes           |    15800  |\n",
      "|    time_elapsed       |     3235  |\n",
      "|    total_timesteps    |   632000  |\n",
      "| train/                |           |\n",
      "|    loss               |   -3.883  |\n",
      "|    policy_loss        |   -3.832  |\n",
      "|    value_loss         |    0.023  |\n",
      "|    explained_variance |   -1.127  |\n",
      "|    n_updates          |    15800  |\n",
      "| aux_train/            |           |\n",
      "|    aux_confidence_mse |    0.010  |\n",
      "| rnd_net_dist/         |           |\n",
      "|    mean_rnd_bonus     |    0.000  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        |   40.000  |\n",
      "|    ep_rew_mean        |    0.360  |\n",
      "|    ep_rew_std         |    0.480  |\n",
      "|    policy_entropy     |    0.626  |\n",
      "|    advantage_mean     |   -0.151  |\n",
      "|    advantage_std      |    0.105  |\n",
      "|    aux_loss_mean      |    0.007  |\n",
      "| time/                 |           |\n",
      "|    fps                |      195  |\n",
      "|    episodes           |    15900  |\n",
      "|    time_elapsed       |     3257  |\n",
      "|    total_timesteps    |   636000  |\n",
      "| train/                |           |\n",
      "|    loss               |   -3.106  |\n",
      "|    policy_loss        |   -3.061  |\n",
      "|    value_loss         |    0.034  |\n",
      "|    explained_variance |   -1.598  |\n",
      "|    n_updates          |    15900  |\n",
      "| aux_train/            |           |\n",
      "|    aux_confidence_mse |    0.008  |\n",
      "| rnd_net_dist/         |           |\n",
      "|    mean_rnd_bonus     |    0.000  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        |   40.000  |\n",
      "|    ep_rew_mean        |    0.320  |\n",
      "|    ep_rew_std         |    0.466  |\n",
      "|    policy_entropy     |    0.602  |\n",
      "|    advantage_mean     |    0.265  |\n",
      "|    advantage_std      |    0.177  |\n",
      "|    aux_loss_mean      |    0.007  |\n",
      "| time/                 |           |\n",
      "|    fps                |      195  |\n",
      "|    episodes           |    16000  |\n",
      "|    time_elapsed       |     3280  |\n",
      "|    total_timesteps    |   640000  |\n",
      "| train/                |           |\n",
      "|    loss               |    5.883  |\n",
      "|    policy_loss        |    5.893  |\n",
      "|    value_loss         |    0.101  |\n",
      "|    explained_variance |    0.297  |\n",
      "|    n_updates          |    16000  |\n",
      "| aux_train/            |           |\n",
      "|    aux_confidence_mse |    0.007  |\n",
      "| rnd_net_dist/         |           |\n",
      "|    mean_rnd_bonus     |    0.000  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        |   40.000  |\n",
      "|    ep_rew_mean        |    0.390  |\n",
      "|    ep_rew_std         |    0.488  |\n",
      "|    policy_entropy     |    0.625  |\n",
      "|    advantage_mean     |    0.242  |\n",
      "|    advantage_std      |    0.186  |\n",
      "|    aux_loss_mean      |    0.006  |\n",
      "| time/                 |           |\n",
      "|    fps                |      195  |\n",
      "|    episodes           |    16100  |\n",
      "|    time_elapsed       |     3302  |\n",
      "|    total_timesteps    |   644000  |\n",
      "| train/                |           |\n",
      "|    loss               |    4.974  |\n",
      "|    policy_loss        |    4.990  |\n",
      "|    value_loss         |    0.092  |\n",
      "|    explained_variance |    0.142  |\n",
      "|    n_updates          |    16100  |\n",
      "| aux_train/            |           |\n",
      "|    aux_confidence_mse |    0.007  |\n",
      "| rnd_net_dist/         |           |\n",
      "|    mean_rnd_bonus     |    0.000  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        |   40.000  |\n",
      "|    ep_rew_mean        |    0.310  |\n",
      "|    ep_rew_std         |    0.462  |\n",
      "|    policy_entropy     |    0.623  |\n",
      "|    advantage_mean     |   -0.122  |\n",
      "|    advantage_std      |    0.090  |\n",
      "|    aux_loss_mean      |    0.007  |\n",
      "| time/                 |           |\n",
      "|    fps                |      194  |\n",
      "|    episodes           |    16200  |\n",
      "|    time_elapsed       |     3324  |\n",
      "|    total_timesteps    |   648000  |\n",
      "| train/                |           |\n",
      "|    loss               |   -2.772  |\n",
      "|    policy_loss        |   -2.721  |\n",
      "|    value_loss         |    0.023  |\n",
      "|    explained_variance |   -1.808  |\n",
      "|    n_updates          |    16200  |\n",
      "| aux_train/            |           |\n",
      "|    aux_confidence_mse |    0.003  |\n",
      "| rnd_net_dist/         |           |\n",
      "|    mean_rnd_bonus     |    0.000  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        |   40.000  |\n",
      "|    ep_rew_mean        |    0.340  |\n",
      "|    ep_rew_std         |    0.474  |\n",
      "|    policy_entropy     |    0.619  |\n",
      "|    advantage_mean     |   -0.135  |\n",
      "|    advantage_std      |    0.090  |\n",
      "|    aux_loss_mean      |    0.006  |\n",
      "| time/                 |           |\n",
      "|    fps                |      194  |\n",
      "|    episodes           |    16300  |\n",
      "|    time_elapsed       |     3347  |\n",
      "|    total_timesteps    |   652000  |\n",
      "| train/                |           |\n",
      "|    loss               |   -3.542  |\n",
      "|    policy_loss        |   -3.494  |\n",
      "|    value_loss         |    0.026  |\n",
      "|    explained_variance |   -1.006  |\n",
      "|    n_updates          |    16300  |\n",
      "| aux_train/            |           |\n",
      "|    aux_confidence_mse |    0.006  |\n",
      "| rnd_net_dist/         |           |\n",
      "|    mean_rnd_bonus     |    0.000  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        |   40.000  |\n",
      "|    ep_rew_mean        |    0.320  |\n",
      "|    ep_rew_std         |    0.466  |\n",
      "|    policy_entropy     |    0.624  |\n",
      "|    advantage_mean     |   -0.130  |\n",
      "|    advantage_std      |    0.085  |\n",
      "|    aux_loss_mean      |    0.008  |\n",
      "| time/                 |           |\n",
      "|    fps                |      194  |\n",
      "|    episodes           |    16400  |\n",
      "|    time_elapsed       |     3370  |\n",
      "|    total_timesteps    |   656000  |\n",
      "| train/                |           |\n",
      "|    loss               |   -2.870  |\n",
      "|    policy_loss        |   -2.821  |\n",
      "|    value_loss         |    0.024  |\n",
      "|    explained_variance |   -1.104  |\n",
      "|    n_updates          |    16400  |\n",
      "| aux_train/            |           |\n",
      "|    aux_confidence_mse |    0.009  |\n",
      "| rnd_net_dist/         |           |\n",
      "|    mean_rnd_bonus     |    0.000  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        |   40.000  |\n",
      "|    ep_rew_mean        |    0.340  |\n",
      "|    ep_rew_std         |    0.474  |\n",
      "|    policy_entropy     |    0.540  |\n",
      "|    advantage_mean     |    0.260  |\n",
      "|    advantage_std      |    0.193  |\n",
      "|    aux_loss_mean      |    0.015  |\n",
      "| time/                 |           |\n",
      "|    fps                |      194  |\n",
      "|    episodes           |    16500  |\n",
      "|    time_elapsed       |     3392  |\n",
      "|    total_timesteps    |   660000  |\n",
      "| train/                |           |\n",
      "|    loss               |    3.943  |\n",
      "|    policy_loss        |    3.943  |\n",
      "|    value_loss         |    0.104  |\n",
      "|    explained_variance |    0.103  |\n",
      "|    n_updates          |    16500  |\n",
      "| aux_train/            |           |\n",
      "|    aux_confidence_mse |    0.024  |\n",
      "| rnd_net_dist/         |           |\n",
      "|    mean_rnd_bonus     |    0.000  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        |   40.000  |\n",
      "|    ep_rew_mean        |    0.320  |\n",
      "|    ep_rew_std         |    0.466  |\n",
      "|    policy_entropy     |    0.552  |\n",
      "|    advantage_mean     |    0.263  |\n",
      "|    advantage_std      |    0.186  |\n",
      "|    aux_loss_mean      |    0.018  |\n",
      "| time/                 |           |\n",
      "|    fps                |      194  |\n",
      "|    episodes           |    16600  |\n",
      "|    time_elapsed       |     3415  |\n",
      "|    total_timesteps    |   664000  |\n",
      "| train/                |           |\n",
      "|    loss               |    5.530  |\n",
      "|    policy_loss        |    5.532  |\n",
      "|    value_loss         |    0.103  |\n",
      "|    explained_variance |    0.227  |\n",
      "|    n_updates          |    16600  |\n",
      "| aux_train/            |           |\n",
      "|    aux_confidence_mse |    0.021  |\n",
      "| rnd_net_dist/         |           |\n",
      "|    mean_rnd_bonus     |    0.000  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        |   40.000  |\n",
      "|    ep_rew_mean        |    0.290  |\n",
      "|    ep_rew_std         |    0.454  |\n",
      "|    policy_entropy     |    0.597  |\n",
      "|    advantage_mean     |   -0.115  |\n",
      "|    advantage_std      |    0.074  |\n",
      "|    aux_loss_mean      |    0.020  |\n",
      "| time/                 |           |\n",
      "|    fps                |      194  |\n",
      "|    episodes           |    16700  |\n",
      "|    time_elapsed       |     3437  |\n",
      "|    total_timesteps    |   668000  |\n",
      "| train/                |           |\n",
      "|    loss               |   -3.458  |\n",
      "|    policy_loss        |   -3.410  |\n",
      "|    value_loss         |    0.019  |\n",
      "|    explained_variance |   -1.037  |\n",
      "|    n_updates          |    16700  |\n",
      "| aux_train/            |           |\n",
      "|    aux_confidence_mse |    0.015  |\n",
      "| rnd_net_dist/         |           |\n",
      "|    mean_rnd_bonus     |    0.000  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        |   40.000  |\n",
      "|    ep_rew_mean        |    0.280  |\n",
      "|    ep_rew_std         |    0.449  |\n",
      "|    policy_entropy     |    0.560  |\n",
      "|    advantage_mean     |   -0.110  |\n",
      "|    advantage_std      |    0.085  |\n",
      "|    aux_loss_mean      |    0.022  |\n",
      "| time/                 |           |\n",
      "|    fps                |      194  |\n",
      "|    episodes           |    16800  |\n",
      "|    time_elapsed       |     3460  |\n",
      "|    total_timesteps    |   672000  |\n",
      "| train/                |           |\n",
      "|    loss               |   -1.838  |\n",
      "|    policy_loss        |   -1.793  |\n",
      "|    value_loss         |    0.019  |\n",
      "|    explained_variance |   -1.774  |\n",
      "|    n_updates          |    16800  |\n",
      "| aux_train/            |           |\n",
      "|    aux_confidence_mse |    0.014  |\n",
      "| rnd_net_dist/         |           |\n",
      "|    mean_rnd_bonus     |    0.000  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        |   40.000  |\n",
      "|    ep_rew_mean        |    0.330  |\n",
      "|    ep_rew_std         |    0.470  |\n",
      "|    policy_entropy     |    0.475  |\n",
      "|    advantage_mean     |   -0.123  |\n",
      "|    advantage_std      |    0.075  |\n",
      "|    aux_loss_mean      |    0.022  |\n",
      "| time/                 |           |\n",
      "|    fps                |      194  |\n",
      "|    episodes           |    16900  |\n",
      "|    time_elapsed       |     3482  |\n",
      "|    total_timesteps    |   676000  |\n",
      "| train/                |           |\n",
      "|    loss               |   -3.411  |\n",
      "|    policy_loss        |   -3.378  |\n",
      "|    value_loss         |    0.021  |\n",
      "|    explained_variance |   -0.733  |\n",
      "|    n_updates          |    16900  |\n",
      "| aux_train/            |           |\n",
      "|    aux_confidence_mse |    0.034  |\n",
      "| rnd_net_dist/         |           |\n",
      "|    mean_rnd_bonus     |    0.000  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        |   40.000  |\n",
      "|    ep_rew_mean        |    0.310  |\n",
      "|    ep_rew_std         |    0.462  |\n",
      "|    policy_entropy     |    0.501  |\n",
      "|    advantage_mean     |    0.272  |\n",
      "|    advantage_std      |    0.188  |\n",
      "|    aux_loss_mean      |    0.024  |\n",
      "| time/                 |           |\n",
      "|    fps                |      194  |\n",
      "|    episodes           |    17000  |\n",
      "|    time_elapsed       |     3504  |\n",
      "|    total_timesteps    |   680000  |\n",
      "| train/                |           |\n",
      "|    loss               |    4.938  |\n",
      "|    policy_loss        |    4.931  |\n",
      "|    value_loss         |    0.109  |\n",
      "|    explained_variance |    0.214  |\n",
      "|    n_updates          |    17000  |\n",
      "| aux_train/            |           |\n",
      "|    aux_confidence_mse |    0.032  |\n",
      "| rnd_net_dist/         |           |\n",
      "|    mean_rnd_bonus     |    0.000  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        |   40.000  |\n",
      "|    ep_rew_mean        |    0.390  |\n",
      "|    ep_rew_std         |    0.488  |\n",
      "|    policy_entropy     |    0.460  |\n",
      "|    advantage_mean     |    0.232  |\n",
      "|    advantage_std      |    0.174  |\n",
      "|    aux_loss_mean      |    0.023  |\n",
      "| time/                 |           |\n",
      "|    fps                |      193  |\n",
      "|    episodes           |    17100  |\n",
      "|    time_elapsed       |     3527  |\n",
      "|    total_timesteps    |   684000  |\n",
      "| train/                |           |\n",
      "|    loss               |    3.330  |\n",
      "|    policy_loss        |    3.331  |\n",
      "|    value_loss         |    0.084  |\n",
      "|    explained_variance |    0.223  |\n",
      "|    n_updates          |    17100  |\n",
      "| aux_train/            |           |\n",
      "|    aux_confidence_mse |    0.030  |\n",
      "| rnd_net_dist/         |           |\n",
      "|    mean_rnd_bonus     |    0.000  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        |   40.000  |\n",
      "|    ep_rew_mean        |    0.330  |\n",
      "|    ep_rew_std         |    0.470  |\n",
      "|    policy_entropy     |    0.505  |\n",
      "|    advantage_mean     |   -0.135  |\n",
      "|    advantage_std      |    0.096  |\n",
      "|    aux_loss_mean      |    0.023  |\n",
      "| time/                 |           |\n",
      "|    fps                |      193  |\n",
      "|    episodes           |    17200  |\n",
      "|    time_elapsed       |     3549  |\n",
      "|    total_timesteps    |   688000  |\n",
      "| train/                |           |\n",
      "|    loss               |   -2.401  |\n",
      "|    policy_loss        |   -2.366  |\n",
      "|    value_loss         |    0.027  |\n",
      "|    explained_variance |   -1.556  |\n",
      "|    n_updates          |    17200  |\n",
      "| aux_train/            |           |\n",
      "|    aux_confidence_mse |    0.024  |\n",
      "| rnd_net_dist/         |           |\n",
      "|    mean_rnd_bonus     |    0.000  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        |   40.000  |\n",
      "|    ep_rew_mean        |    0.340  |\n",
      "|    ep_rew_std         |    0.474  |\n",
      "|    policy_entropy     |    0.523  |\n",
      "|    advantage_mean     |    0.264  |\n",
      "|    advantage_std      |    0.186  |\n",
      "|    aux_loss_mean      |    0.027  |\n",
      "| time/                 |           |\n",
      "|    fps                |      193  |\n",
      "|    episodes           |    17300  |\n",
      "|    time_elapsed       |     3571  |\n",
      "|    total_timesteps    |   692000  |\n",
      "| train/                |           |\n",
      "|    loss               |    8.332  |\n",
      "|    policy_loss        |    8.331  |\n",
      "|    value_loss         |    0.103  |\n",
      "|    explained_variance |    0.209  |\n",
      "|    n_updates          |    17300  |\n",
      "| aux_train/            |           |\n",
      "|    aux_confidence_mse |    0.020  |\n",
      "| rnd_net_dist/         |           |\n",
      "|    mean_rnd_bonus     |    0.000  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        |   40.000  |\n",
      "|    ep_rew_mean        |    0.360  |\n",
      "|    ep_rew_std         |    0.480  |\n",
      "|    policy_entropy     |    0.400  |\n",
      "|    advantage_mean     |    0.243  |\n",
      "|    advantage_std      |    0.191  |\n",
      "|    aux_loss_mean      |    0.038  |\n",
      "| time/                 |           |\n",
      "|    fps                |      193  |\n",
      "|    episodes           |    17400  |\n",
      "|    time_elapsed       |     3594  |\n",
      "|    total_timesteps    |   696000  |\n",
      "| train/                |           |\n",
      "|    loss               |    2.419  |\n",
      "|    policy_loss        |    2.406  |\n",
      "|    value_loss         |    0.095  |\n",
      "|    explained_variance |    0.046  |\n",
      "|    n_updates          |    17400  |\n",
      "| aux_train/            |           |\n",
      "|    aux_confidence_mse |    0.052  |\n",
      "| rnd_net_dist/         |           |\n",
      "|    mean_rnd_bonus     |    0.000  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        |   40.000  |\n",
      "|    ep_rew_mean        |    0.370  |\n",
      "|    ep_rew_std         |    0.483  |\n",
      "|    policy_entropy     |    0.482  |\n",
      "|    advantage_mean     |   -0.148  |\n",
      "|    advantage_std      |    0.090  |\n",
      "|    aux_loss_mean      |    0.037  |\n",
      "| time/                 |           |\n",
      "|    fps                |      193  |\n",
      "|    episodes           |    17500  |\n",
      "|    time_elapsed       |     3617  |\n",
      "|    total_timesteps    |   700000  |\n",
      "| train/                |           |\n",
      "|    loss               |   -3.766  |\n",
      "|    policy_loss        |   -3.736  |\n",
      "|    value_loss         |    0.030  |\n",
      "|    explained_variance |   -0.823  |\n",
      "|    n_updates          |    17500  |\n",
      "| aux_train/            |           |\n",
      "|    aux_confidence_mse |    0.029  |\n",
      "| rnd_net_dist/         |           |\n",
      "|    mean_rnd_bonus     |    0.000  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        |   40.000  |\n",
      "|    ep_rew_mean        |    0.300  |\n",
      "|    ep_rew_std         |    0.458  |\n",
      "|    policy_entropy     |    0.442  |\n",
      "|    advantage_mean     |    0.243  |\n",
      "|    advantage_std      |    0.191  |\n",
      "|    aux_loss_mean      |    0.033  |\n",
      "| time/                 |           |\n",
      "|    fps                |      193  |\n",
      "|    episodes           |    17600  |\n",
      "|    time_elapsed       |     3641  |\n",
      "|    total_timesteps    |   704000  |\n",
      "| train/                |           |\n",
      "|    loss               |    4.405  |\n",
      "|    policy_loss        |    4.397  |\n",
      "|    value_loss         |    0.094  |\n",
      "|    explained_variance |    0.083  |\n",
      "|    n_updates          |    17600  |\n",
      "| aux_train/            |           |\n",
      "|    aux_confidence_mse |    0.044  |\n",
      "| rnd_net_dist/         |           |\n",
      "|    mean_rnd_bonus     |    0.000  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        |   40.000  |\n",
      "|    ep_rew_mean        |    0.330  |\n",
      "|    ep_rew_std         |    0.470  |\n",
      "|    policy_entropy     |    0.505  |\n",
      "|    advantage_mean     |   -0.132  |\n",
      "|    advantage_std      |    0.094  |\n",
      "|    aux_loss_mean      |    0.032  |\n",
      "| time/                 |           |\n",
      "|    fps                |      193  |\n",
      "|    episodes           |    17700  |\n",
      "|    time_elapsed       |     3664  |\n",
      "|    total_timesteps    |   708000  |\n",
      "| train/                |           |\n",
      "|    loss               |   -2.052  |\n",
      "|    policy_loss        |   -2.017  |\n",
      "|    value_loss         |    0.026  |\n",
      "|    explained_variance |   -1.693  |\n",
      "|    n_updates          |    17700  |\n",
      "| aux_train/            |           |\n",
      "|    aux_confidence_mse |    0.020  |\n",
      "| rnd_net_dist/         |           |\n",
      "|    mean_rnd_bonus     |    0.000  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        |   40.000  |\n",
      "|    ep_rew_mean        |    0.400  |\n",
      "|    ep_rew_std         |    0.490  |\n",
      "|    policy_entropy     |    0.522  |\n",
      "|    advantage_mean     |   -0.147  |\n",
      "|    advantage_std      |    0.115  |\n",
      "|    aux_loss_mean      |    0.035  |\n",
      "| time/                 |           |\n",
      "|    fps                |      193  |\n",
      "|    episodes           |    17800  |\n",
      "|    time_elapsed       |     3688  |\n",
      "|    total_timesteps    |   712000  |\n",
      "| train/                |           |\n",
      "|    loss               |   -2.285  |\n",
      "|    policy_loss        |   -2.252  |\n",
      "|    value_loss         |    0.035  |\n",
      "|    explained_variance |   -1.888  |\n",
      "|    n_updates          |    17800  |\n",
      "| aux_train/            |           |\n",
      "|    aux_confidence_mse |    0.018  |\n",
      "| rnd_net_dist/         |           |\n",
      "|    mean_rnd_bonus     |    0.000  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        |   40.000  |\n",
      "|    ep_rew_mean        |    0.420  |\n",
      "|    ep_rew_std         |    0.494  |\n",
      "|    policy_entropy     |    0.471  |\n",
      "|    advantage_mean     |   -0.161  |\n",
      "|    advantage_std      |    0.104  |\n",
      "|    aux_loss_mean      |    0.028  |\n",
      "| time/                 |           |\n",
      "|    fps                |      192  |\n",
      "|    episodes           |    17900  |\n",
      "|    time_elapsed       |     3711  |\n",
      "|    total_timesteps    |   716000  |\n",
      "| train/                |           |\n",
      "|    loss               |   -2.593  |\n",
      "|    policy_loss        |   -2.567  |\n",
      "|    value_loss         |    0.037  |\n",
      "|    explained_variance |   -1.170  |\n",
      "|    n_updates          |    17900  |\n",
      "| aux_train/            |           |\n",
      "|    aux_confidence_mse |    0.026  |\n",
      "| rnd_net_dist/         |           |\n",
      "|    mean_rnd_bonus     |    0.000  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        |   40.000  |\n",
      "|    ep_rew_mean        |    0.350  |\n",
      "|    ep_rew_std         |    0.477  |\n",
      "|    policy_entropy     |    0.459  |\n",
      "|    advantage_mean     |   -0.132  |\n",
      "|    advantage_std      |    0.082  |\n",
      "|    aux_loss_mean      |    0.028  |\n",
      "| time/                 |           |\n",
      "|    fps                |      192  |\n",
      "|    episodes           |    18000  |\n",
      "|    time_elapsed       |     3735  |\n",
      "|    total_timesteps    |   720000  |\n",
      "| train/                |           |\n",
      "|    loss               |   -2.894  |\n",
      "|    policy_loss        |   -2.863  |\n",
      "|    value_loss         |    0.024  |\n",
      "|    explained_variance |   -0.847  |\n",
      "|    n_updates          |    18000  |\n",
      "| aux_train/            |           |\n",
      "|    aux_confidence_mse |    0.029  |\n",
      "| rnd_net_dist/         |           |\n",
      "|    mean_rnd_bonus     |    0.000  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        |   40.000  |\n",
      "|    ep_rew_mean        |    0.430  |\n",
      "|    ep_rew_std         |    0.495  |\n",
      "|    policy_entropy     |    0.474  |\n",
      "|    advantage_mean     |   -0.153  |\n",
      "|    advantage_std      |    0.094  |\n",
      "|    aux_loss_mean      |    0.029  |\n",
      "| time/                 |           |\n",
      "|    fps                |      192  |\n",
      "|    episodes           |    18100  |\n",
      "|    time_elapsed       |     3758  |\n",
      "|    total_timesteps    |   724000  |\n",
      "| train/                |           |\n",
      "|    loss               |   -3.422  |\n",
      "|    policy_loss        |   -3.394  |\n",
      "|    value_loss         |    0.032  |\n",
      "|    explained_variance |   -0.895  |\n",
      "|    n_updates          |    18100  |\n",
      "| aux_train/            |           |\n",
      "|    aux_confidence_mse |    0.030  |\n",
      "| rnd_net_dist/         |           |\n",
      "|    mean_rnd_bonus     |    0.000  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        |   40.000  |\n",
      "|    ep_rew_mean        |    0.420  |\n",
      "|    ep_rew_std         |    0.494  |\n",
      "|    policy_entropy     |    0.409  |\n",
      "|    advantage_mean     |    0.229  |\n",
      "|    advantage_std      |    0.178  |\n",
      "|    aux_loss_mean      |    0.031  |\n",
      "| time/                 |           |\n",
      "|    fps                |      192  |\n",
      "|    episodes           |    18200  |\n",
      "|    time_elapsed       |     3782  |\n",
      "|    total_timesteps    |   728000  |\n",
      "| train/                |           |\n",
      "|    loss               |    1.668  |\n",
      "|    policy_loss        |    1.663  |\n",
      "|    value_loss         |    0.083  |\n",
      "|    explained_variance |    0.150  |\n",
      "|    n_updates          |    18200  |\n",
      "| aux_train/            |           |\n",
      "|    aux_confidence_mse |    0.038  |\n",
      "| rnd_net_dist/         |           |\n",
      "|    mean_rnd_bonus     |    0.000  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        |   40.000  |\n",
      "|    ep_rew_mean        |    0.320  |\n",
      "|    ep_rew_std         |    0.466  |\n",
      "|    policy_entropy     |    0.445  |\n",
      "|    advantage_mean     |    0.255  |\n",
      "|    advantage_std      |    0.188  |\n",
      "|    aux_loss_mean      |    0.036  |\n",
      "| time/                 |           |\n",
      "|    fps                |      192  |\n",
      "|    episodes           |    18300  |\n",
      "|    time_elapsed       |     3805  |\n",
      "|    total_timesteps    |   732000  |\n",
      "| train/                |           |\n",
      "|    loss               |    2.555  |\n",
      "|    policy_loss        |    2.547  |\n",
      "|    value_loss         |    0.099  |\n",
      "|    explained_variance |    0.163  |\n",
      "|    n_updates          |    18300  |\n",
      "| aux_train/            |           |\n",
      "|    aux_confidence_mse |    0.028  |\n",
      "| rnd_net_dist/         |           |\n",
      "|    mean_rnd_bonus     |    0.000  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        |   40.000  |\n",
      "|    ep_rew_mean        |    0.400  |\n",
      "|    ep_rew_std         |    0.490  |\n",
      "|    policy_entropy     |    0.438  |\n",
      "|    advantage_mean     |   -0.137  |\n",
      "|    advantage_std      |    0.092  |\n",
      "|    aux_loss_mean      |    0.033  |\n",
      "| time/                 |           |\n",
      "|    fps                |      192  |\n",
      "|    episodes           |    18400  |\n",
      "|    time_elapsed       |     3828  |\n",
      "|    total_timesteps    |   736000  |\n",
      "| train/                |           |\n",
      "|    loss               |   -2.623  |\n",
      "|    policy_loss        |   -2.595  |\n",
      "|    value_loss         |    0.027  |\n",
      "|    explained_variance |   -1.401  |\n",
      "|    n_updates          |    18400  |\n",
      "| aux_train/            |           |\n",
      "|    aux_confidence_mse |    0.026  |\n",
      "| rnd_net_dist/         |           |\n",
      "|    mean_rnd_bonus     |    0.000  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        |   40.000  |\n",
      "|    ep_rew_mean        |    0.410  |\n",
      "|    ep_rew_std         |    0.492  |\n",
      "|    policy_entropy     |    0.516  |\n",
      "|    advantage_mean     |    0.229  |\n",
      "|    advantage_std      |    0.166  |\n",
      "|    aux_loss_mean      |    0.026  |\n",
      "| time/                 |           |\n",
      "|    fps                |      192  |\n",
      "|    episodes           |    18500  |\n",
      "|    time_elapsed       |     3852  |\n",
      "|    total_timesteps    |   740000  |\n",
      "| train/                |           |\n",
      "|    loss               |    3.652  |\n",
      "|    policy_loss        |    3.662  |\n",
      "|    value_loss         |    0.079  |\n",
      "|    explained_variance |    0.262  |\n",
      "|    n_updates          |    18500  |\n",
      "| aux_train/            |           |\n",
      "|    aux_confidence_mse |    0.023  |\n",
      "| rnd_net_dist/         |           |\n",
      "|    mean_rnd_bonus     |    0.000  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        |   40.000  |\n",
      "|    ep_rew_mean        |    0.380  |\n",
      "|    ep_rew_std         |    0.485  |\n",
      "|    policy_entropy     |    0.445  |\n",
      "|    advantage_mean     |    0.227  |\n",
      "|    advantage_std      |    0.181  |\n",
      "|    aux_loss_mean      |    0.025  |\n",
      "| time/                 |           |\n",
      "|    fps                |      191  |\n",
      "|    episodes           |    18600  |\n",
      "|    time_elapsed       |     3875  |\n",
      "|    total_timesteps    |   744000  |\n",
      "| train/                |           |\n",
      "|    loss               |    3.876  |\n",
      "|    policy_loss        |    3.876  |\n",
      "|    value_loss         |    0.083  |\n",
      "|    explained_variance |    0.126  |\n",
      "|    n_updates          |    18600  |\n",
      "| aux_train/            |           |\n",
      "|    aux_confidence_mse |    0.030  |\n",
      "| rnd_net_dist/         |           |\n",
      "|    mean_rnd_bonus     |    0.000  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        |   40.000  |\n",
      "|    ep_rew_mean        |    0.390  |\n",
      "|    ep_rew_std         |    0.488  |\n",
      "|    policy_entropy     |    0.463  |\n",
      "|    advantage_mean     |    0.235  |\n",
      "|    advantage_std      |    0.167  |\n",
      "|    aux_loss_mean      |    0.023  |\n",
      "| time/                 |           |\n",
      "|    fps                |      191  |\n",
      "|    episodes           |    18700  |\n",
      "|    time_elapsed       |     3899  |\n",
      "|    total_timesteps    |   748000  |\n",
      "| train/                |           |\n",
      "|    loss               |    2.822  |\n",
      "|    policy_loss        |    2.824  |\n",
      "|    value_loss         |    0.083  |\n",
      "|    explained_variance |    0.287  |\n",
      "|    n_updates          |    18700  |\n",
      "| aux_train/            |           |\n",
      "|    aux_confidence_mse |    0.031  |\n",
      "| rnd_net_dist/         |           |\n",
      "|    mean_rnd_bonus     |    0.000  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        |   40.000  |\n",
      "|    ep_rew_mean        |    0.320  |\n",
      "|    ep_rew_std         |    0.466  |\n",
      "|    policy_entropy     |    0.562  |\n",
      "|    advantage_mean     |   -0.134  |\n",
      "|    advantage_std      |    0.089  |\n",
      "|    aux_loss_mean      |    0.027  |\n",
      "| time/                 |           |\n",
      "|    fps                |      191  |\n",
      "|    episodes           |    18800  |\n",
      "|    time_elapsed       |     3922  |\n",
      "|    total_timesteps    |   752000  |\n",
      "| train/                |           |\n",
      "|    loss               |   -2.375  |\n",
      "|    policy_loss        |   -2.333  |\n",
      "|    value_loss         |    0.026  |\n",
      "|    explained_variance |   -1.173  |\n",
      "|    n_updates          |    18800  |\n",
      "| aux_train/            |           |\n",
      "|    aux_confidence_mse |    0.018  |\n",
      "| rnd_net_dist/         |           |\n",
      "|    mean_rnd_bonus     |    0.000  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        |   40.000  |\n",
      "|    ep_rew_mean        |    0.350  |\n",
      "|    ep_rew_std         |    0.477  |\n",
      "|    policy_entropy     |    0.398  |\n",
      "|    advantage_mean     |   -0.135  |\n",
      "|    advantage_std      |    0.082  |\n",
      "|    aux_loss_mean      |    0.031  |\n",
      "| time/                 |           |\n",
      "|    fps                |      191  |\n",
      "|    episodes           |    18900  |\n",
      "|    time_elapsed       |     3945  |\n",
      "|    total_timesteps    |   756000  |\n",
      "| train/                |           |\n",
      "|    loss               |   -1.620  |\n",
      "|    policy_loss        |   -1.597  |\n",
      "|    value_loss         |    0.025  |\n",
      "|    explained_variance |   -0.924  |\n",
      "|    n_updates          |    18900  |\n",
      "| aux_train/            |           |\n",
      "|    aux_confidence_mse |    0.043  |\n",
      "| rnd_net_dist/         |           |\n",
      "|    mean_rnd_bonus     |    0.000  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        |   40.000  |\n",
      "|    ep_rew_mean        |    0.310  |\n",
      "|    ep_rew_std         |    0.462  |\n",
      "|    policy_entropy     |    0.418  |\n",
      "|    advantage_mean     |   -0.117  |\n",
      "|    advantage_std      |    0.083  |\n",
      "|    aux_loss_mean      |    0.031  |\n",
      "| time/                 |           |\n",
      "|    fps                |      191  |\n",
      "|    episodes           |    19000  |\n",
      "|    time_elapsed       |     3969  |\n",
      "|    total_timesteps    |   760000  |\n",
      "| train/                |           |\n",
      "|    loss               |   -1.401  |\n",
      "|    policy_loss        |   -1.373  |\n",
      "|    value_loss         |    0.020  |\n",
      "|    explained_variance |   -1.494  |\n",
      "|    n_updates          |    19000  |\n",
      "| aux_train/            |           |\n",
      "|    aux_confidence_mse |    0.036  |\n",
      "| rnd_net_dist/         |           |\n",
      "|    mean_rnd_bonus     |    0.000  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        |   40.000  |\n",
      "|    ep_rew_mean        |    0.370  |\n",
      "|    ep_rew_std         |    0.483  |\n",
      "|    policy_entropy     |    0.483  |\n",
      "|    advantage_mean     |   -0.144  |\n",
      "|    advantage_std      |    0.091  |\n",
      "|    aux_loss_mean      |    0.027  |\n",
      "| time/                 |           |\n",
      "|    fps                |      191  |\n",
      "|    episodes           |    19100  |\n",
      "|    time_elapsed       |     3993  |\n",
      "|    total_timesteps    |   764000  |\n",
      "| train/                |           |\n",
      "|    loss               |   -2.718  |\n",
      "|    policy_loss        |   -2.686  |\n",
      "|    value_loss         |    0.029  |\n",
      "|    explained_variance |   -1.043  |\n",
      "|    n_updates          |    19100  |\n",
      "| aux_train/            |           |\n",
      "|    aux_confidence_mse |    0.021  |\n",
      "| rnd_net_dist/         |           |\n",
      "|    mean_rnd_bonus     |    0.000  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        |   40.000  |\n",
      "|    ep_rew_mean        |    0.260  |\n",
      "|    ep_rew_std         |    0.439  |\n",
      "|    policy_entropy     |    0.530  |\n",
      "|    advantage_mean     |    0.280  |\n",
      "|    advantage_std      |    0.203  |\n",
      "|    aux_loss_mean      |    0.024  |\n",
      "| time/                 |           |\n",
      "|    fps                |      191  |\n",
      "|    episodes           |    19200  |\n",
      "|    time_elapsed       |     4016  |\n",
      "|    total_timesteps    |   768000  |\n",
      "| train/                |           |\n",
      "|    loss               |    7.857  |\n",
      "|    policy_loss        |    7.849  |\n",
      "|    value_loss         |    0.119  |\n",
      "|    explained_variance |    0.108  |\n",
      "|    n_updates          |    19200  |\n",
      "| aux_train/            |           |\n",
      "|    aux_confidence_mse |    0.014  |\n",
      "| rnd_net_dist/         |           |\n",
      "|    mean_rnd_bonus     |    0.000  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        |   40.000  |\n",
      "|    ep_rew_mean        |    0.390  |\n",
      "|    ep_rew_std         |    0.488  |\n",
      "|    policy_entropy     |    0.470  |\n",
      "|    advantage_mean     |   -0.149  |\n",
      "|    advantage_std      |    0.104  |\n",
      "|    aux_loss_mean      |    0.027  |\n",
      "| time/                 |           |\n",
      "|    fps                |      191  |\n",
      "|    episodes           |    19300  |\n",
      "|    time_elapsed       |     4039  |\n",
      "|    total_timesteps    |   772000  |\n",
      "| train/                |           |\n",
      "|    loss               |   -1.752  |\n",
      "|    policy_loss        |   -1.725  |\n",
      "|    value_loss         |    0.033  |\n",
      "|    explained_variance |   -1.791  |\n",
      "|    n_updates          |    19300  |\n",
      "| aux_train/            |           |\n",
      "|    aux_confidence_mse |    0.030  |\n",
      "| rnd_net_dist/         |           |\n",
      "|    mean_rnd_bonus     |    0.000  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        |   40.000  |\n",
      "|    ep_rew_mean        |    0.340  |\n",
      "|    ep_rew_std         |    0.474  |\n",
      "|    policy_entropy     |    0.544  |\n",
      "|    advantage_mean     |    0.257  |\n",
      "|    advantage_std      |    0.171  |\n",
      "|    aux_loss_mean      |    0.027  |\n",
      "| time/                 |           |\n",
      "|    fps                |      191  |\n",
      "|    episodes           |    19400  |\n",
      "|    time_elapsed       |     4062  |\n",
      "|    total_timesteps    |   776000  |\n",
      "| train/                |           |\n",
      "|    loss               |    6.930  |\n",
      "|    policy_loss        |    6.936  |\n",
      "|    value_loss         |    0.094  |\n",
      "|    explained_variance |    0.310  |\n",
      "|    n_updates          |    19400  |\n",
      "| aux_train/            |           |\n",
      "|    aux_confidence_mse |    0.019  |\n",
      "| rnd_net_dist/         |           |\n",
      "|    mean_rnd_bonus     |    0.000  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        |   40.000  |\n",
      "|    ep_rew_mean        |    0.340  |\n",
      "|    ep_rew_std         |    0.474  |\n",
      "|    policy_entropy     |    0.464  |\n",
      "|    advantage_mean     |    0.250  |\n",
      "|    advantage_std      |    0.190  |\n",
      "|    aux_loss_mean      |    0.021  |\n",
      "| time/                 |           |\n",
      "|    fps                |      190  |\n",
      "|    episodes           |    19500  |\n",
      "|    time_elapsed       |     4086  |\n",
      "|    total_timesteps    |   780000  |\n",
      "| train/                |           |\n",
      "|    loss               |    4.191  |\n",
      "|    policy_loss        |    4.186  |\n",
      "|    value_loss         |    0.098  |\n",
      "|    explained_variance |    0.133  |\n",
      "|    n_updates          |    19500  |\n",
      "| aux_train/            |           |\n",
      "|    aux_confidence_mse |    0.024  |\n",
      "| rnd_net_dist/         |           |\n",
      "|    mean_rnd_bonus     |    0.000  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        |   40.000  |\n",
      "|    ep_rew_mean        |    0.290  |\n",
      "|    ep_rew_std         |    0.454  |\n",
      "|    policy_entropy     |    0.479  |\n",
      "|    advantage_mean     |    0.261  |\n",
      "|    advantage_std      |    0.185  |\n",
      "|    aux_loss_mean      |    0.018  |\n",
      "| time/                 |           |\n",
      "|    fps                |      190  |\n",
      "|    episodes           |    19600  |\n",
      "|    time_elapsed       |     4110  |\n",
      "|    total_timesteps    |   784000  |\n",
      "| train/                |           |\n",
      "|    loss               |    5.220  |\n",
      "|    policy_loss        |    5.215  |\n",
      "|    value_loss         |    0.102  |\n",
      "|    explained_variance |    0.204  |\n",
      "|    n_updates          |    19600  |\n",
      "| aux_train/            |           |\n",
      "|    aux_confidence_mse |    0.019  |\n",
      "| rnd_net_dist/         |           |\n",
      "|    mean_rnd_bonus     |    0.000  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        |   40.000  |\n",
      "|    ep_rew_mean        |    0.380  |\n",
      "|    ep_rew_std         |    0.485  |\n",
      "|    policy_entropy     |    0.395  |\n",
      "|    advantage_mean     |    0.240  |\n",
      "|    advantage_std      |    0.181  |\n",
      "|    aux_loss_mean      |    0.017  |\n",
      "| time/                 |           |\n",
      "|    fps                |      190  |\n",
      "|    episodes           |    19700  |\n",
      "|    time_elapsed       |     4134  |\n",
      "|    total_timesteps    |   788000  |\n",
      "| train/                |           |\n",
      "|    loss               |    2.584  |\n",
      "|    policy_loss        |    2.576  |\n",
      "|    value_loss         |    0.090  |\n",
      "|    explained_variance |    0.169  |\n",
      "|    n_updates          |    19700  |\n",
      "| aux_train/            |           |\n",
      "|    aux_confidence_mse |    0.024  |\n",
      "| rnd_net_dist/         |           |\n",
      "|    mean_rnd_bonus     |    0.000  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        |   40.000  |\n",
      "|    ep_rew_mean        |    0.360  |\n",
      "|    ep_rew_std         |    0.480  |\n",
      "|    policy_entropy     |    0.531  |\n",
      "|    advantage_mean     |   -0.120  |\n",
      "|    advantage_std      |    0.109  |\n",
      "|    aux_loss_mean      |    0.019  |\n",
      "| time/                 |           |\n",
      "|    fps                |      190  |\n",
      "|    episodes           |    19800  |\n",
      "|    time_elapsed       |     4157  |\n",
      "|    total_timesteps    |   792000  |\n",
      "| train/                |           |\n",
      "|    loss               |   -2.703  |\n",
      "|    policy_loss        |   -2.665  |\n",
      "|    value_loss         |    0.026  |\n",
      "|    explained_variance |   -2.125  |\n",
      "|    n_updates          |    19800  |\n",
      "| aux_train/            |           |\n",
      "|    aux_confidence_mse |    0.019  |\n",
      "| rnd_net_dist/         |           |\n",
      "|    mean_rnd_bonus     |    0.000  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        |   40.000  |\n",
      "|    ep_rew_mean        |    0.410  |\n",
      "|    ep_rew_std         |    0.492  |\n",
      "|    policy_entropy     |    0.566  |\n",
      "|    advantage_mean     |    0.226  |\n",
      "|    advantage_std      |    0.173  |\n",
      "|    aux_loss_mean      |    0.017  |\n",
      "| time/                 |           |\n",
      "|    fps                |      190  |\n",
      "|    episodes           |    19900  |\n",
      "|    time_elapsed       |     4180  |\n",
      "|    total_timesteps    |   796000  |\n",
      "| train/                |           |\n",
      "|    loss               |    4.496  |\n",
      "|    policy_loss        |    4.511  |\n",
      "|    value_loss         |    0.080  |\n",
      "|    explained_variance |    0.204  |\n",
      "|    n_updates          |    19900  |\n",
      "| aux_train/            |           |\n",
      "|    aux_confidence_mse |    0.013  |\n",
      "| rnd_net_dist/         |           |\n",
      "|    mean_rnd_bonus     |    0.000  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        |   40.000  |\n",
      "|    ep_rew_mean        |    0.330  |\n",
      "|    ep_rew_std         |    0.470  |\n",
      "|    policy_entropy     |    0.416  |\n",
      "|    advantage_mean     |    0.253  |\n",
      "|    advantage_std      |    0.179  |\n",
      "|    aux_loss_mean      |    0.017  |\n",
      "| time/                 |           |\n",
      "|    fps                |      190  |\n",
      "|    episodes           |    20000  |\n",
      "|    time_elapsed       |     4204  |\n",
      "|    total_timesteps    |   800000  |\n",
      "| train/                |           |\n",
      "|    loss               |    3.011  |\n",
      "|    policy_loss        |    3.003  |\n",
      "|    value_loss         |    0.095  |\n",
      "|    explained_variance |    0.233  |\n",
      "|    n_updates          |    20000  |\n",
      "| aux_train/            |           |\n",
      "|    aux_confidence_mse |    0.025  |\n",
      "| rnd_net_dist/         |           |\n",
      "|    mean_rnd_bonus     |    0.000  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        |   40.000  |\n",
      "|    ep_rew_mean        |    0.370  |\n",
      "|    ep_rew_std         |    0.483  |\n",
      "|    policy_entropy     |    0.514  |\n",
      "|    advantage_mean     |   -0.139  |\n",
      "|    advantage_std      |    0.085  |\n",
      "|    aux_loss_mean      |    0.016  |\n",
      "| time/                 |           |\n",
      "|    fps                |      190  |\n",
      "|    episodes           |    20100  |\n",
      "|    time_elapsed       |     4227  |\n",
      "|    total_timesteps    |   804000  |\n",
      "| train/                |           |\n",
      "|    loss               |   -2.809  |\n",
      "|    policy_loss        |   -2.773  |\n",
      "|    value_loss         |    0.026  |\n",
      "|    explained_variance |   -0.973  |\n",
      "|    n_updates          |    20100  |\n",
      "| aux_train/            |           |\n",
      "|    aux_confidence_mse |    0.017  |\n",
      "| rnd_net_dist/         |           |\n",
      "|    mean_rnd_bonus     |    0.000  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        |   40.000  |\n",
      "|    ep_rew_mean        |    0.450  |\n",
      "|    ep_rew_std         |    0.497  |\n",
      "|    policy_entropy     |    0.541  |\n",
      "|    advantage_mean     |    0.225  |\n",
      "|    advantage_std      |    0.176  |\n",
      "|    aux_loss_mean      |    0.016  |\n",
      "| time/                 |           |\n",
      "|    fps                |      190  |\n",
      "|    episodes           |    20200  |\n",
      "|    time_elapsed       |     4250  |\n",
      "|    total_timesteps    |   808000  |\n",
      "| train/                |           |\n",
      "|    loss               |    2.992  |\n",
      "|    policy_loss        |    3.004  |\n",
      "|    value_loss         |    0.081  |\n",
      "|    explained_variance |    0.164  |\n",
      "|    n_updates          |    20200  |\n",
      "| aux_train/            |           |\n",
      "|    aux_confidence_mse |    0.013  |\n",
      "| rnd_net_dist/         |           |\n",
      "|    mean_rnd_bonus     |    0.000  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        |   40.000  |\n",
      "|    ep_rew_mean        |    0.410  |\n",
      "|    ep_rew_std         |    0.492  |\n",
      "|    policy_entropy     |    0.406  |\n",
      "|    advantage_mean     |   -0.155  |\n",
      "|    advantage_std      |    0.089  |\n",
      "|    aux_loss_mean      |    0.019  |\n",
      "| time/                 |           |\n",
      "|    fps                |      189  |\n",
      "|    episodes           |    20300  |\n",
      "|    time_elapsed       |     4274  |\n",
      "|    total_timesteps    |   812000  |\n",
      "| train/                |           |\n",
      "|    loss               |   -1.579  |\n",
      "|    policy_loss        |   -1.557  |\n",
      "|    value_loss         |    0.032  |\n",
      "|    explained_variance |   -0.616  |\n",
      "|    n_updates          |    20300  |\n",
      "| aux_train/            |           |\n",
      "|    aux_confidence_mse |    0.029  |\n",
      "| rnd_net_dist/         |           |\n",
      "|    mean_rnd_bonus     |    0.000  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        |   40.000  |\n",
      "|    ep_rew_mean        |    0.280  |\n",
      "|    ep_rew_std         |    0.449  |\n",
      "|    policy_entropy     |    0.542  |\n",
      "|    advantage_mean     |    0.261  |\n",
      "|    advantage_std      |    0.188  |\n",
      "|    aux_loss_mean      |    0.020  |\n",
      "| time/                 |           |\n",
      "|    fps                |      189  |\n",
      "|    episodes           |    20400  |\n",
      "|    time_elapsed       |     4298  |\n",
      "|    total_timesteps    |   816000  |\n",
      "| train/                |           |\n",
      "|    loss               |    5.065  |\n",
      "|    policy_loss        |    5.066  |\n",
      "|    value_loss         |    0.103  |\n",
      "|    explained_variance |    0.177  |\n",
      "|    n_updates          |    20400  |\n",
      "| aux_train/            |           |\n",
      "|    aux_confidence_mse |    0.017  |\n",
      "| rnd_net_dist/         |           |\n",
      "|    mean_rnd_bonus     |    0.000  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        |   40.000  |\n",
      "|    ep_rew_mean        |    0.360  |\n",
      "|    ep_rew_std         |    0.480  |\n",
      "|    policy_entropy     |    0.460  |\n",
      "|    advantage_mean     |    0.249  |\n",
      "|    advantage_std      |    0.186  |\n",
      "|    aux_loss_mean      |    0.019  |\n",
      "| time/                 |           |\n",
      "|    fps                |      189  |\n",
      "|    episodes           |    20500  |\n",
      "|    time_elapsed       |     4321  |\n",
      "|    total_timesteps    |   820000  |\n",
      "| train/                |           |\n",
      "|    loss               |    3.257  |\n",
      "|    policy_loss        |    3.253  |\n",
      "|    value_loss         |    0.096  |\n",
      "|    explained_variance |    0.163  |\n",
      "|    n_updates          |    20500  |\n",
      "| aux_train/            |           |\n",
      "|    aux_confidence_mse |    0.024  |\n",
      "| rnd_net_dist/         |           |\n",
      "|    mean_rnd_bonus     |    0.000  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        |   40.000  |\n",
      "|    ep_rew_mean        |    0.410  |\n",
      "|    ep_rew_std         |    0.492  |\n",
      "|    policy_entropy     |    0.517  |\n",
      "|    advantage_mean     |   -0.153  |\n",
      "|    advantage_std      |    0.100  |\n",
      "|    aux_loss_mean      |    0.019  |\n",
      "| time/                 |           |\n",
      "|    fps                |      189  |\n",
      "|    episodes           |    20600  |\n",
      "|    time_elapsed       |     4344  |\n",
      "|    total_timesteps    |   824000  |\n",
      "| train/                |           |\n",
      "|    loss               |   -3.286  |\n",
      "|    policy_loss        |   -3.253  |\n",
      "|    value_loss         |    0.033  |\n",
      "|    explained_variance |   -1.134  |\n",
      "|    n_updates          |    20600  |\n",
      "| aux_train/            |           |\n",
      "|    aux_confidence_mse |    0.020  |\n",
      "| rnd_net_dist/         |           |\n",
      "|    mean_rnd_bonus     |    0.000  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        |   40.000  |\n",
      "|    ep_rew_mean        |    0.380  |\n",
      "|    ep_rew_std         |    0.485  |\n",
      "|    policy_entropy     |    0.483  |\n",
      "|    advantage_mean     |   -0.140  |\n",
      "|    advantage_std      |    0.092  |\n",
      "|    aux_loss_mean      |    0.021  |\n",
      "| time/                 |           |\n",
      "|    fps                |      189  |\n",
      "|    episodes           |    20700  |\n",
      "|    time_elapsed       |     4368  |\n",
      "|    total_timesteps    |   828000  |\n",
      "| train/                |           |\n",
      "|    loss               |   -2.434  |\n",
      "|    policy_loss        |   -2.402  |\n",
      "|    value_loss         |    0.028  |\n",
      "|    explained_variance |   -1.149  |\n",
      "|    n_updates          |    20700  |\n",
      "| aux_train/            |           |\n",
      "|    aux_confidence_mse |    0.020  |\n",
      "| rnd_net_dist/         |           |\n",
      "|    mean_rnd_bonus     |    0.000  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        |   40.000  |\n",
      "|    ep_rew_mean        |    0.350  |\n",
      "|    ep_rew_std         |    0.477  |\n",
      "|    policy_entropy     |    0.518  |\n",
      "|    advantage_mean     |   -0.142  |\n",
      "|    advantage_std      |    0.091  |\n",
      "|    aux_loss_mean      |    0.022  |\n",
      "| time/                 |           |\n",
      "|    fps                |      189  |\n",
      "|    episodes           |    20800  |\n",
      "|    time_elapsed       |     4391  |\n",
      "|    total_timesteps    |   832000  |\n",
      "| train/                |           |\n",
      "|    loss               |   -2.269  |\n",
      "|    policy_loss        |   -2.233  |\n",
      "|    value_loss         |    0.028  |\n",
      "|    explained_variance |   -1.128  |\n",
      "|    n_updates          |    20800  |\n",
      "| aux_train/            |           |\n",
      "|    aux_confidence_mse |    0.021  |\n",
      "| rnd_net_dist/         |           |\n",
      "|    mean_rnd_bonus     |    0.000  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        |   40.000  |\n",
      "|    ep_rew_mean        |    0.380  |\n",
      "|    ep_rew_std         |    0.485  |\n",
      "|    policy_entropy     |    0.541  |\n",
      "|    advantage_mean     |   -0.149  |\n",
      "|    advantage_std      |    0.093  |\n",
      "|    aux_loss_mean      |    0.024  |\n",
      "| time/                 |           |\n",
      "|    fps                |      189  |\n",
      "|    episodes           |    20900  |\n",
      "|    time_elapsed       |     4415  |\n",
      "|    total_timesteps    |   836000  |\n",
      "| train/                |           |\n",
      "|    loss               |   -2.959  |\n",
      "|    policy_loss        |   -2.922  |\n",
      "|    value_loss         |    0.030  |\n",
      "|    explained_variance |   -1.022  |\n",
      "|    n_updates          |    20900  |\n",
      "| aux_train/            |           |\n",
      "|    aux_confidence_mse |    0.017  |\n",
      "| rnd_net_dist/         |           |\n",
      "|    mean_rnd_bonus     |    0.000  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        |   40.000  |\n",
      "|    ep_rew_mean        |    0.350  |\n",
      "|    ep_rew_std         |    0.477  |\n",
      "|    policy_entropy     |    0.530  |\n",
      "|    advantage_mean     |   -0.138  |\n",
      "|    advantage_std      |    0.088  |\n",
      "|    aux_loss_mean      |    0.026  |\n",
      "| time/                 |           |\n",
      "|    fps                |      189  |\n",
      "|    episodes           |    21000  |\n",
      "|    time_elapsed       |     4438  |\n",
      "|    total_timesteps    |   840000  |\n",
      "| train/                |           |\n",
      "|    loss               |   -3.192  |\n",
      "|    policy_loss        |   -3.154  |\n",
      "|    value_loss         |    0.027  |\n",
      "|    explained_variance |   -1.119  |\n",
      "|    n_updates          |    21000  |\n",
      "| aux_train/            |           |\n",
      "|    aux_confidence_mse |    0.020  |\n",
      "| rnd_net_dist/         |           |\n",
      "|    mean_rnd_bonus     |    0.000  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        |   40.000  |\n",
      "|    ep_rew_mean        |    0.380  |\n",
      "|    ep_rew_std         |    0.485  |\n",
      "|    policy_entropy     |    0.501  |\n",
      "|    advantage_mean     |   -0.134  |\n",
      "|    advantage_std      |    0.079  |\n",
      "|    aux_loss_mean      |    0.021  |\n",
      "| time/                 |           |\n",
      "|    fps                |      189  |\n",
      "|    episodes           |    21100  |\n",
      "|    time_elapsed       |     4462  |\n",
      "|    total_timesteps    |   844000  |\n",
      "| train/                |           |\n",
      "|    loss               |   -2.655  |\n",
      "|    policy_loss        |   -2.618  |\n",
      "|    value_loss         |    0.024  |\n",
      "|    explained_variance |   -0.735  |\n",
      "|    n_updates          |    21100  |\n",
      "| aux_train/            |           |\n",
      "|    aux_confidence_mse |    0.018  |\n",
      "| rnd_net_dist/         |           |\n",
      "|    mean_rnd_bonus     |    0.000  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        |   40.000  |\n",
      "|    ep_rew_mean        |    0.340  |\n",
      "|    ep_rew_std         |    0.474  |\n",
      "|    policy_entropy     |    0.449  |\n",
      "|    advantage_mean     |   -0.138  |\n",
      "|    advantage_std      |    0.084  |\n",
      "|    aux_loss_mean      |    0.025  |\n",
      "| time/                 |           |\n",
      "|    fps                |      189  |\n",
      "|    episodes           |    21200  |\n",
      "|    time_elapsed       |     4485  |\n",
      "|    total_timesteps    |   848000  |\n",
      "| train/                |           |\n",
      "|    loss               |   -2.706  |\n",
      "|    policy_loss        |   -2.677  |\n",
      "|    value_loss         |    0.026  |\n",
      "|    explained_variance |   -0.767  |\n",
      "|    n_updates          |    21200  |\n",
      "| aux_train/            |           |\n",
      "|    aux_confidence_mse |    0.026  |\n",
      "| rnd_net_dist/         |           |\n",
      "|    mean_rnd_bonus     |    0.000  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        |   40.000  |\n",
      "|    ep_rew_mean        |    0.320  |\n",
      "|    ep_rew_std         |    0.466  |\n",
      "|    policy_entropy     |    0.378  |\n",
      "|    advantage_mean     |   -0.129  |\n",
      "|    advantage_std      |    0.084  |\n",
      "|    aux_loss_mean      |    0.024  |\n",
      "| time/                 |           |\n",
      "|    fps                |      188  |\n",
      "|    episodes           |    21300  |\n",
      "|    time_elapsed       |     4508  |\n",
      "|    total_timesteps    |   852000  |\n",
      "| train/                |           |\n",
      "|    loss               |   -2.307  |\n",
      "|    policy_loss        |   -2.284  |\n",
      "|    value_loss         |    0.023  |\n",
      "|    explained_variance |   -1.191  |\n",
      "|    n_updates          |    21300  |\n",
      "| aux_train/            |           |\n",
      "|    aux_confidence_mse |    0.037  |\n",
      "| rnd_net_dist/         |           |\n",
      "|    mean_rnd_bonus     |    0.000  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        |   40.000  |\n",
      "|    ep_rew_mean        |    0.350  |\n",
      "|    ep_rew_std         |    0.477  |\n",
      "|    policy_entropy     |    0.418  |\n",
      "|    advantage_mean     |    0.256  |\n",
      "|    advantage_std      |    0.178  |\n",
      "|    aux_loss_mean      |    0.022  |\n",
      "| time/                 |           |\n",
      "|    fps                |      188  |\n",
      "|    episodes           |    21400  |\n",
      "|    time_elapsed       |     4532  |\n",
      "|    total_timesteps    |   856000  |\n",
      "| train/                |           |\n",
      "|    loss               |    3.667  |\n",
      "|    policy_loss        |    3.658  |\n",
      "|    value_loss         |    0.096  |\n",
      "|    explained_variance |    0.266  |\n",
      "|    n_updates          |    21400  |\n",
      "| aux_train/            |           |\n",
      "|    aux_confidence_mse |    0.022  |\n",
      "| rnd_net_dist/         |           |\n",
      "|    mean_rnd_bonus     |    0.000  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        |   40.000  |\n",
      "|    ep_rew_mean        |    0.330  |\n",
      "|    ep_rew_std         |    0.470  |\n",
      "|    policy_entropy     |    0.431  |\n",
      "|    advantage_mean     |   -0.136  |\n",
      "|    advantage_std      |    0.083  |\n",
      "|    aux_loss_mean      |    0.022  |\n",
      "| time/                 |           |\n",
      "|    fps                |      188  |\n",
      "|    episodes           |    21500  |\n",
      "|    time_elapsed       |     4555  |\n",
      "|    total_timesteps    |   860000  |\n",
      "| train/                |           |\n",
      "|    loss               |   -2.127  |\n",
      "|    policy_loss        |   -2.099  |\n",
      "|    value_loss         |    0.025  |\n",
      "|    explained_variance |   -0.969  |\n",
      "|    n_updates          |    21500  |\n",
      "| aux_train/            |           |\n",
      "|    aux_confidence_mse |    0.024  |\n",
      "| rnd_net_dist/         |           |\n",
      "|    mean_rnd_bonus     |    0.000  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        |   40.000  |\n",
      "|    ep_rew_mean        |    0.430  |\n",
      "|    ep_rew_std         |    0.495  |\n",
      "|    policy_entropy     |    0.429  |\n",
      "|    advantage_mean     |   -0.155  |\n",
      "|    advantage_std      |    0.105  |\n",
      "|    aux_loss_mean      |    0.019  |\n",
      "| time/                 |           |\n",
      "|    fps                |      188  |\n",
      "|    episodes           |    21600  |\n",
      "|    time_elapsed       |     4578  |\n",
      "|    total_timesteps    |   864000  |\n",
      "| train/                |           |\n",
      "|    loss               |   -2.003  |\n",
      "|    policy_loss        |   -1.979  |\n",
      "|    value_loss         |    0.035  |\n",
      "|    explained_variance |   -1.216  |\n",
      "|    n_updates          |    21600  |\n",
      "| aux_train/            |           |\n",
      "|    aux_confidence_mse |    0.017  |\n",
      "| rnd_net_dist/         |           |\n",
      "|    mean_rnd_bonus     |    0.000  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        |   40.000  |\n",
      "|    ep_rew_mean        |    0.250  |\n",
      "|    ep_rew_std         |    0.433  |\n",
      "|    policy_entropy     |    0.433  |\n",
      "|    advantage_mean     |   -0.111  |\n",
      "|    advantage_std      |    0.077  |\n",
      "|    aux_loss_mean      |    0.023  |\n",
      "| time/                 |           |\n",
      "|    fps                |      188  |\n",
      "|    episodes           |    21700  |\n",
      "|    time_elapsed       |     4602  |\n",
      "|    total_timesteps    |   868000  |\n",
      "| train/                |           |\n",
      "|    loss               |   -1.864  |\n",
      "|    policy_loss        |   -1.833  |\n",
      "|    value_loss         |    0.018  |\n",
      "|    explained_variance |   -1.480  |\n",
      "|    n_updates          |    21700  |\n",
      "| aux_train/            |           |\n",
      "|    aux_confidence_mse |    0.029  |\n",
      "| rnd_net_dist/         |           |\n",
      "|    mean_rnd_bonus     |    0.000  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        |   40.000  |\n",
      "|    ep_rew_mean        |    0.410  |\n",
      "|    ep_rew_std         |    0.492  |\n",
      "|    policy_entropy     |    0.531  |\n",
      "|    advantage_mean     |    0.254  |\n",
      "|    advantage_std      |    0.195  |\n",
      "|    aux_loss_mean      |    0.024  |\n",
      "| time/                 |           |\n",
      "|    fps                |      188  |\n",
      "|    episodes           |    21800  |\n",
      "|    time_elapsed       |     4626  |\n",
      "|    total_timesteps    |   872000  |\n",
      "| train/                |           |\n",
      "|    loss               |    5.676  |\n",
      "|    policy_loss        |    5.677  |\n",
      "|    value_loss         |    0.102  |\n",
      "|    explained_variance |    0.063  |\n",
      "|    n_updates          |    21800  |\n",
      "| aux_train/            |           |\n",
      "|    aux_confidence_mse |    0.019  |\n",
      "| rnd_net_dist/         |           |\n",
      "|    mean_rnd_bonus     |    0.000  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        |   40.000  |\n",
      "|    ep_rew_mean        |    0.420  |\n",
      "|    ep_rew_std         |    0.494  |\n",
      "|    policy_entropy     |    0.419  |\n",
      "|    advantage_mean     |    0.222  |\n",
      "|    advantage_std      |    0.171  |\n",
      "|    aux_loss_mean      |    0.024  |\n",
      "| time/                 |           |\n",
      "|    fps                |      188  |\n",
      "|    episodes           |    21900  |\n",
      "|    time_elapsed       |     4649  |\n",
      "|    total_timesteps    |   876000  |\n",
      "| train/                |           |\n",
      "|    loss               |    5.501  |\n",
      "|    policy_loss        |    5.501  |\n",
      "|    value_loss         |    0.078  |\n",
      "|    explained_variance |    0.195  |\n",
      "|    n_updates          |    21900  |\n",
      "| aux_train/            |           |\n",
      "|    aux_confidence_mse |    0.031  |\n",
      "| rnd_net_dist/         |           |\n",
      "|    mean_rnd_bonus     |    0.000  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        |   40.000  |\n",
      "|    ep_rew_mean        |    0.370  |\n",
      "|    ep_rew_std         |    0.483  |\n",
      "|    policy_entropy     |    0.450  |\n",
      "|    advantage_mean     |    0.245  |\n",
      "|    advantage_std      |    0.171  |\n",
      "|    aux_loss_mean      |    0.023  |\n",
      "| time/                 |           |\n",
      "|    fps                |      188  |\n",
      "|    episodes           |    22000  |\n",
      "|    time_elapsed       |     4672  |\n",
      "|    total_timesteps    |   880000  |\n",
      "| train/                |           |\n",
      "|    loss               |    4.566  |\n",
      "|    policy_loss        |    4.564  |\n",
      "|    value_loss         |    0.088  |\n",
      "|    explained_variance |    0.260  |\n",
      "|    n_updates          |    22000  |\n",
      "| aux_train/            |           |\n",
      "|    aux_confidence_mse |    0.021  |\n",
      "| rnd_net_dist/         |           |\n",
      "|    mean_rnd_bonus     |    0.000  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        |   40.000  |\n",
      "|    ep_rew_mean        |    0.360  |\n",
      "|    ep_rew_std         |    0.480  |\n",
      "|    policy_entropy     |    0.515  |\n",
      "|    advantage_mean     |    0.248  |\n",
      "|    advantage_std      |    0.186  |\n",
      "|    aux_loss_mean      |    0.024  |\n",
      "| time/                 |           |\n",
      "|    fps                |      188  |\n",
      "|    episodes           |    22100  |\n",
      "|    time_elapsed       |     4696  |\n",
      "|    total_timesteps    |   884000  |\n",
      "| train/                |           |\n",
      "|    loss               |    6.089  |\n",
      "|    policy_loss        |    6.092  |\n",
      "|    value_loss         |    0.095  |\n",
      "|    explained_variance |    0.149  |\n",
      "|    n_updates          |    22100  |\n",
      "| aux_train/            |           |\n",
      "|    aux_confidence_mse |    0.018  |\n",
      "| rnd_net_dist/         |           |\n",
      "|    mean_rnd_bonus     |    0.000  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        |   40.000  |\n",
      "|    ep_rew_mean        |    0.360  |\n",
      "|    ep_rew_std         |    0.480  |\n",
      "|    policy_entropy     |    0.412  |\n",
      "|    advantage_mean     |   -0.139  |\n",
      "|    advantage_std      |    0.080  |\n",
      "|    aux_loss_mean      |    0.027  |\n",
      "| time/                 |           |\n",
      "|    fps                |      188  |\n",
      "|    episodes           |    22200  |\n",
      "|    time_elapsed       |     4719  |\n",
      "|    total_timesteps    |   888000  |\n",
      "| train/                |           |\n",
      "|    loss               |   -2.387  |\n",
      "|    policy_loss        |   -2.362  |\n",
      "|    value_loss         |    0.025  |\n",
      "|    explained_variance |   -0.685  |\n",
      "|    n_updates          |    22200  |\n",
      "| aux_train/            |           |\n",
      "|    aux_confidence_mse |    0.033  |\n",
      "| rnd_net_dist/         |           |\n",
      "|    mean_rnd_bonus     |    0.000  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        |   40.000  |\n",
      "|    ep_rew_mean        |    0.340  |\n",
      "|    ep_rew_std         |    0.474  |\n",
      "|    policy_entropy     |    0.323  |\n",
      "|    advantage_mean     |   -0.140  |\n",
      "|    advantage_std      |    0.094  |\n",
      "|    aux_loss_mean      |    0.032  |\n",
      "| time/                 |           |\n",
      "|    fps                |      188  |\n",
      "|    episodes           |    22300  |\n",
      "|    time_elapsed       |     4742  |\n",
      "|    total_timesteps    |   892000  |\n",
      "| train/                |           |\n",
      "|    loss               |   -1.481  |\n",
      "|    policy_loss        |   -1.468  |\n",
      "|    value_loss         |    0.028  |\n",
      "|    explained_variance |   -1.270  |\n",
      "|    n_updates          |    22300  |\n",
      "| aux_train/            |           |\n",
      "|    aux_confidence_mse |    0.054  |\n",
      "| rnd_net_dist/         |           |\n",
      "|    mean_rnd_bonus     |    0.000  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        |   40.000  |\n",
      "|    ep_rew_mean        |    0.320  |\n",
      "|    ep_rew_std         |    0.466  |\n",
      "|    policy_entropy     |    0.483  |\n",
      "|    advantage_mean     |    0.259  |\n",
      "|    advantage_std      |    0.187  |\n",
      "|    aux_loss_mean      |    0.029  |\n",
      "| time/                 |           |\n",
      "|    fps                |      188  |\n",
      "|    episodes           |    22400  |\n",
      "|    time_elapsed       |     4765  |\n",
      "|    total_timesteps    |   896000  |\n",
      "| train/                |           |\n",
      "|    loss               |    7.568  |\n",
      "|    policy_loss        |    7.564  |\n",
      "|    value_loss         |    0.101  |\n",
      "|    explained_variance |    0.201  |\n",
      "|    n_updates          |    22400  |\n",
      "| aux_train/            |           |\n",
      "|    aux_confidence_mse |    0.023  |\n",
      "| rnd_net_dist/         |           |\n",
      "|    mean_rnd_bonus     |    0.000  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        |   40.000  |\n",
      "|    ep_rew_mean        |    0.410  |\n",
      "|    ep_rew_std         |    0.492  |\n",
      "|    policy_entropy     |    0.558  |\n",
      "|    advantage_mean     |   -0.146  |\n",
      "|    advantage_std      |    0.092  |\n",
      "|    aux_loss_mean      |    0.026  |\n",
      "| time/                 |           |\n",
      "|    fps                |      187  |\n",
      "|    episodes           |    22500  |\n",
      "|    time_elapsed       |     4789  |\n",
      "|    total_timesteps    |   900000  |\n",
      "| train/                |           |\n",
      "|    loss               |   -2.648  |\n",
      "|    policy_loss        |   -2.609  |\n",
      "|    value_loss         |    0.030  |\n",
      "|    explained_variance |   -1.129  |\n",
      "|    n_updates          |    22500  |\n",
      "| aux_train/            |           |\n",
      "|    aux_confidence_mse |    0.016  |\n",
      "| rnd_net_dist/         |           |\n",
      "|    mean_rnd_bonus     |    0.000  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        |   40.000  |\n",
      "|    ep_rew_mean        |    0.350  |\n",
      "|    ep_rew_std         |    0.477  |\n",
      "|    policy_entropy     |    0.418  |\n",
      "|    advantage_mean     |   -0.139  |\n",
      "|    advantage_std      |    0.099  |\n",
      "|    aux_loss_mean      |    0.025  |\n",
      "| time/                 |           |\n",
      "|    fps                |      187  |\n",
      "|    episodes           |    22600  |\n",
      "|    time_elapsed       |     4812  |\n",
      "|    total_timesteps    |   904000  |\n",
      "| train/                |           |\n",
      "|    loss               |   -1.655  |\n",
      "|    policy_loss        |   -1.630  |\n",
      "|    value_loss         |    0.029  |\n",
      "|    explained_variance |   -1.410  |\n",
      "|    n_updates          |    22600  |\n",
      "| aux_train/            |           |\n",
      "|    aux_confidence_mse |    0.027  |\n",
      "| rnd_net_dist/         |           |\n",
      "|    mean_rnd_bonus     |    0.000  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        |   40.000  |\n",
      "|    ep_rew_mean        |    0.320  |\n",
      "|    ep_rew_std         |    0.466  |\n",
      "|    policy_entropy     |    0.484  |\n",
      "|    advantage_mean     |    0.256  |\n",
      "|    advantage_std      |    0.185  |\n",
      "|    aux_loss_mean      |    0.017  |\n",
      "| time/                 |           |\n",
      "|    fps                |      187  |\n",
      "|    episodes           |    22700  |\n",
      "|    time_elapsed       |     4836  |\n",
      "|    total_timesteps    |   908000  |\n",
      "| train/                |           |\n",
      "|    loss               |    5.043  |\n",
      "|    policy_loss        |    5.041  |\n",
      "|    value_loss         |    0.099  |\n",
      "|    explained_variance |    0.198  |\n",
      "|    n_updates          |    22700  |\n",
      "| aux_train/            |           |\n",
      "|    aux_confidence_mse |    0.018  |\n",
      "| rnd_net_dist/         |           |\n",
      "|    mean_rnd_bonus     |    0.000  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        |   40.000  |\n",
      "|    ep_rew_mean        |    0.380  |\n",
      "|    ep_rew_std         |    0.485  |\n",
      "|    policy_entropy     |    0.369  |\n",
      "|    advantage_mean     |   -0.150  |\n",
      "|    advantage_std      |    0.091  |\n",
      "|    aux_loss_mean      |    0.018  |\n",
      "| time/                 |           |\n",
      "|    fps                |      187  |\n",
      "|    episodes           |    22800  |\n",
      "|    time_elapsed       |     4859  |\n",
      "|    total_timesteps    |   912000  |\n",
      "| train/                |           |\n",
      "|    loss               |   -2.407  |\n",
      "|    policy_loss        |   -2.389  |\n",
      "|    value_loss         |    0.030  |\n",
      "|    explained_variance |   -0.753  |\n",
      "|    n_updates          |    22800  |\n",
      "| aux_train/            |           |\n",
      "|    aux_confidence_mse |    0.031  |\n",
      "| rnd_net_dist/         |           |\n",
      "|    mean_rnd_bonus     |    0.000  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        |   40.000  |\n",
      "|    ep_rew_mean        |    0.310  |\n",
      "|    ep_rew_std         |    0.462  |\n",
      "|    policy_entropy     |    0.508  |\n",
      "|    advantage_mean     |    0.250  |\n",
      "|    advantage_std      |    0.189  |\n",
      "|    aux_loss_mean      |    0.019  |\n",
      "| time/                 |           |\n",
      "|    fps                |      187  |\n",
      "|    episodes           |    22900  |\n",
      "|    time_elapsed       |     4882  |\n",
      "|    total_timesteps    |   916000  |\n",
      "| train/                |           |\n",
      "|    loss               |    7.314  |\n",
      "|    policy_loss        |    7.315  |\n",
      "|    value_loss         |    0.097  |\n",
      "|    explained_variance |    0.153  |\n",
      "|    n_updates          |    22900  |\n",
      "| aux_train/            |           |\n",
      "|    aux_confidence_mse |    0.015  |\n",
      "| rnd_net_dist/         |           |\n",
      "|    mean_rnd_bonus     |    0.000  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        |   40.000  |\n",
      "|    ep_rew_mean        |    0.360  |\n",
      "|    ep_rew_std         |    0.480  |\n",
      "|    policy_entropy     |    0.464  |\n",
      "|    advantage_mean     |    0.255  |\n",
      "|    advantage_std      |    0.178  |\n",
      "|    aux_loss_mean      |    0.015  |\n",
      "| time/                 |           |\n",
      "|    fps                |      187  |\n",
      "|    episodes           |    23000  |\n",
      "|    time_elapsed       |     4905  |\n",
      "|    total_timesteps    |   920000  |\n",
      "| train/                |           |\n",
      "|    loss               |    4.030  |\n",
      "|    policy_loss        |    4.028  |\n",
      "|    value_loss         |    0.096  |\n",
      "|    explained_variance |    0.247  |\n",
      "|    n_updates          |    23000  |\n",
      "| aux_train/            |           |\n",
      "|    aux_confidence_mse |    0.010  |\n",
      "| rnd_net_dist/         |           |\n",
      "|    mean_rnd_bonus     |    0.000  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        |   40.000  |\n",
      "|    ep_rew_mean        |    0.540  |\n",
      "|    ep_rew_std         |    0.498  |\n",
      "|    policy_entropy     |    0.592  |\n",
      "|    advantage_mean     |    0.207  |\n",
      "|    advantage_std      |    0.165  |\n",
      "|    aux_loss_mean      |    0.011  |\n",
      "| time/                 |           |\n",
      "|    fps                |      187  |\n",
      "|    episodes           |    23100  |\n",
      "|    time_elapsed       |     4929  |\n",
      "|    total_timesteps    |   924000  |\n",
      "| train/                |           |\n",
      "|    loss               |    5.831  |\n",
      "|    policy_loss        |    5.855  |\n",
      "|    value_loss         |    0.069  |\n",
      "|    explained_variance |    0.179  |\n",
      "|    n_updates          |    23100  |\n",
      "| aux_train/            |           |\n",
      "|    aux_confidence_mse |    0.011  |\n",
      "| rnd_net_dist/         |           |\n",
      "|    mean_rnd_bonus     |    0.000  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        |   40.000  |\n",
      "|    ep_rew_mean        |    0.360  |\n",
      "|    ep_rew_std         |    0.480  |\n",
      "|    policy_entropy     |    0.570  |\n",
      "|    advantage_mean     |   -0.155  |\n",
      "|    advantage_std      |    0.090  |\n",
      "|    aux_loss_mean      |    0.011  |\n",
      "| time/                 |           |\n",
      "|    fps                |      187  |\n",
      "|    episodes           |    23200  |\n",
      "|    time_elapsed       |     4952  |\n",
      "|    total_timesteps    |   928000  |\n",
      "| train/                |           |\n",
      "|    loss               |   -3.459  |\n",
      "|    policy_loss        |   -3.419  |\n",
      "|    value_loss         |    0.032  |\n",
      "|    explained_variance |   -0.726  |\n",
      "|    n_updates          |    23200  |\n",
      "| aux_train/            |           |\n",
      "|    aux_confidence_mse |    0.013  |\n",
      "| rnd_net_dist/         |           |\n",
      "|    mean_rnd_bonus     |    0.000  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        |   40.000  |\n",
      "|    ep_rew_mean        |    0.380  |\n",
      "|    ep_rew_std         |    0.485  |\n",
      "|    policy_entropy     |    0.602  |\n",
      "|    advantage_mean     |   -0.140  |\n",
      "|    advantage_std      |    0.092  |\n",
      "|    aux_loss_mean      |    0.011  |\n",
      "| time/                 |           |\n",
      "|    fps                |      187  |\n",
      "|    episodes           |    23300  |\n",
      "|    time_elapsed       |     4975  |\n",
      "|    total_timesteps    |   932000  |\n",
      "| train/                |           |\n",
      "|    loss               |   -3.090  |\n",
      "|    policy_loss        |   -3.044  |\n",
      "|    value_loss         |    0.028  |\n",
      "|    explained_variance |   -1.416  |\n",
      "|    n_updates          |    23300  |\n",
      "| aux_train/            |           |\n",
      "|    aux_confidence_mse |    0.008  |\n",
      "| rnd_net_dist/         |           |\n",
      "|    mean_rnd_bonus     |    0.000  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        |   40.000  |\n",
      "|    ep_rew_mean        |    0.330  |\n",
      "|    ep_rew_std         |    0.470  |\n",
      "|    policy_entropy     |    0.491  |\n",
      "|    advantage_mean     |   -0.135  |\n",
      "|    advantage_std      |    0.084  |\n",
      "|    aux_loss_mean      |    0.010  |\n",
      "| time/                 |           |\n",
      "|    fps                |      187  |\n",
      "|    episodes           |    23400  |\n",
      "|    time_elapsed       |     4999  |\n",
      "|    total_timesteps    |   936000  |\n",
      "| train/                |           |\n",
      "|    loss               |   -3.022  |\n",
      "|    policy_loss        |   -2.986  |\n",
      "|    value_loss         |    0.025  |\n",
      "|    explained_variance |   -0.962  |\n",
      "|    n_updates          |    23400  |\n",
      "| aux_train/            |           |\n",
      "|    aux_confidence_mse |    0.008  |\n",
      "| rnd_net_dist/         |           |\n",
      "|    mean_rnd_bonus     |    0.000  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        |   40.000  |\n",
      "|    ep_rew_mean        |    0.380  |\n",
      "|    ep_rew_std         |    0.485  |\n",
      "|    policy_entropy     |    0.524  |\n",
      "|    advantage_mean     |   -0.140  |\n",
      "|    advantage_std      |    0.085  |\n",
      "|    aux_loss_mean      |    0.009  |\n",
      "| time/                 |           |\n",
      "|    fps                |      187  |\n",
      "|    episodes           |    23500  |\n",
      "|    time_elapsed       |     5022  |\n",
      "|    total_timesteps    |   940000  |\n",
      "| train/                |           |\n",
      "|    loss               |   -2.663  |\n",
      "|    policy_loss        |   -2.625  |\n",
      "|    value_loss         |    0.027  |\n",
      "|    explained_variance |   -0.869  |\n",
      "|    n_updates          |    23500  |\n",
      "| aux_train/            |           |\n",
      "|    aux_confidence_mse |    0.008  |\n",
      "| rnd_net_dist/         |           |\n",
      "|    mean_rnd_bonus     |    0.000  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        |   40.000  |\n",
      "|    ep_rew_mean        |    0.340  |\n",
      "|    ep_rew_std         |    0.474  |\n",
      "|    policy_entropy     |    0.521  |\n",
      "|    advantage_mean     |    0.258  |\n",
      "|    advantage_std      |    0.178  |\n",
      "|    aux_loss_mean      |    0.010  |\n",
      "| time/                 |           |\n",
      "|    fps                |      187  |\n",
      "|    episodes           |    23600  |\n",
      "|    time_elapsed       |     5046  |\n",
      "|    total_timesteps    |   944000  |\n",
      "| train/                |           |\n",
      "|    loss               |    3.779  |\n",
      "|    policy_loss        |    3.781  |\n",
      "|    value_loss         |    0.098  |\n",
      "|    explained_variance |    0.238  |\n",
      "|    n_updates          |    23600  |\n",
      "| aux_train/            |           |\n",
      "|    aux_confidence_mse |    0.014  |\n",
      "| rnd_net_dist/         |           |\n",
      "|    mean_rnd_bonus     |    0.000  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        |   40.000  |\n",
      "|    ep_rew_mean        |    0.370  |\n",
      "|    ep_rew_std         |    0.483  |\n",
      "|    policy_entropy     |    0.432  |\n",
      "|    advantage_mean     |   -0.146  |\n",
      "|    advantage_std      |    0.090  |\n",
      "|    aux_loss_mean      |    0.010  |\n",
      "| time/                 |           |\n",
      "|    fps                |      187  |\n",
      "|    episodes           |    23700  |\n",
      "|    time_elapsed       |     5069  |\n",
      "|    total_timesteps    |   948000  |\n",
      "| train/                |           |\n",
      "|    loss               |   -2.209  |\n",
      "|    policy_loss        |   -2.182  |\n",
      "|    value_loss         |    0.029  |\n",
      "|    explained_variance |   -0.846  |\n",
      "|    n_updates          |    23700  |\n",
      "| aux_train/            |           |\n",
      "|    aux_confidence_mse |    0.018  |\n",
      "| rnd_net_dist/         |           |\n",
      "|    mean_rnd_bonus     |    0.000  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        |   40.000  |\n",
      "|    ep_rew_mean        |    0.340  |\n",
      "|    ep_rew_std         |    0.474  |\n",
      "|    policy_entropy     |    0.548  |\n",
      "|    advantage_mean     |   -0.135  |\n",
      "|    advantage_std      |    0.094  |\n",
      "|    aux_loss_mean      |    0.011  |\n",
      "| time/                 |           |\n",
      "|    fps                |      186  |\n",
      "|    episodes           |    23800  |\n",
      "|    time_elapsed       |     5092  |\n",
      "|    total_timesteps    |   952000  |\n",
      "| train/                |           |\n",
      "|    loss               |   -2.979  |\n",
      "|    policy_loss        |   -2.938  |\n",
      "|    value_loss         |    0.027  |\n",
      "|    explained_variance |   -1.296  |\n",
      "|    n_updates          |    23800  |\n",
      "| aux_train/            |           |\n",
      "|    aux_confidence_mse |    0.007  |\n",
      "| rnd_net_dist/         |           |\n",
      "|    mean_rnd_bonus     |    0.000  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        |   40.000  |\n",
      "|    ep_rew_mean        |    0.360  |\n",
      "|    ep_rew_std         |    0.480  |\n",
      "|    policy_entropy     |    0.479  |\n",
      "|    advantage_mean     |   -0.144  |\n",
      "|    advantage_std      |    0.088  |\n",
      "|    aux_loss_mean      |    0.010  |\n",
      "| time/                 |           |\n",
      "|    fps                |      186  |\n",
      "|    episodes           |    23900  |\n",
      "|    time_elapsed       |     5115  |\n",
      "|    total_timesteps    |   956000  |\n",
      "| train/                |           |\n",
      "|    loss               |   -2.810  |\n",
      "|    policy_loss        |   -2.777  |\n",
      "|    value_loss         |    0.028  |\n",
      "|    explained_variance |   -1.007  |\n",
      "|    n_updates          |    23900  |\n",
      "| aux_train/            |           |\n",
      "|    aux_confidence_mse |    0.010  |\n",
      "| rnd_net_dist/         |           |\n",
      "|    mean_rnd_bonus     |    0.000  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        |   40.000  |\n",
      "|    ep_rew_mean        |    0.350  |\n",
      "|    ep_rew_std         |    0.477  |\n",
      "|    policy_entropy     |    0.527  |\n",
      "|    advantage_mean     |    0.244  |\n",
      "|    advantage_std      |    0.191  |\n",
      "|    aux_loss_mean      |    0.009  |\n",
      "| time/                 |           |\n",
      "|    fps                |      186  |\n",
      "|    episodes           |    24000  |\n",
      "|    time_elapsed       |     5138  |\n",
      "|    total_timesteps    |   960000  |\n",
      "| train/                |           |\n",
      "|    loss               |    5.086  |\n",
      "|    policy_loss        |    5.090  |\n",
      "|    value_loss         |    0.095  |\n",
      "|    explained_variance |    0.093  |\n",
      "|    n_updates          |    24000  |\n",
      "| aux_train/            |           |\n",
      "|    aux_confidence_mse |    0.010  |\n",
      "| rnd_net_dist/         |           |\n",
      "|    mean_rnd_bonus     |    0.000  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        |   40.000  |\n",
      "|    ep_rew_mean        |    0.320  |\n",
      "|    ep_rew_std         |    0.466  |\n",
      "|    policy_entropy     |    0.539  |\n",
      "|    advantage_mean     |   -0.137  |\n",
      "|    advantage_std      |    0.089  |\n",
      "|    aux_loss_mean      |    0.009  |\n",
      "| time/                 |           |\n",
      "|    fps                |      186  |\n",
      "|    episodes           |    24100  |\n",
      "|    time_elapsed       |     5161  |\n",
      "|    total_timesteps    |   964000  |\n",
      "| train/                |           |\n",
      "|    loss               |   -1.983  |\n",
      "|    policy_loss        |   -1.944  |\n",
      "|    value_loss         |    0.027  |\n",
      "|    explained_variance |   -1.229  |\n",
      "|    n_updates          |    24100  |\n",
      "| aux_train/            |           |\n",
      "|    aux_confidence_mse |    0.010  |\n",
      "| rnd_net_dist/         |           |\n",
      "|    mean_rnd_bonus     |    0.000  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        |   40.000  |\n",
      "|    ep_rew_mean        |    0.320  |\n",
      "|    ep_rew_std         |    0.466  |\n",
      "|    policy_entropy     |    0.592  |\n",
      "|    advantage_mean     |   -0.127  |\n",
      "|    advantage_std      |    0.090  |\n",
      "|    aux_loss_mean      |    0.008  |\n",
      "| time/                 |           |\n",
      "|    fps                |      186  |\n",
      "|    episodes           |    24200  |\n",
      "|    time_elapsed       |     5185  |\n",
      "|    total_timesteps    |   968000  |\n",
      "| train/                |           |\n",
      "|    loss               |   -2.456  |\n",
      "|    policy_loss        |   -2.409  |\n",
      "|    value_loss         |    0.024  |\n",
      "|    explained_variance |   -1.511  |\n",
      "|    n_updates          |    24200  |\n",
      "| aux_train/            |           |\n",
      "|    aux_confidence_mse |    0.008  |\n",
      "| rnd_net_dist/         |           |\n",
      "|    mean_rnd_bonus     |    0.000  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        |   40.000  |\n",
      "|    ep_rew_mean        |    0.370  |\n",
      "|    ep_rew_std         |    0.483  |\n",
      "|    policy_entropy     |    0.577  |\n",
      "|    advantage_mean     |    0.248  |\n",
      "|    advantage_std      |    0.181  |\n",
      "|    aux_loss_mean      |    0.008  |\n",
      "| time/                 |           |\n",
      "|    fps                |      186  |\n",
      "|    episodes           |    24300  |\n",
      "|    time_elapsed       |     5208  |\n",
      "|    total_timesteps    |   972000  |\n",
      "| train/                |           |\n",
      "|    loss               |    4.865  |\n",
      "|    policy_loss        |    4.875  |\n",
      "|    value_loss         |    0.093  |\n",
      "|    explained_variance |    0.198  |\n",
      "|    n_updates          |    24300  |\n",
      "| aux_train/            |           |\n",
      "|    aux_confidence_mse |    0.009  |\n",
      "| rnd_net_dist/         |           |\n",
      "|    mean_rnd_bonus     |    0.000  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        |   40.000  |\n",
      "|    ep_rew_mean        |    0.360  |\n",
      "|    ep_rew_std         |    0.480  |\n",
      "|    policy_entropy     |    0.524  |\n",
      "|    advantage_mean     |   -0.147  |\n",
      "|    advantage_std      |    0.090  |\n",
      "|    aux_loss_mean      |    0.008  |\n",
      "| time/                 |           |\n",
      "|    fps                |      186  |\n",
      "|    episodes           |    24400  |\n",
      "|    time_elapsed       |     5231  |\n",
      "|    total_timesteps    |   976000  |\n",
      "| train/                |           |\n",
      "|    loss               |   -2.232  |\n",
      "|    policy_loss        |   -2.195  |\n",
      "|    value_loss         |    0.029  |\n",
      "|    explained_variance |   -0.895  |\n",
      "|    n_updates          |    24400  |\n",
      "| aux_train/            |           |\n",
      "|    aux_confidence_mse |    0.009  |\n",
      "| rnd_net_dist/         |           |\n",
      "|    mean_rnd_bonus     |    0.000  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        |   40.000  |\n",
      "|    ep_rew_mean        |    0.340  |\n",
      "|    ep_rew_std         |    0.474  |\n",
      "|    policy_entropy     |    0.554  |\n",
      "|    advantage_mean     |    0.258  |\n",
      "|    advantage_std      |    0.194  |\n",
      "|    aux_loss_mean      |    0.008  |\n",
      "| time/                 |           |\n",
      "|    fps                |      186  |\n",
      "|    episodes           |    24500  |\n",
      "|    time_elapsed       |     5254  |\n",
      "|    total_timesteps    |   980000  |\n",
      "| train/                |           |\n",
      "|    loss               |    5.733  |\n",
      "|    policy_loss        |    5.736  |\n",
      "|    value_loss         |    0.103  |\n",
      "|    explained_variance |    0.125  |\n",
      "|    n_updates          |    24500  |\n",
      "| aux_train/            |           |\n",
      "|    aux_confidence_mse |    0.006  |\n",
      "| rnd_net_dist/         |           |\n",
      "|    mean_rnd_bonus     |    0.000  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        |   40.000  |\n",
      "|    ep_rew_mean        |    0.330  |\n",
      "|    ep_rew_std         |    0.470  |\n",
      "|    policy_entropy     |    0.597  |\n",
      "|    advantage_mean     |   -0.129  |\n",
      "|    advantage_std      |    0.082  |\n",
      "|    aux_loss_mean      |    0.008  |\n",
      "| time/                 |           |\n",
      "|    fps                |      186  |\n",
      "|    episodes           |    24600  |\n",
      "|    time_elapsed       |     5278  |\n",
      "|    total_timesteps    |   984000  |\n",
      "| train/                |           |\n",
      "|    loss               |   -3.013  |\n",
      "|    policy_loss        |   -2.966  |\n",
      "|    value_loss         |    0.023  |\n",
      "|    explained_variance |   -1.001  |\n",
      "|    n_updates          |    24600  |\n",
      "| aux_train/            |           |\n",
      "|    aux_confidence_mse |    0.008  |\n",
      "| rnd_net_dist/         |           |\n",
      "|    mean_rnd_bonus     |    0.000  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        |   40.000  |\n",
      "|    ep_rew_mean        |    0.360  |\n",
      "|    ep_rew_std         |    0.480  |\n",
      "|    policy_entropy     |    0.610  |\n",
      "|    advantage_mean     |   -0.131  |\n",
      "|    advantage_std      |    0.077  |\n",
      "|    aux_loss_mean      |    0.008  |\n",
      "| time/                 |           |\n",
      "|    fps                |      186  |\n",
      "|    episodes           |    24700  |\n",
      "|    time_elapsed       |     5301  |\n",
      "|    total_timesteps    |   988000  |\n",
      "| train/                |           |\n",
      "|    loss               |   -2.840  |\n",
      "|    policy_loss        |   -2.791  |\n",
      "|    value_loss         |    0.023  |\n",
      "|    explained_variance |   -0.857  |\n",
      "|    n_updates          |    24700  |\n",
      "| aux_train/            |           |\n",
      "|    aux_confidence_mse |    0.009  |\n",
      "| rnd_net_dist/         |           |\n",
      "|    mean_rnd_bonus     |    0.000  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        |   40.000  |\n",
      "|    ep_rew_mean        |    0.430  |\n",
      "|    ep_rew_std         |    0.495  |\n",
      "|    policy_entropy     |    0.600  |\n",
      "|    advantage_mean     |    0.220  |\n",
      "|    advantage_std      |    0.174  |\n",
      "|    aux_loss_mean      |    0.010  |\n",
      "| time/                 |           |\n",
      "|    fps                |      186  |\n",
      "|    episodes           |    24800  |\n",
      "|    time_elapsed       |     5324  |\n",
      "|    total_timesteps    |   992000  |\n",
      "| train/                |           |\n",
      "|    loss               |    4.065  |\n",
      "|    policy_loss        |    4.085  |\n",
      "|    value_loss         |    0.078  |\n",
      "|    explained_variance |    0.162  |\n",
      "|    n_updates          |    24800  |\n",
      "| aux_train/            |           |\n",
      "|    aux_confidence_mse |    0.013  |\n",
      "| rnd_net_dist/         |           |\n",
      "|    mean_rnd_bonus     |    0.000  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        |   40.000  |\n",
      "|    ep_rew_mean        |    0.500  |\n",
      "|    ep_rew_std         |    0.500  |\n",
      "|    policy_entropy     |    0.620  |\n",
      "|    advantage_mean     |    0.199  |\n",
      "|    advantage_std      |    0.160  |\n",
      "|    aux_loss_mean      |    0.011  |\n",
      "| time/                 |           |\n",
      "|    fps                |      186  |\n",
      "|    episodes           |    24900  |\n",
      "|    time_elapsed       |     5348  |\n",
      "|    total_timesteps    |   996000  |\n",
      "| train/                |           |\n",
      "|    loss               |    4.711  |\n",
      "|    policy_loss        |    4.739  |\n",
      "|    value_loss         |    0.064  |\n",
      "|    explained_variance |    0.217  |\n",
      "|    n_updates          |    24900  |\n",
      "| aux_train/            |           |\n",
      "|    aux_confidence_mse |    0.013  |\n",
      "| rnd_net_dist/         |           |\n",
      "|    mean_rnd_bonus     |    0.000  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        |   40.000  |\n",
      "|    ep_rew_mean        |    0.360  |\n",
      "|    ep_rew_std         |    0.480  |\n",
      "|    policy_entropy     |    0.592  |\n",
      "|    advantage_mean     |    0.233  |\n",
      "|    advantage_std      |    0.169  |\n",
      "|    aux_loss_mean      |    0.008  |\n",
      "| time/                 |           |\n",
      "|    fps                |      186  |\n",
      "|    episodes           |    25000  |\n",
      "|    time_elapsed       |     5371  |\n",
      "|    total_timesteps    |  1000000  |\n",
      "| train/                |           |\n",
      "|    loss               |    4.752  |\n",
      "|    policy_loss        |    4.770  |\n",
      "|    value_loss         |    0.082  |\n",
      "|    explained_variance |    0.238  |\n",
      "|    n_updates          |    25000  |\n",
      "| aux_train/            |           |\n",
      "|    aux_confidence_mse |    0.006  |\n",
      "| rnd_net_dist/         |           |\n",
      "|    mean_rnd_bonus     |    0.000  |\n",
      "-------------------------------------\n",
      "Training complete. Total episodes: 25000, total steps: 1000000\n"
     ]
    }
   ],
   "source": [
    "env = MemoryTaskEnv(delay=64, difficulty=1)\n",
    "env = gym.make(\"MarketHiddenRegimeMemory-v0\", n_steps=40)\n",
    "memory = DifferentiableEpisodicMemory(obs_dim=env.observation_space.shape[0], mem_dim=32, max_size=16)\n",
    "#policy = MemoryTransformerPolicy#ExternalMemoryTransformerPolicy  \n",
    "policy = ExternalMemoryTransformerPolicy  \n",
    "aux_modules = [\n",
    "        #CueAuxModule(feat_dim=32, n_classes=3),\n",
    "        ConfidenceAuxModule(feat_dim=32)\n",
    "    ]\n",
    "agent = ExternalMemoryPPO(\n",
    "    policy_class=policy,\n",
    "    use_rnd=True,\n",
    "    env=env,\n",
    "    memory=memory,   \n",
    "    aux_modules=aux_modules,\n",
    "    device=\"cpu\",\n",
    "    learning_rate=1e-4,\n",
    "    her=False,\n",
    "    intrinsic_expl=False,\n",
    "    verbose=1,\n",
    "    reward_norm=False,\n",
    "    ent_coef=0.1\n",
    ")\n",
    "agent.learn(total_timesteps=1_000_000, log_interval=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ef1db623-1116-49a9-8e65-ada567951cfd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.28, 0.4489988864128729)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rews = []\n",
    "for i in range(100):\n",
    "    obs = env.reset()[0]\n",
    "    target = obs[0]\n",
    "    done = False\n",
    "    agent.reset_trajectory()\n",
    "    total_rew = 0\n",
    "    while not done:\n",
    "        \n",
    "        action = agent.predict(obs,deterministic=True)\n",
    "        obs,rew,done,_,_ = env.step(action)\n",
    "        total_rew += rew\n",
    "    rews.append(total_rew)\n",
    "    #print(total_rew)\n",
    "np.mean(rews),np.std(rews)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aef336bd-4825-449b-ae49-e09ee4020dbf",
   "metadata": {},
   "source": [
    "# Benchmark \n",
    "\n",
    "**Agents:**\n",
    "\n",
    "* PPO + MlpPolicy\n",
    "* RecurrentPPO + MlpLstmPolicy\n",
    "* PPO + MemoryTransformerPolicy\n",
    "\n",
    "**Environment**\n",
    "\n",
    "* MemoryTaskEnv\n",
    "    * Delays: 4,32,128 \n",
    "    * Dificulties: easy, hard\n",
    "\n",
    "**Evaluation:**\n",
    "* Eval loop will consist of:\n",
    "    * 50% episodes with target = 1\n",
    "    * 50% episodes with target = 0\n",
    "    * same environment for all agents\n",
    "    * A minimum of 20 episodes\n",
    "* Will return:\n",
    "    * Mean episode reward\n",
    "    * Std episode reward "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cf22684-49ea-4fc3-b31f-98c74bb0b91a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "76d9342c-e675-4b97-b228-de0fea8662a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\filip_a58djhu\\anaconda3\\envs\\trading\\lib\\site-packages\\gym\\envs\\registration.py:307: DeprecationWarning: The package name gym_minigrid has been deprecated in favor of minigrid. Please uninstall gym_minigrid and install minigrid with `pip install minigrid`. Future releases will be maintained under the new package name minigrid.\n",
      "  fn()\n"
     ]
    }
   ],
   "source": [
    "from sb3_contrib import RecurrentPPO\n",
    "from stable_baselines3 import PPO\n",
    "from gymnasium.wrappers import RecordEpisodeStatistics\n",
    "from tqdm import tqdm\n",
    "from tabulate import tabulate\n",
    "\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "class AgentPerformanceBenchmark:\n",
    "    \"\"\"\n",
    "    Benchmark class for standardized evaluation and reporting of agent performance\n",
    "    on memory-based RL tasks. Handles experiment setup, training, evaluation, and result display.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, env_config):\n",
    "        \"\"\"\n",
    "        Initializes the benchmark runner with experiment parameters.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        env_config : dict\n",
    "            Configuration dictionary with environment and experiment parameters.\n",
    "        \"\"\"\n",
    "        self.env_config = env_config\n",
    "        self.env = MemoryTaskEnv(\n",
    "            delay=env_config[\"delay\"],\n",
    "            difficulty=env_config.get(\"difficulty\", 0)\n",
    "        )\n",
    "        self.n_eval_episodes = env_config.get(\"n_eval_episodes\", 20)\n",
    "        self.verbose = env_config.get(\"verbose\", 0)\n",
    "        self.log_interval = env_config.get(\"log_interval\", 10)\n",
    "        self.learning_rate = env_config.get(\"learning_rate\", 1e-3)\n",
    "        self.total_timesteps = env_config.get(\"total_timesteps\", 10000)\n",
    "        self.eval_base = env_config.get(\"eval_base\", False)\n",
    "        self.mode_name = env_config.get(\n",
    "            \"mode_name\", \"EASY\" if env_config.get(\"difficulty\", 0) == 0 else \"HARD\"\n",
    "        )\n",
    "\n",
    "    def print_train_results(self, reward, std, model_name):\n",
    "        \"\"\"\n",
    "        Print formatted summary of evaluation results.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        reward : float\n",
    "            Mean episode reward (percentage, -1.0 to 1.0)\n",
    "        std : float\n",
    "            Standard deviation of episode reward\n",
    "        model_name : str\n",
    "            Name of the agent/model\n",
    "        \"\"\"\n",
    "        print(\n",
    "            f\"[{model_name} @ MemoryTaskEnv with delay={self.env_config['delay']} in {self.mode_name} Mode]  \"\n",
    "            f\"mean_ep_rew: {reward*100:.1f}% .  std_ep_rew: {std:.2f} in {self.n_eval_episodes} episodes\"\n",
    "        )\n",
    "\n",
    "    def evaluate(self, model, model_name, deterministic=True, verbose=False):\n",
    "        \"\"\"\n",
    "        Evaluates an agent over multiple episodes, balancing both target classes.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        model : object\n",
    "            The RL agent (must implement `predict`)\n",
    "        model_name : str\n",
    "            Name for reporting\n",
    "        deterministic : bool\n",
    "            Use deterministic policy for evaluation\n",
    "        verbose : bool\n",
    "            Print result summary if True\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        mean_return : float\n",
    "        std_return : float\n",
    "        \"\"\"\n",
    "        returns = []\n",
    "        target_counter = [0, 0]\n",
    "        eval_complete = False\n",
    "        eval_runs = 0\n",
    "        n_target_samples = int(self.n_eval_episodes / 2)\n",
    "\n",
    "        while not eval_complete:\n",
    "            # Reset model's memory if possible (important for memory agents)\n",
    "            if hasattr(model, \"reset_trajectory\") and callable(getattr(model, \"reset_trajectory\")):\n",
    "                model.reset_trajectory()\n",
    "\n",
    "            obs, _ = self.env.reset()\n",
    "            target = int(obs[0])\n",
    "            eval_runs += 1\n",
    "\n",
    "            # Infinite loop guard\n",
    "            if eval_runs > 1000:\n",
    "                print(\"Warning: Evaluation ran over 1000 attempts, aborting early.\")\n",
    "                break\n",
    "\n",
    "            # Balance classes: skip if this target has enough samples\n",
    "            if target_counter[target] >= n_target_samples:\n",
    "                continue\n",
    "            target_counter[target] += 1\n",
    "\n",
    "            done = False\n",
    "            total_reward = 0.0\n",
    "            \n",
    "            while not done:\n",
    "                \n",
    "                action = model.predict(obs, deterministic=deterministic)\n",
    "                if isinstance(action, tuple):\n",
    "                    action = action[0]\n",
    "                obs, reward, done, _, _ = self.env.step(action)\n",
    "                total_reward += reward\n",
    "                \n",
    "            returns.append(total_reward)\n",
    "            eval_complete = sum(target_counter) >= self.n_eval_episodes\n",
    "\n",
    "        mean_return = np.mean(returns)\n",
    "        std_return = np.std(returns)\n",
    "        if verbose:\n",
    "            self.print_train_results(mean_return, std_return, model_name)\n",
    "        return mean_return, std_return\n",
    "\n",
    "    def run(self):\n",
    "        \"\"\"\n",
    "        Runs the full training and evaluation pipeline for all specified agents.\n",
    "        Returns\n",
    "        -------\n",
    "        results : list of dicts\n",
    "            Each dict contains experiment config and result metrics.\n",
    "        \"\"\"\n",
    "        results = []\n",
    "        print(\n",
    "            f\"\\nTraining in {self.mode_name} mode with delay of {self.env_config['delay']} steps\\n\"\n",
    "        )\n",
    "    \n",
    "        # List of agent configs to iterate through\n",
    "        agents = []\n",
    "        if self.eval_base:\n",
    "            agents.append(('PPO', lambda: PPO(\n",
    "                'MlpPolicy',\n",
    "                RecordEpisodeStatistics(self.env),\n",
    "                learning_rate=self.learning_rate,\n",
    "                verbose=self.verbose\n",
    "            )))\n",
    "            agents.append(('RecurrentPPO', lambda: RecurrentPPO(\n",
    "                \"MlpLstmPolicy\",\n",
    "                RecordEpisodeStatistics(self.env),\n",
    "                verbose=self.verbose,\n",
    "                learning_rate=self.learning_rate\n",
    "            )))\n",
    "        agents.append(('MemoryPPO', lambda: MemoryPPO(\n",
    "            MemoryTransformerPolicy,\n",
    "            self.env,\n",
    "            learning_rate=self.learning_rate,\n",
    "            her=False,\n",
    "            verbose=self.verbose,\n",
    "            aux=False\n",
    "        )))\n",
    "    \n",
    "        # Custom tqdm progress bar for agent training & evaluation\n",
    "        with tqdm(total=len(agents)*2 + 1, desc=\"Benchmark Progress\", unit=\"step\") as pbar:\n",
    "            for agent_name, agent_builder in agents:\n",
    "                # TRAIN\n",
    "                pbar.set_description(f\"Training {agent_name}\")\n",
    "                start_time = time.time()\n",
    "                model = agent_builder()\n",
    "                model.learn(\n",
    "                    total_timesteps=self.total_timesteps,\n",
    "                    log_interval=self.log_interval\n",
    "                )\n",
    "                duration = time.time() - start_time\n",
    "                pbar.update(1)\n",
    "    \n",
    "                # EVALUATE\n",
    "                pbar.set_description(f\"Evaluating {agent_name}\")\n",
    "                mean, std = self.evaluate(model, agent_name, verbose=False)\n",
    "                pbar.update(1)\n",
    "    \n",
    "                results.append({\n",
    "                    **self.env_config,\n",
    "                    'agent': agent_name,\n",
    "                    'mean_return': mean,\n",
    "                    'std_return': std,\n",
    "                    'duration': duration\n",
    "                })\n",
    "    \n",
    "            # Before printing table, update tqdm and close bar\n",
    "            pbar.set_description(\"Finalizing Results\")\n",
    "            pbar.update(1)\n",
    "    \n",
    "        # --- Format & Print Table ---\n",
    "        pdf = pd.DataFrame(results)\n",
    "        pdf = pdf[['agent', 'delay', 'mode_name', 'mean_return', 'std_return', 'duration']]\n",
    "        pdf.rename(\n",
    "            columns={\n",
    "                \"agent\": \"Agent\",\n",
    "                \"delay\": \"Delay\",\n",
    "                \"mode_name\": \"Mode\",\n",
    "                \"mean_return\": \"Mean Ep Rew\",\n",
    "                \"std_return\": \"Std Ep Rew\",\n",
    "                \"duration\": \"Duration (s)\"\n",
    "            },\n",
    "            inplace=True\n",
    "        )\n",
    "        print(tabulate(pdf, headers=\"keys\", tablefmt=\"rounded_outline\"))\n",
    "    \n",
    "        return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2cdedb5d-5a55-4c3f-8309-dfbeefe75b47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training in EASY mode with delay of 4 steps\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training MemoryPPO:  57%|█████▋    | 4/7 [00:33<00:25,  8.44s/step]     \n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "\nModule 'MemoryTransformerPolicy' has no attribute 'aux_modules' (This attribute exists on the Python module, but we failed to convert Python type: 'list' to a TorchScript type. List trace inputs must have elements. Its type was inferred; try adding a type annotation for the attribute.):\n  File \"C:\\Users\\filip_a58djhu\\AppData\\Local\\Temp\\ipykernel_22968\\1245403817.py\", line 158\n        value = self.value_head(feat)\n        aux_preds = {}\n        for aux in self.aux_modules:\n                   ~~~~~~~~~~~~~~~~ <--- HERE\n            aux_preds[aux.name] = aux.head(feat)\n        return logits, value.squeeze(-1), aux_preds\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 64\u001b[0m\n\u001b[0;32m     62\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m exp \u001b[38;5;129;01min\u001b[39;00m EXPERIMENTS:\n\u001b[0;32m     63\u001b[0m     benchmark \u001b[38;5;241m=\u001b[39m AgentPerformanceBenchmark(exp)\n\u001b[1;32m---> 64\u001b[0m     results\u001b[38;5;241m.\u001b[39mappend(\u001b[43mbenchmark\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n",
      "Cell \u001b[1;32mIn[11], line 166\u001b[0m, in \u001b[0;36mAgentPerformanceBenchmark.run\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    164\u001b[0m pbar\u001b[38;5;241m.\u001b[39mset_description(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTraining \u001b[39m\u001b[38;5;132;01m{\u001b[39;00magent_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    165\u001b[0m start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m--> 166\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43magent_builder\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    167\u001b[0m model\u001b[38;5;241m.\u001b[39mlearn(\n\u001b[0;32m    168\u001b[0m     total_timesteps\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtotal_timesteps,\n\u001b[0;32m    169\u001b[0m     log_interval\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlog_interval\n\u001b[0;32m    170\u001b[0m )\n\u001b[0;32m    171\u001b[0m duration \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m start_time\n",
      "Cell \u001b[1;32mIn[11], line 151\u001b[0m, in \u001b[0;36mAgentPerformanceBenchmark.run.<locals>.<lambda>\u001b[1;34m()\u001b[0m\n\u001b[0;32m    139\u001b[0m     agents\u001b[38;5;241m.\u001b[39mappend((\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPPO\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;28;01mlambda\u001b[39;00m: PPO(\n\u001b[0;32m    140\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMlpPolicy\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m    141\u001b[0m         RecordEpisodeStatistics(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv),\n\u001b[0;32m    142\u001b[0m         learning_rate\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlearning_rate,\n\u001b[0;32m    143\u001b[0m         verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverbose\n\u001b[0;32m    144\u001b[0m     )))\n\u001b[0;32m    145\u001b[0m     agents\u001b[38;5;241m.\u001b[39mappend((\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mRecurrentPPO\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;28;01mlambda\u001b[39;00m: RecurrentPPO(\n\u001b[0;32m    146\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMlpLstmPolicy\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    147\u001b[0m         RecordEpisodeStatistics(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv),\n\u001b[0;32m    148\u001b[0m         verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverbose,\n\u001b[0;32m    149\u001b[0m         learning_rate\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlearning_rate\n\u001b[0;32m    150\u001b[0m     )))\n\u001b[1;32m--> 151\u001b[0m agents\u001b[38;5;241m.\u001b[39mappend((\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMemoryPPO\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;28;01mlambda\u001b[39;00m: \u001b[43mMemoryPPO\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    152\u001b[0m \u001b[43m    \u001b[49m\u001b[43mMemoryTransformerPolicy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    153\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    154\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlearning_rate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    155\u001b[0m \u001b[43m    \u001b[49m\u001b[43mher\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    156\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    157\u001b[0m \u001b[43m    \u001b[49m\u001b[43maux\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\n\u001b[0;32m    158\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m))\n\u001b[0;32m    160\u001b[0m \u001b[38;5;66;03m# Custom tqdm progress bar for agent training & evaluation\u001b[39;00m\n\u001b[0;32m    161\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m tqdm(total\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlen\u001b[39m(agents)\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m2\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m, desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBenchmark Progress\u001b[39m\u001b[38;5;124m\"\u001b[39m, unit\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstep\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m pbar:\n",
      "Cell \u001b[1;32mIn[2], line 217\u001b[0m, in \u001b[0;36mMemoryPPO.__init__\u001b[1;34m(self, policy_class, env, verbose, learning_rate, gamma, lam, device, her, reward_norm, aux, intrinsic_expl, intrinsic_eta, ent_coef)\u001b[0m\n\u001b[0;32m    215\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverbose \u001b[38;5;241m=\u001b[39m verbose\n\u001b[0;32m    216\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpolicy \u001b[38;5;241m=\u001b[39m policy_class(obs_dim\u001b[38;5;241m=\u001b[39menv\u001b[38;5;241m.\u001b[39mobservation_space\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m])\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m--> 217\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpolicy \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjit\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscript\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpolicy\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    218\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer \u001b[38;5;241m=\u001b[39m optim\u001b[38;5;241m.\u001b[39mAdam(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpolicy\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39mlearning_rate)\n\u001b[0;32m    219\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining_steps \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\trading\\lib\\site-packages\\torch\\jit\\_script.py:1429\u001b[0m, in \u001b[0;36mscript\u001b[1;34m(obj, optimize, _frames_up, _rcb, example_inputs)\u001b[0m\n\u001b[0;32m   1427\u001b[0m prev \u001b[38;5;241m=\u001b[39m _TOPLEVEL\n\u001b[0;32m   1428\u001b[0m _TOPLEVEL \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m-> 1429\u001b[0m ret \u001b[38;5;241m=\u001b[39m \u001b[43m_script_impl\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1430\u001b[0m \u001b[43m    \u001b[49m\u001b[43mobj\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1431\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptimize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptimize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1432\u001b[0m \u001b[43m    \u001b[49m\u001b[43m_frames_up\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_frames_up\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1433\u001b[0m \u001b[43m    \u001b[49m\u001b[43m_rcb\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_rcb\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1434\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexample_inputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexample_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1435\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1437\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m prev:\n\u001b[0;32m   1438\u001b[0m     log_torchscript_usage(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mscript\u001b[39m\u001b[38;5;124m\"\u001b[39m, model_id\u001b[38;5;241m=\u001b[39m_get_model_id(ret))\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\trading\\lib\\site-packages\\torch\\jit\\_script.py:1147\u001b[0m, in \u001b[0;36m_script_impl\u001b[1;34m(obj, optimize, _frames_up, _rcb, example_inputs)\u001b[0m\n\u001b[0;32m   1145\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(obj, torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mModule):\n\u001b[0;32m   1146\u001b[0m     obj \u001b[38;5;241m=\u001b[39m call_prepare_scriptable_func(obj)\n\u001b[1;32m-> 1147\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjit\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_recursive\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate_script_module\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1148\u001b[0m \u001b[43m        \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjit\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_recursive\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minfer_methods_to_compile\u001b[49m\n\u001b[0;32m   1149\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1150\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1151\u001b[0m     obj \u001b[38;5;241m=\u001b[39m obj\u001b[38;5;241m.\u001b[39m__prepare_scriptable__() \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(obj, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__prepare_scriptable__\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m obj  \u001b[38;5;66;03m# type: ignore[operator]\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\trading\\lib\\site-packages\\torch\\jit\\_recursive.py:557\u001b[0m, in \u001b[0;36mcreate_script_module\u001b[1;34m(nn_module, stubs_fn, share_types, is_tracing)\u001b[0m\n\u001b[0;32m    555\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_tracing:\n\u001b[0;32m    556\u001b[0m     AttributeTypeIsSupportedChecker()\u001b[38;5;241m.\u001b[39mcheck(nn_module)\n\u001b[1;32m--> 557\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcreate_script_module_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnn_module\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconcrete_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstubs_fn\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\trading\\lib\\site-packages\\torch\\jit\\_recursive.py:634\u001b[0m, in \u001b[0;36mcreate_script_module_impl\u001b[1;34m(nn_module, concrete_type, stubs_fn)\u001b[0m\n\u001b[0;32m    632\u001b[0m \u001b[38;5;66;03m# Compile methods if necessary\u001b[39;00m\n\u001b[0;32m    633\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m concrete_type \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m concrete_type_store\u001b[38;5;241m.\u001b[39mmethods_compiled:\n\u001b[1;32m--> 634\u001b[0m     \u001b[43mcreate_methods_and_properties_from_stubs\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    635\u001b[0m \u001b[43m        \u001b[49m\u001b[43mconcrete_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmethod_stubs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mproperty_stubs\u001b[49m\n\u001b[0;32m    636\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    637\u001b[0m     \u001b[38;5;66;03m# Create hooks after methods to ensure no name collisions between hooks and methods.\u001b[39;00m\n\u001b[0;32m    638\u001b[0m     \u001b[38;5;66;03m# If done before, hooks can overshadow methods that aren't exported.\u001b[39;00m\n\u001b[0;32m    639\u001b[0m     create_hooks_from_stubs(concrete_type, hook_stubs, pre_hook_stubs)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\trading\\lib\\site-packages\\torch\\jit\\_recursive.py:466\u001b[0m, in \u001b[0;36mcreate_methods_and_properties_from_stubs\u001b[1;34m(concrete_type, method_stubs, property_stubs)\u001b[0m\n\u001b[0;32m    463\u001b[0m property_defs \u001b[38;5;241m=\u001b[39m [p\u001b[38;5;241m.\u001b[39mdef_ \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m property_stubs]\n\u001b[0;32m    464\u001b[0m property_rcbs \u001b[38;5;241m=\u001b[39m [p\u001b[38;5;241m.\u001b[39mresolution_callback \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m property_stubs]\n\u001b[1;32m--> 466\u001b[0m \u001b[43mconcrete_type\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_create_methods_and_properties\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    467\u001b[0m \u001b[43m    \u001b[49m\u001b[43mproperty_defs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mproperty_rcbs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmethod_defs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmethod_rcbs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmethod_defaults\u001b[49m\n\u001b[0;32m    468\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: \nModule 'MemoryTransformerPolicy' has no attribute 'aux_modules' (This attribute exists on the Python module, but we failed to convert Python type: 'list' to a TorchScript type. List trace inputs must have elements. Its type was inferred; try adding a type annotation for the attribute.):\n  File \"C:\\Users\\filip_a58djhu\\AppData\\Local\\Temp\\ipykernel_22968\\1245403817.py\", line 158\n        value = self.value_head(feat)\n        aux_preds = {}\n        for aux in self.aux_modules:\n                   ~~~~~~~~~~~~~~~~ <--- HERE\n            aux_preds[aux.name] = aux.head(feat)\n        return logits, value.squeeze(-1), aux_preds\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# ---- Batch experiment setup ----\n",
    "if __name__ == \"__main__\":\n",
    "    results = []\n",
    "    EXPERIMENTS = [\n",
    "        dict(\n",
    "            delay=4,\n",
    "            n_train_episodes=2000,\n",
    "            total_timesteps=2000*4,\n",
    "            difficulty=0,\n",
    "            mode_name=\"EASY\",\n",
    "            verbose=0,\n",
    "            eval_base=True\n",
    "        ),\n",
    "        dict(\n",
    "            delay=4,\n",
    "            n_train_episodes=5000,\n",
    "            total_timesteps=5000*4,\n",
    "            difficulty=1,\n",
    "            mode_name=\"HARD\",\n",
    "            verbose=0,\n",
    "            eval_base=True\n",
    "        ),\n",
    "        dict(\n",
    "            delay=32,\n",
    "            n_train_episodes=7500,\n",
    "            total_timesteps=7500*32,\n",
    "            difficulty=0,\n",
    "            mode_name=\"EASY\",\n",
    "            verbose=0,\n",
    "            eval_base=False\n",
    "        ),\n",
    "        dict(\n",
    "            delay=32,\n",
    "            n_train_episodes=10000,\n",
    "            total_timesteps=10000*32,\n",
    "            difficulty=1,\n",
    "            mode_name=\"HARD\",\n",
    "            verbose=0,\n",
    "            eval_base=False\n",
    "        ),\n",
    "        dict(\n",
    "            delay=64,\n",
    "            n_train_episodes=15000,\n",
    "            total_timesteps=15000*64,\n",
    "            difficulty=0,\n",
    "            mode_name=\"HARD\",\n",
    "            verbose=0,\n",
    "            eval_base=False\n",
    "        ),dict(\n",
    "            delay=128,\n",
    "            n_train_episodes=20000,\n",
    "            total_timesteps=20000*128,\n",
    "            difficulty=0,\n",
    "            mode_name=\"HARD\",\n",
    "            verbose=0,\n",
    "            eval_base=False\n",
    "        )]\n",
    "    \n",
    "    for exp in EXPERIMENTS:\n",
    "        benchmark = AgentPerformanceBenchmark(exp)\n",
    "        results.append(benchmark.run())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3d02543-dfa2-45da-9158-a6042fed1563",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "results_nested = results\n",
    "flat_results = [item for sublist in results_nested for item in sublist]\n",
    "\n",
    "df = pd.DataFrame(flat_results)\n",
    "\n",
    "group_keys = df.groupby(['delay', 'difficulty']).groups.keys()\n",
    "grouped = df.groupby(['delay', 'difficulty'])\n",
    "\n",
    "agent_list = sorted(df['agent'].unique())\n",
    "agent_colors = {\n",
    "    agent: color for agent, color in zip(\n",
    "        agent_list,\n",
    "        ['#0072B2', '#009E73', '#D55E00', '#CC79A7', '#F0E442', '#56B4E9']\n",
    "    )\n",
    "}\n",
    "\n",
    "grouped = df.groupby(['delay', 'difficulty'])\n",
    "\n",
    "n_groups = len(grouped)\n",
    "fig, axes = plt.subplots(1, n_groups, figsize=(4*n_groups, 6), sharey=True)\n",
    "\n",
    "if n_groups == 1:\n",
    "    axes = [axes]  \n",
    "\n",
    "for ax, ((delay, difficulty), group) in zip(axes, grouped):\n",
    "    mode = group['mode_name'].iloc[0]\n",
    "    agents = group['agent']\n",
    "    means = group['mean_return']\n",
    "    stds = group['std_return']\n",
    "    \n",
    "    # Get bar colors based on agent names\n",
    "    bar_colors = [agent_colors[agent] for agent in agents]\n",
    "    \n",
    "    bars = ax.bar(\n",
    "        agents,\n",
    "        means,\n",
    "        yerr=stds,\n",
    "        capsize=6,\n",
    "        color=bar_colors,\n",
    "        width=0.7\n",
    "    )\n",
    "    ax.set_title(f\"Delay={delay}\\nMode={mode}\", fontsize=12)\n",
    "    ax.set_xlabel(\"Agent\")\n",
    "    ax.set_ylim(-1.2, 1.2)\n",
    "    ax.axhline(0, color='grey', linestyle='--', linewidth=1)\n",
    "    ax.set_xticks(range(len(agents)))\n",
    "    ax.set_xticklabels(agents, rotation=20)\n",
    "    if ax == axes[0]:\n",
    "        ax.set_ylabel(\"Mean Return\")\n",
    "\n",
    "\n",
    "handles = [plt.Rectangle((0,0),1,1, color=agent_colors[agent]) for agent in agent_list]\n",
    "labels = agent_list\n",
    "fig.legend(handles, labels, title=\"Agent\", loc='lower center', ncol=len(agent_list), bbox_to_anchor=(0.5, -0.02))\n",
    "\n",
    "plt.suptitle(\"Agent Performance per Experiment Group\", fontsize=16, y=1.04)\n",
    "plt.tight_layout()\n",
    "plt.subplots_adjust(bottom=0.18)  \n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29b5d1b9-009c-460d-8cf6-06628fa3577b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df[['delay', 'difficulty', 'mode_name', 'agent', 'mean_return', 'std_return']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb60080d-5142-4556-bba2-6ab1aac45523",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fb0e097-ae8a-44ac-9460-ca3c7daa2f1b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a2ccdbb-610c-4bc6-94c3-dbe5ddf65336",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tabulate import tabulate\n",
    "\n",
    "def print_training_stats(\n",
    "    mean_len, mean_rew, mean_entropy, adv_mean, adv_std, mean_aux,\n",
    "    aux_metrics_log, fps, episodes, elapsed, steps,\n",
    "    loss, policy_loss, value_loss, explained_var\n",
    "):\n",
    "    print(aux_metrics_log)\n",
    "    # Gather rollout stats\n",
    "    rollout_stats = [\n",
    "        [\"ep_len_mean\",      f\"{mean_len:8.2f}\"],\n",
    "        [\"ep_rew_mean\",      f\"{mean_rew:8.2f}\"],\n",
    "        [\"policy_entropy\",   f\"{mean_entropy:8.3f}\"],\n",
    "        [\"advantage_mean\",   f\"{adv_mean:8.3f}\"],\n",
    "        [\"advantage_std\",    f\"{adv_std:8.3f}\"],\n",
    "        [\"aux_loss_mean\",    f\"{mean_aux:8.3f}\"],\n",
    "    ]\n",
    "\n",
    "    # Add aux metrics\n",
    "    for aux_name, metrics in aux_metrics_log.items():\n",
    "        for k, v in metrics.items():\n",
    "            rollout_stats.append([f\"aux_{aux_name}_{k}\", f\"{v:8.4f}\"])\n",
    "\n",
    "    # Time stats\n",
    "    time_stats = [\n",
    "        [\"fps\",            f\"{fps:8d}\"],\n",
    "        [\"episodes\",       f\"{episodes:8d}\"],\n",
    "        [\"time_elapsed\",   f\"{elapsed:8.1f}\"],\n",
    "        [\"total_timesteps\",f\"{steps:8d}\"],\n",
    "    ]\n",
    "    \"\"\"\n",
    "    # Training stats\n",
    "    train_stats = [\n",
    "        [\"loss\",              f\"{loss.item():8.3f}\"],\n",
    "        [\"policy_loss\",       f\"{policy_loss.item():8.3f}\"],\n",
    "        [\"value_loss\",        f\"{value_loss.item():8.3f}\"],\n",
    "        [\"explained_variance\",f\"{explained_var.item():8.3f}\"],\n",
    "        [\"n_updates\",         f\"{episodes:8d}\"],\n",
    "    ]\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\"*20 + \" ROLLOUT \" + \"=\"*20)\n",
    "    print(tabulate(rollout_stats, headers=[\"Metric\", \"Value\"], tablefmt=\"github\"))\n",
    "    print(\"\\n\" + \"=\"*20 + \" TIME \" + \"=\"*22)\n",
    "    print(tabulate(time_stats, headers=[\"Metric\", \"Value\"], tablefmt=\"github\"))\n",
    "    print(\"\\n\" + \"=\"*20 + \" TRAIN \" + \"=\"*21)\n",
    "    #print(tabulate(train_stats, headers=[\"Metric\", \"Value\"], tablefmt=\"github\"))\n",
    "    print(\"=\"*50 + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f31172d-5c0e-4c1d-9ccb-56930a346dc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_sb3_style_log(\n",
    "    mean_len, mean_rew, mean_entropy, adv_mean, adv_std, mean_aux,\n",
    "    aux_metrics_log, fps, episodes, elapsed, steps,\n",
    "    loss, policy_loss, value_loss, explained_var\n",
    "):\n",
    "    print(\"-\" * 38)\n",
    "    print(f\"| rollout/                 |\")\n",
    "    print(f\"|    {'ep_len_mean':<21} | {mean_len:8.2f}\")\n",
    "    print(f\"|    {'ep_rew_mean':<21} | {mean_rew:8.2f}\")\n",
    "    print(f\"|    {'policy_entropy':<21} | {mean_entropy:8.3f}\")\n",
    "    print(f\"|    {'advantage_mean':<21} | {adv_mean:8.3f}\")\n",
    "    print(f\"|    {'advantage_std':<21} | {adv_std:8.3f}\")\n",
    "    print(f\"|    {'aux_loss_mean':<21} | {mean_aux:8.3f}\")\n",
    "    # Print auxiliary metrics if present\n",
    "    for aux_name, metrics in aux_metrics_log.items():\n",
    "        for k, v in metrics.items():\n",
    "            metric_name = f\"aux_{aux_name}_{k}\"\n",
    "            print(f\"|    {metric_name:<21} | {v:8.4f}\")\n",
    "\n",
    "    print(f\"| time/                    |\")\n",
    "    print(f\"|    {'fps':<21} | {fps:8d}\")\n",
    "    print(f\"|    {'episodes':<21} | {episodes:8d}\")\n",
    "    print(f\"|    {'time_elapsed':<21} | {elapsed:8.1f}\")\n",
    "    print(f\"|    {'total_timesteps':<21} | {steps:8d}\")\n",
    "\n",
    "    print(f\"| train/                   |\")\n",
    "    print(f\"|    {'loss':<21} | {loss.item():8.3f}\")\n",
    "    print(f\"|    {'policy_loss':<21} | {policy_loss.item():8.3f}\")\n",
    "    print(f\"|    {'value_loss':<21} | {value_loss.item():8.3f}\")\n",
    "    print(f\"|    {'explained_variance':<21} | {explained_var.item():8.3f}\")\n",
    "    print(f\"|    {'n_updates':<21} | {episodes:8d}\")\n",
    "    print(\"-\" * 38)\n",
    "\n",
    "\"\"\"\n",
    "--------------------------------------\n",
    "| rollout/               |           |\n",
    "|    ep_len_mean         |     8.00  |\n",
    "|    ep_rew_mean         |     0.98  |\n",
    "|    policy_entropy      |    0.563  |\n",
    "|    advantage_mean      |   -1.623  |\n",
    "|    advantage_std       |    0.229  |\n",
    "|    aux_loss_mean       |    0.000  |\n",
    "| time/                  |           |\n",
    "|    fps                 |      281  |\n",
    "|    episodes            |    61600  |\n",
    "|    time_elapsed        |   1749.3  |\n",
    "|    total_timesteps     |   492800  |\n",
    "| train/                 |           |\n",
    "|    loss                |  -14.634  |\n",
    "|    policy_loss         |  -15.917  |\n",
    "|    value_loss          |    2.679  |\n",
    "|    explained_variance  |   -0.068  |\n",
    "|    n_updates           |    61600  |\n",
    "--------------------------------------\n",
    "\"\"\"\n",
    "print('xx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d430964e-7be0-4be5-b416-2bf277a73918",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_sb3_style_log_box(stats):\n",
    "    # Flatten stats for max width calculation\n",
    "    all_rows = []\n",
    "    for section in stats:\n",
    "        all_rows.append((section[\"header\"] + \"/\", None, True))\n",
    "        for k, v in section[\"stats\"].items():\n",
    "            all_rows.append((k, v, False))\n",
    "    # Compute widths\n",
    "    key_width = max(\n",
    "        len(\"    \" + k) if not is_section else len(k)\n",
    "        for k, v, is_section in all_rows\n",
    "    )\n",
    "    val_width = 10\n",
    "    box_width = 2 + key_width + 3 + val_width \n",
    "\n",
    "    def fmt_row(label, value, is_section):\n",
    "        if is_section:\n",
    "            return f\"| {label:<{key_width}}|{' ' * val_width} |\"\n",
    "        else:\n",
    "            # Format value: float = 8.3f, int = 8d, tensor fallback\n",
    "            if hasattr(value, 'item'):\n",
    "                value = value.item()\n",
    "            if isinstance(value, float):\n",
    "                s_value = f\"{value:8.3f}\"\n",
    "            elif isinstance(value, int):\n",
    "                s_value = f\"{value:8d}\"\n",
    "            else:\n",
    "                s_value = str(value)\n",
    "            s_value_centered = f\"{s_value:^{val_width}}\"\n",
    "            return f\"|    {label:<{key_width-4}} |{s_value_centered} |\"\n",
    "\n",
    "    print(\"-\" * box_width)\n",
    "    for section in stats:\n",
    "        print(fmt_row(section[\"header\"] + \"/\", None, True))\n",
    "        for k, v in section[\"stats\"].items():\n",
    "            print(fmt_row(k, v, False))\n",
    "    print(\"-\" * box_width)\n",
    "\n",
    "# Example usage:\n",
    "if __name__ == \"__main__\":\n",
    "    import torch\n",
    "    stats = [{\n",
    "        \"header\":\"rollout\",\n",
    "        \"stats\":dict(\n",
    "            ep_len_mean=8.0,\n",
    "            ep_rew_mean=0.98,\n",
    "            policy_entropy=0.563,\n",
    "            advantage_mean=-1.623,\n",
    "            advantage_std=0.229,\n",
    "            aux_loss_mean=0.0\n",
    "        )},{\n",
    "        \"header\":\"time\",\n",
    "        \"stats\":dict(\n",
    "            fps=281,\n",
    "            episodes=61600,\n",
    "            time_elapsed=1749.3,\n",
    "            total_timesteps=492800,\n",
    "        )},{\n",
    "        \"header\":\"train\",\n",
    "        \"stats\":dict(\n",
    "            loss=torch.tensor(-14.634),\n",
    "            policy_loss=torch.tensor(-15.917),\n",
    "            value_loss=torch.tensor(2.679),\n",
    "            explained_variance=torch.tensor(-0.068),\n",
    "            n_updates=61600\n",
    "        )}\n",
    "    ]\n",
    "    print_sb3_style_log_box(stats)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef75c87c-c3f7-4436-b29f-b8b749bd9a34",
   "metadata": {},
   "source": [
    "# Progress check list\n",
    "\n",
    "## Implemented Features\n",
    "\n",
    "* [x] **GAE (Generalized Advantage Estimation)**\n",
    "* [x] **Reward Normalization**\n",
    "* [x] **Auxiliary Head (cue prediction)**\n",
    "* [x] **Intrinsic State Novelty Bonus**\n",
    "* [x] **HER (Hindsight Experience Replay)**\n",
    "* [x] **Transformer Policy (sequence aware)**\n",
    "* [x] **SB3-style Logging/Diagnostics**\n",
    "* [x] **Clipped Surrogate Objective** *(PPO clip range)*\n",
    "* [x] **Entropy Bonus Tuning**\n",
    "* [x] **Value Function Clipping**\n",
    "* [x] **Observation Normalization**\n",
    "* [x] **Fine-grained Logging** *(KL, grad norm, action dist)*\n",
    "\n",
    "---\n",
    "\n",
    "## Next Tweaks\n",
    "\n",
    "### 1. PPO Core Improvements\n",
    "\n",
    "* [x] **Clipped Surrogate Objective:**\n",
    "  (Implement the PPO `clip_range` for policy ratio.)\n",
    "* [x] **Entropy Bonus Tuning:**\n",
    "  (Different entropy coefficients.)\n",
    "\n",
    "### 2. Policy/Value Stabilization\n",
    "\n",
    "* [x] **Value Function Clipping:**\n",
    "  (Clip value updates for more stability.)\n",
    "\n",
    "### 3. Generalization & Robustness\n",
    "\n",
    "* [x] **Observation Normalization:**\n",
    "  (Normalize obs per feature dimension.)\n",
    "\n",
    "### 4. Training Techniques\n",
    "\n",
    "* [ ] **Curriculum Learning:**\n",
    "  (Ramp up environment delay over training.)\n",
    "* [ ] **KL Penalty / Early Stopping:**\n",
    "  (Monitor KL divergence; adapt learning or stop early.)\n",
    "* [ ] **Multi-Task Heads:**\n",
    "  (Add additional prediction heads, e.g., next-step, event marker.)\n",
    "\n",
    "### 5. Exploration/Intrinsic Motivation\n",
    "\n",
    "* [ ] **Random Network Distillation (RND):**\n",
    "  (Encourage exploring states with high prediction error.)\n",
    "* [ ] **Parameter Noise:**\n",
    "  (Add noise to weights for more diverse policies.)\n",
    "\n",
    "### 6. Diagnostics/Analysis\n",
    "\n",
    "* [ ] **Distributional RL Head:**\n",
    "  (Predict return distribution, not just mean.)\n",
    "\n",
    "---\n",
    "\n",
    "## Real-World Stock/Market Adaptation\n",
    "\n",
    "1. [ ] **Extend Observation/Action Spaces:**\n",
    "   (Use richer features: price, volume, meta-events, etc.)\n",
    "2. [ ] **Contextualize \"Goal\"/HER:**\n",
    "   (E.g., relabel episodes based on \"target\" events or future outcome.)\n",
    "3. [ ] **Event/Pattern Detection Head:**\n",
    "   (Auxiliary head for predicting/flagging event boundaries.)\n",
    "4. [ ] **Longer Trajectories & Robust Memory:**\n",
    "   (Train with variable or longer delays. Use curriculum.)\n",
    "5. [ ] **Batch Training/Efficient Sampling:**\n",
    "   (Switch to multi-episode minibatches for efficiency.)\n",
    "6. [ ] **Backtesting & Market Evaluation:**\n",
    "   (Test generalization to unseen market regimes/periods.)\n",
    "7. [ ] **Regime-Aware Curriculum:**\n",
    "   (Adapt delay, noise, or event frequency according to market regime.)\n",
    "8. [ ] **Add Macro/External Signals:**\n",
    "   (Economic indicators, sector events, etc.)\n",
    "9. [ ] **Production Robustness:**\n",
    "   (Safe action constraints, stable inference, memory persistence.)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b65b27f-8a35-47a7-9b40-b1b5b668f9a4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52852a70-5353-4d72-8808-3293c5d511e8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "id": "c1579a94-ae3a-459f-b19d-2a8b360d9959",
   "metadata": {},
   "source": [
    "## UNDER DEVELOPMENT\n",
    "\n",
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.distributions import Categorical\n",
    "import torch.nn.functional as F\n",
    "import time\n",
    "from collections import defaultdict\n",
    "\n",
    "\n",
    "class ObsNormalizer:\n",
    "    def __init__(self, shape, epsilon=1e-8):\n",
    "        self.mean = np.zeros(shape, dtype=np.float32)\n",
    "        self.var = np.ones(shape, dtype=np.float32)\n",
    "        self.count = 1e-4\n",
    "        self.epsilon = epsilon\n",
    "\n",
    "    def update(self, obs_batch):\n",
    "        obs_batch = np.array(obs_batch)\n",
    "        batch_mean = obs_batch.mean(axis=0)\n",
    "        batch_var = obs_batch.var(axis=0)\n",
    "        batch_count = obs_batch.shape[0]\n",
    "        self.mean = (self.mean * self.count + batch_mean * batch_count) / (self.count + batch_count)\n",
    "        self.var = (self.var * self.count + batch_var * batch_count) / (self.count + batch_count)\n",
    "        self.count += batch_count\n",
    "\n",
    "    def normalize(self, obs):\n",
    "        return (obs - self.mean) / (np.sqrt(self.var) + self.epsilon)\n",
    "        \n",
    "# === 2. Reward Normalizer ===\n",
    "class RewardNormalizer:\n",
    "    def __init__(self, epsilon=1e-8):\n",
    "        self.mean = 0.0\n",
    "        self.var = 1.0\n",
    "        self.count = 1e-4\n",
    "        self.epsilon = epsilon\n",
    "\n",
    "    def update(self, rewards):\n",
    "        rewards = np.array(rewards)\n",
    "        batch_mean = rewards.mean()\n",
    "        batch_var = rewards.var()\n",
    "        batch_count = len(rewards)\n",
    "        self.mean = (self.mean * self.count + batch_mean * batch_count) / (self.count + batch_count)\n",
    "        self.var = (self.var * self.count + batch_var * batch_count) / (self.count + batch_count)\n",
    "        self.count += batch_count\n",
    "\n",
    "    def normalize(self, rewards):\n",
    "        rewards = np.array(rewards)\n",
    "        return ((rewards - self.mean) / (np.sqrt(self.var) + self.epsilon)).tolist()\n",
    "\n",
    "# === 3. State Counter for Intrinsic Reward ===\n",
    "class StateCounter:\n",
    "    def __init__(self):\n",
    "        self.counts = defaultdict(int)\n",
    "    def count(self, obs):\n",
    "        key = tuple(np.round(obs, 2))  # discretize to 2 decimals\n",
    "        self.counts[key] += 1\n",
    "        return self.counts[key]\n",
    "    def intrinsic_reward(self, obs):\n",
    "        c = self.count(obs)\n",
    "        return 1.0 / np.sqrt(c)\n",
    "\n",
    "# === 4. GAE and Explained Variance ===\n",
    "def compute_explained_variance(y_pred, y_true):\n",
    "    var_y = torch.var(y_true)\n",
    "    if var_y == 0:\n",
    "        return torch.tensor(0.0)\n",
    "    return 1 - torch.var(y_true - y_pred) / (var_y + 1e-8)\n",
    "\n",
    "def compute_gae(rewards, values, gamma=0.99, lam=0.95, last_value=0.0):\n",
    "    T = len(rewards)\n",
    "    advantages = torch.zeros(T, dtype=torch.float32, device=values.device)\n",
    "    gae = 0.0\n",
    "    values_ext = torch.cat([values, torch.tensor([last_value], dtype=torch.float32, device=values.device)])\n",
    "    for t in reversed(range(T)):\n",
    "        delta = rewards[t] + gamma * values_ext[t + 1] - values_ext[t]\n",
    "        gae = delta + gamma * lam * gae\n",
    "        advantages[t] = gae\n",
    "    return advantages\n",
    "\n",
    "# === 5. Memory Transformer Policy with Auxiliary Head ===\n",
    "class MemoryTransformerPolicy(nn.Module):\n",
    "    def __init__(self, obs_dim, mem_dim=32, nhead=4):\n",
    "        super().__init__()\n",
    "        self.mem_dim = mem_dim\n",
    "        self.embed = nn.Linear(obs_dim, mem_dim)\n",
    "        self.pos_embed = nn.Embedding(256, mem_dim)\n",
    "        encoder_layer = nn.TransformerEncoderLayer(d_model=mem_dim, nhead=nhead, batch_first=True)\n",
    "        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=1)\n",
    "        self.policy_head = nn.Linear(mem_dim, 2)\n",
    "        self.value_head = nn.Linear(mem_dim, 1)\n",
    "        self.aux_head = nn.Linear(mem_dim, 2)  # Auxiliary: predict initial cue (target)\n",
    "\n",
    "    def forward(self, trajectory):\n",
    "        T = trajectory.shape[0]\n",
    "        x = self.embed(trajectory)\n",
    "        pos = torch.arange(T, device=trajectory.device)\n",
    "        x = x + self.pos_embed(pos)\n",
    "        x = x.unsqueeze(0)\n",
    "        x = self.transformer(x)\n",
    "        feat = x[0, -1]\n",
    "        logits = self.policy_head(feat)\n",
    "        value = self.value_head(feat)\n",
    "        aux_pred = self.aux_head(feat)  # predict cue\n",
    "        return logits, value.squeeze(-1), aux_pred\n",
    "\n",
    "# === 6. HER-enabled MemoryPPO ===\n",
    "class MemoryPPOV2:\n",
    "    def __init__(\n",
    "        self, \n",
    "        policy_class, \n",
    "        env, \n",
    "        verbose=0,\n",
    "        learning_rate=1e-3, \n",
    "        gamma=0.99, \n",
    "        lam=0.95, \n",
    "        device=\"cpu\",\n",
    "        her=True,                 # enable HER\n",
    "        reward_norm=True,         # enable reward normalization\n",
    "        aux=True,                 # enable auxiliary loss\n",
    "        intrinsic_expl=True,      # enable intrinsic exploration\n",
    "        intrinsic_eta=0.05 ,      # intrinsic bonus multiplier\n",
    "        clip_range=0.2,           # PPO clipping parameter\n",
    "        ent_coef=0.01,            # entropy bonus coefficient\n",
    "        vf_clip_param=0.2,        # value function clipping\n",
    "        obs_norm=True             # observation normalization\n",
    "    ):\n",
    "        self.env = env\n",
    "        self.device = torch.device(device)\n",
    "        self.gamma = gamma\n",
    "        self.lam = lam\n",
    "        self.verbose= verbose\n",
    "        self.policy = policy_class(obs_dim=env.observation_space.shape[0]).to(self.device)\n",
    "        self.optimizer = optim.Adam(self.policy.parameters(), lr=learning_rate)\n",
    "        self.training_steps = 0\n",
    "        self.episode_rewards = []\n",
    "        self.episode_lengths = []\n",
    "        self.her = her\n",
    "        self.reward_norm = reward_norm\n",
    "        self.aux = aux\n",
    "        self.intrinsic_expl = intrinsic_expl\n",
    "        self.intrinsic_eta = intrinsic_eta\n",
    "        self.clip_range = clip_range\n",
    "        self.ent_coef = ent_coef\n",
    "        self.vf_clip_param = vf_clip_param\n",
    "        self.obs_norm = obs_norm\n",
    "        self.reward_normalizer = RewardNormalizer()\n",
    "        self.state_counter = StateCounter()\n",
    "        # Internal trajectory buffer \n",
    "        self.trajectory = []\n",
    "        if self.obs_norm:\n",
    "            self.obs_normalizer = ObsNormalizer(env.observation_space.shape[0])\n",
    "            self._obs_norm_fitted = False\n",
    "    \n",
    "    def reset_trajectory(self):\n",
    "        \"\"\"Reset internal trajectory buffer (call after episode or env.reset).\"\"\"\n",
    "        self.trajectory = []\n",
    "        \n",
    "    def normalize_obs(self, obs):\n",
    "        if self.obs_norm:\n",
    "            obs = self.obs_normalizer.normalize(obs)\n",
    "        return obs\n",
    "\n",
    "    def run_episode(self, her_target=None, store_obs_for_norm=False):\n",
    "        obs, _ = self.env.reset()\n",
    "        if her_target is not None:\n",
    "            obs[0] = her_target  # Set initial cue to HER goal\n",
    "\n",
    "        done = False\n",
    "        trajectory = []\n",
    "        rewards = []\n",
    "        actions = []\n",
    "        log_probs = []\n",
    "        values = []\n",
    "        entropies_ep = []\n",
    "        aux_preds = []\n",
    "        obs_history = []\n",
    "        t = 0\n",
    "\n",
    "        initial_cue = int(obs[0])  # ground-truth for aux head\n",
    "\n",
    "        while not done:\n",
    "            obs_normed = self.normalize_obs(obs)\n",
    "            obs_t = torch.tensor(obs_normed, dtype=torch.float32, device=self.device)\n",
    "            trajectory.append(obs_t)\n",
    "            traj = torch.stack(trajectory)\n",
    "            logits, value, aux_pred = self.policy(traj)\n",
    "            dist = Categorical(logits=logits)\n",
    "            action = dist.sample()\n",
    "            log_prob = dist.log_prob(action)\n",
    "            entropy = dist.entropy()\n",
    "            obs, reward, done, _, _ = self.env.step(action.item())\n",
    "            if store_obs_for_norm:\n",
    "                obs_history.append(obs)\n",
    "            # Intrinsic reward\n",
    "            if self.intrinsic_expl:\n",
    "                reward += self.intrinsic_eta * self.state_counter.intrinsic_reward(obs)\n",
    "            actions.append(action)\n",
    "            log_probs.append(log_prob)\n",
    "            rewards.append(torch.tensor(reward, dtype=torch.float32, device=self.device))\n",
    "            values.append(value)\n",
    "            entropies_ep.append(entropy)\n",
    "            aux_preds.append(aux_pred)\n",
    "            t += 1\n",
    "        return {\n",
    "            \"trajectory\": trajectory,\n",
    "            \"actions\": actions,\n",
    "            \"rewards\": rewards,\n",
    "            \"log_probs\": log_probs,\n",
    "            \"values\": values,\n",
    "            \"entropies\": entropies_ep,\n",
    "            \"aux_preds\": aux_preds,\n",
    "            \"initial_cue\": initial_cue,\n",
    "            \"obs_history\": obs_history\n",
    "        }\n",
    "\n",
    "    def learn(self, total_timesteps=2000, log_interval=100):\n",
    "        steps = 0\n",
    "        episodes = 0\n",
    "        all_returns = []\n",
    "        start_time = time.time()\n",
    "        entropies = []\n",
    "        advantages_stats = []\n",
    "        aux_losses = []\n",
    "        grad_norms = []\n",
    "        kl_divs = []\n",
    "        action_dist_list = []\n",
    "\n",
    "        # ---- Pre-fill obs normalizer ----\n",
    "        if self.obs_norm:\n",
    "            all_obs = []\n",
    "            for _ in range(10):\n",
    "                ep = self.run_episode(store_obs_for_norm=True)\n",
    "                all_obs.extend(ep[\"obs_history\"])\n",
    "            self.obs_normalizer.update(all_obs)\n",
    "\n",
    "        old_policy_params = [p.clone().detach() for p in self.policy.parameters()]\n",
    "\n",
    "        while steps < total_timesteps:\n",
    "            # ---- Run normal episode ----\n",
    "            episode = self.run_episode()\n",
    "            trajectory = episode[\"trajectory\"]\n",
    "            actions = episode[\"actions\"]\n",
    "            rewards = episode[\"rewards\"]\n",
    "            log_probs = episode[\"log_probs\"]\n",
    "            values = episode[\"values\"]\n",
    "            entropies_ep = episode[\"entropies\"]\n",
    "            aux_preds = episode[\"aux_preds\"]\n",
    "            initial_cue = episode[\"initial_cue\"]\n",
    "         \n",
    "            # ---- Reward normalization ----\n",
    "            if self.reward_norm:\n",
    "                self.reward_normalizer.update([r.item() for r in rewards])\n",
    "                rewards = [torch.tensor(rn, dtype=torch.float32, device=self.device)\n",
    "                           for rn in self.reward_normalizer.normalize([r.item() for r in rewards])]\n",
    "\n",
    "            # ---- HER ----\n",
    "            if self.her:\n",
    "                her_target = int(actions[-1].item())\n",
    "                her_episode = self.run_episode(her_target=her_target)\n",
    "                for k in [\"trajectory\", \"actions\", \"rewards\", \"log_probs\", \"values\", \"entropies\", \"aux_preds\"]:\n",
    "                    episode[k] += her_episode[k]\n",
    "                initial_cue = [initial_cue, her_target]\n",
    "            else:\n",
    "                initial_cue = [initial_cue]\n",
    "\n",
    "            # ---- Batchify for loss ----\n",
    "            trajectory = episode[\"trajectory\"]\n",
    "            actions = episode[\"actions\"]\n",
    "            rewards = episode[\"rewards\"]\n",
    "            log_probs = episode[\"log_probs\"]\n",
    "            values = episode[\"values\"]\n",
    "            entropies_ep = episode[\"entropies\"]\n",
    "            aux_preds = episode[\"aux_preds\"]\n",
    "            T = len(rewards)\n",
    "\n",
    "            rewards_t = torch.stack(rewards)\n",
    "            values_t = torch.stack(values)\n",
    "            log_probs_t = torch.stack(log_probs)\n",
    "            actions_t = torch.stack(actions)\n",
    "            aux_preds_t = torch.stack(aux_preds)\n",
    "            last_value = 0.0  # Value for last state (terminal)\n",
    "            advantages = compute_gae(rewards_t, values_t, gamma=self.gamma, lam=self.lam, last_value=last_value)\n",
    "            returns = advantages + values_t.detach()\n",
    "\n",
    "            # ---- Old log probs for PPO ratio ----\n",
    "            with torch.no_grad():\n",
    "                # Re-run old policy on same trajectory for ratio\n",
    "                old_logits, _, _ = self.policy(torch.stack(trajectory).detach())\n",
    "                old_dist = Categorical(logits=old_logits)\n",
    "                old_log_probs_t = old_dist.log_prob(actions_t)\n",
    "\n",
    "            # ---- PPO Clipped Surrogate Objective ----\n",
    "            ratio = torch.exp(log_probs_t - old_log_probs_t)\n",
    "            surr1 = ratio * advantages.detach()\n",
    "            surr2 = torch.clamp(ratio, 1.0 - self.clip_range, 1.0 + self.clip_range) * advantages.detach()\n",
    "            policy_loss = -torch.min(surr1, surr2).mean()\n",
    "\n",
    "            # ---- Value Function Clipping ----\n",
    "            values_pred_clipped = values_t + (values_t.detach() - values_t).clamp(-self.vf_clip_param, self.vf_clip_param)\n",
    "            value_losses = (values_t - returns).pow(2)\n",
    "            value_losses_clipped = (values_pred_clipped - returns).pow(2)\n",
    "            value_loss = 0.5 * torch.max(value_losses, value_losses_clipped).mean()\n",
    "\n",
    "            entropy_mean = torch.stack(entropies_ep).mean()\n",
    "            explained_var = compute_explained_variance(values_t, returns)\n",
    "\n",
    "            # --- Auxiliary loss: cue prediction (classification) ---\n",
    "            aux_loss = torch.tensor(0.0, device=self.device)\n",
    "            if self.aux:\n",
    "                if self.her:\n",
    "                    cues = torch.tensor(initial_cue, dtype=torch.long, device=self.device) \n",
    "                    aux_preds_to_use = torch.stack([aux_preds_t[0], aux_preds_t[T // 2]])\n",
    "                else:\n",
    "                    cues = torch.tensor([initial_cue[0]], dtype=torch.long, device=self.device) \n",
    "                    aux_preds_to_use = aux_preds_t[0].unsqueeze(0)\n",
    "                assert aux_preds_to_use.shape[0] == cues.shape[0], f\"Shape mismatch: preds {aux_preds_to_use.shape}, cues {cues.shape}\"\n",
    "                aux_loss = F.cross_entropy(aux_preds_to_use, cues)\n",
    "                aux_losses.append(aux_loss.item())\n",
    "\n",
    "            # ---- KL divergence (current vs. old policy) ----\n",
    "            kl_div = (old_log_probs_t - log_probs_t).mean().item()\n",
    "            kl_divs.append(kl_div)\n",
    "\n",
    "            # ---- Action distribution (diagnostics) ----\n",
    "            probs = torch.softmax(old_logits, dim=-1).detach().cpu().numpy()\n",
    "            action_dist_list.append(probs.mean(axis=0))\n",
    "\n",
    "            # --- Total loss ---\n",
    "            loss = policy_loss + value_loss + 0.1 * aux_loss - self.ent_coef * entropy_mean\n",
    "\n",
    "            self.optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "\n",
    "            # --- Gradient Norm Logging ---\n",
    "            total_norm = 0.0\n",
    "            for p in self.policy.parameters():\n",
    "                if p.grad is not None:\n",
    "                    param_norm = p.grad.data.norm(2)\n",
    "                    total_norm += param_norm.item() ** 2\n",
    "            total_norm = total_norm ** 0.5\n",
    "            grad_norms.append(total_norm)\n",
    "\n",
    "            self.optimizer.step()\n",
    "\n",
    "            total_reward = sum([r.item() for r in rewards])\n",
    "            all_returns.append(total_reward)\n",
    "            self.episode_rewards.append(total_reward)\n",
    "            self.episode_lengths.append(T)\n",
    "            episodes += 1\n",
    "            steps += T\n",
    "\n",
    "            # SB3-like logging\n",
    "            if episodes % log_interval == 0 and self.verbose == 1:\n",
    "                elapsed = time.time() - start_time\n",
    "                mean_rew = np.mean(self.episode_rewards[-log_interval:])\n",
    "                mean_len = np.mean(self.episode_lengths[-log_interval:])\n",
    "                fps = int(steps / (elapsed + 1e-8))\n",
    "                adv_mean = advantages.mean().item()\n",
    "                adv_std = advantages.std().item()\n",
    "                mean_entropy = entropy_mean.item()\n",
    "                mean_aux = np.mean(aux_losses[-log_interval:]) if aux_losses else 0.0\n",
    "                mean_kl = np.mean(kl_divs[-log_interval:]) if kl_divs else 0.0\n",
    "                mean_grad = np.mean(grad_norms[-log_interval:]) if grad_norms else 0.0\n",
    "                mean_act_dist = np.mean(np.array(action_dist_list[-log_interval:]), axis=0) if action_dist_list else None\n",
    "\n",
    "                print(\"-\" * 40)\n",
    "                print(f\"| rollout/               |\")\n",
    "                print(f\"|    ep_len_mean         | {mean_len:8.2f}\")\n",
    "                print(f\"|    ep_rew_mean         | {mean_rew:8.2f}\")\n",
    "                print(f\"|    policy_entropy      | {mean_entropy:8.3f}\")\n",
    "                print(f\"|    advantage_mean      | {adv_mean:8.3f}\")\n",
    "                print(f\"|    advantage_std       | {adv_std:8.3f}\")\n",
    "                print(f\"|    aux_loss_mean       | {mean_aux:8.3f}\")\n",
    "                print(f\"| time/                  |\")\n",
    "                print(f\"|    fps                 | {fps:8d}\")\n",
    "                print(f\"|    episodes            | {episodes:8d}\")\n",
    "                print(f\"|    time_elapsed        | {elapsed:8.1f}\")\n",
    "                print(f\"|    total_timesteps     | {steps:8d}\")\n",
    "                print(f\"| train/                 |\")\n",
    "                print(f\"|    loss                | {loss.item():8.3f}\")\n",
    "                print(f\"|    policy_loss         | {policy_loss.item():8.3f}\")\n",
    "                print(f\"|    value_loss          | {value_loss.item():8.3f}\")\n",
    "                print(f\"|    explained_variance  | {explained_var.item():8.3f}\")\n",
    "                print(f\"|    kl_div              | {mean_kl:8.5f}\")\n",
    "                print(f\"|    grad_norm           | {mean_grad:8.3f}\")\n",
    "                if mean_act_dist is not None:\n",
    "                    # Ensure mean_act_dist is iterable (vector of probabilities)\n",
    "                    if np.isscalar(mean_act_dist):\n",
    "                        print(f\"|    action_dist         | [{float(mean_act_dist):.3f}]\")\n",
    "                    else:\n",
    "                        print(f\"|    action_dist         | {[float(f'{a:.3f}') for a in mean_act_dist]}\")\n",
    "                print(f\"|    n_updates           | {episodes:8d}\")\n",
    "                print(\"-\" * 40)\n",
    "        print(f\"Training complete. Total episodes: {episodes}, total steps: {steps}\")\n",
    "    \n",
    "    def predict(self, obs, deterministic=False, done=False):\n",
    "        \"\"\"\n",
    "        Append obs to trajectory buffer and predict action from full sequence.\n",
    "        Optionally, reset buffer if done=True (e.g. after episode ends).\n",
    "        \"\"\"\n",
    "        obs_t = torch.tensor(obs, dtype=torch.float32, device=self.device)\n",
    "        self.trajectory.append(obs_t)\n",
    "        traj = torch.stack(self.trajectory)\n",
    "        self.policy.eval()\n",
    "        with torch.no_grad():\n",
    "            logits, _, _ = self.policy(traj)\n",
    "            if deterministic:\n",
    "                action = torch.argmax(logits).item()\n",
    "            else:\n",
    "                dist = Categorical(logits=logits)\n",
    "                action = dist.sample().item()\n",
    "        self.policy.train()\n",
    "        # If episode ended, clear buffer for next episode\n",
    "        if done:\n",
    "            self.reset_trajectory()\n",
    "        return action\n",
    "\n",
    "    def save(self, path=\"memoryppo.pt\"):\n",
    "        torch.save(self.policy.state_dict(), path)\n",
    "\n",
    "    def load(self, path=\"memoryppo.pt\"):\n",
    "        self.policy.load_state_dict(torch.load(path, map_location=self.device))\n",
    "\n",
    "    def evaluate(self, n_episodes=10, deterministic=False, verbose=True):\n",
    "        returns = []\n",
    "        for _ in range(n_episodes):\n",
    "            obs, _ = self.env.reset()\n",
    "            self.reset_trajectory()\n",
    "            done = False\n",
    "            total_reward = 0.0\n",
    "            while not done:\n",
    "                action = self.predict(obs, deterministic=deterministic)\n",
    "                obs, reward, done, _, _ = self.env.step(action)\n",
    "                total_reward += reward\n",
    "                # Optionally: auto-reset trajectory if done\n",
    "                if done:\n",
    "                    self.reset_trajectory()\n",
    "            returns.append(total_reward)\n",
    "        mean_return = np.mean(returns)\n",
    "        std_return = np.std(returns)\n",
    "        if verbose:\n",
    "            print(f\"Evaluation over {n_episodes} episodes: mean return {mean_return:.2f}, std {std_return:.2f}\")\n",
    "        return mean_return, std_return\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:trading]",
   "language": "python",
   "name": "conda-env-trading-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
