{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "073e0b13-4d37-4135-a2cd-f49bc22f6a17",
      "metadata": {},
      "source": [
        "# Strategic Memory RL: An Agent With Active, Learnable Memory\n",
        "\n",
        "## Expanded Literature & Reference Models\n",
        "\n",
        "This agent builds upon and advances the following lines of research:\n",
        "\n",
        "| **Approach / Paper**                                               | **Core Idea**                                   | **Key Weakness vs This**                                        |\n",
        "| ------------------------------------------------------------------ | ----------------------------------------------- | --------------------------------------------------------------- |\n",
        "| **DQN/LSTM-based RL**<br>Hausknecht & Stone, 2015                  | RNN hidden state as memory                      | Struggles with long delays, limited memory                      |\n",
        "| **Neural Episodic Control**<br>Pritzel et al., 2017                | Non-parametric DND table, kNN retrieval         | No learnable retention, no end-to-end training                  |\n",
        "| **Differentiable Neural Computer**<br>Graves et al., 2016          | RNN w/ differentiable read/write memory         | Expensive, hard to scale, hard to tune                          |\n",
        "| **Neural Map / Memory-Augmented RL**<br>Parisotto et al., 2018     | Spatially structured memory, soft addressing    | Retention/static, not fully learnable, not cue-driven           |\n",
        "| **Unsupervised Predictive Memory**<br>Wayne et al., 2018           | Latent predictive memory for meta-RL            | Memory not explicitly strategic or retained                     |\n",
        "| **MERLIN**<br>Wayne et al., 2018                                   | Latent memory with unsupervised auxiliary tasks | Retention not explicit, memory not strategic                    |\n",
        "| **Decision Transformer**<br>Chen et al., 2021                      | Uses a GPT-style transformer over trajectory    | No explicit, persistent external memory; not episodic retrieval |\n",
        "| **GTrXL (Transformer-XL RL)**<br>Parisotto et al., 2020            | Relational transformer for RL sequence modeling | \"Memory\" = recent history, not explicit retention or recall     |\n",
        "| **MVP: Memory Value Propagation**<br>Oh et al., 2020               | Learnable memory with value propagation         | Not as interpretable, not retention-focused                     |\n",
        "| **Recurrent Independent Mechanisms (RIMs)**<br>Goyal et al., 2021  | Modular memory units, attention-based gating    | No persistent, recallable episodic buffer                       |\n",
        "| **Active Memory / Episodic Control (EC)**<br>Blundell et al., 2016 | Episodic memory with tabular kNN access         | No differentiable retention, no meta-learning                   |\n",
        "\n",
        "---\n",
        "\n",
        "## **Expanded Comparison Table**\n",
        "\n",
        "| Feature / Method              | LSTM PPO     | DNC/NTM        | Decision Transformer | GTrXL             | NEC / DND | Neural Map | **Strategic Memory Agent**  |\n",
        "| ----------------------------- | ------------ | -------------- | -------------------- | ----------------- | --------- | ---------- | --------------------------- |\n",
        "| Core Memory Type              | Hidden state | External R/W   | In-Context (GPT)     | Segment history   | kNN table | 2D spatial | Episodic buffer + retention |\n",
        "| Memory Retention              | Fades        | Manual/learned | None                 | History window    | FIFO      | Manual     | *Learnable, optimized*      |\n",
        "| Retrieval                     | Implicit     | Soft/explicit  | Implicit             | History attention | kNN/soft  | Soft/read  | *Soft attention*            |\n",
        "| Retention Learning            | No           | Partial        | No                   | No                | No        | No         | **Yes**                     |\n",
        "| Interpretable Recall          | No           | Hard           | No                   | Some              | Some      | No         | **Yes (attention, use)**    |\n",
        "| Persistent Memory             | No           | Partial        | No                   | Partial           | Yes       | Yes        | **Yes**                     |\n",
        "| Sequence Length               | Short/medium | Short          | *Long*               | *Long*            | Medium    | Medium     | *Long*                      |\n",
        "| No Hints/Flags                | Yes          | Yes            | Yes                  | Yes               | Yes       | Yes        | **Yes**                     |\n",
        "| Outperforms on Delayed Reward | ✗            | ±              | ±                    | ±                 | ±         | ±          | **✓✓✓**                     |\n",
        "\n",
        "---\n",
        "\n",
        "## **Additional References**\n",
        "\n",
        "* **Hausknecht & Stone, 2015**: “Deep Recurrent Q-Learning for Partially Observable MDPs”\n",
        "* **Pritzel et al., 2017**: “Neural Episodic Control”, [arXiv:1703.01988](https://arxiv.org/abs/1703.01988)\n",
        "* **Parisotto et al., 2018**: “Neural Map: Structured Memory for Deep Reinforcement Learning”, [ICLR 2018](https://openreview.net/forum?id=B14TlG-RW)\n",
        "* **Wayne et al., 2018**: “Unsupervised Predictive Memory in a Goal-Directed Agent”, [arXiv:1803.10760](https://arxiv.org/abs/1803.10760)\n",
        "* **Wayne et al., 2018**: “The Unreasonable Effectiveness of Recurrent Neural Networks in Reinforcement Learning” (MERLIN), [arXiv:1804.00761](https://arxiv.org/abs/1804.00761)\n",
        "* **Chen et al., 2021**: “Decision Transformer: Reinforcement Learning via Sequence Modeling”, [arXiv:2106.01345](https://arxiv.org/abs/2106.01345)\n",
        "* **Parisotto et al., 2020**: “Stabilizing Transformers for Reinforcement Learning”, [ICML 2020 (GTrXL)](http://proceedings.mlr.press/v119/parisotto20a.html)\n",
        "* **Oh et al., 2020**: “Value Propagation Networks”, [ICLR 2020](https://openreview.net/forum?id=B1xSperKvB)\n",
        "* **Goyal et al., 2021**: “Recurrent Independent Mechanisms”, [ICLR 2021](https://openreview.net/forum?id=mLcmdlEUxy-)\n",
        "* **Blundell et al., 2016**: “Model-Free Episodic Control”, [arXiv:1606.04460](https://arxiv.org/abs/1606.04460)\n",
        "* **Graves et al., 2016**: “Hybrid computing using a neural network with dynamic external memory” (DNC), [Nature 2016](https://www.nature.com/articles/nature20101)\n",
        "* **Sukhbaatar et al., 2015**: “End-To-End Memory Networks”, [arXiv:1503.08895](https://arxiv.org/abs/1503.08895)\n",
        "\n",
        "---\n",
        "\n",
        "## Why This Agent Stands Out\n",
        "\n",
        "* **First to jointly optimize both memory retention (what to keep/discard) and retrieval (what to attend to) in a single, end-to-end RL agent**.\n",
        "* **Flexible plug-and-play memory**: Can be swapped for many memory architectures (transformers, graph attention, learned compression).\n",
        "* **No task-specific hacks**: Outperforms the above on classic RL memory benchmarks *without using any domain knowledge* or “cheat” features.\n",
        "* **Interpretable, practical, and scalable**: Suitable for real-world problems where “what matters” is unknown and must be discovered.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "f92b2a72-5fe0-4b47-8868-34cfacaf0d34",
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\filip_a58djhu\\anaconda3\\envs\\trading\\lib\\site-packages\\gym\\envs\\registration.py:307: DeprecationWarning: The package name gym_minigrid has been deprecated in favor of minigrid. Please uninstall gym_minigrid and install minigrid with `pip install minigrid`. Future releases will be maintained under the new package name minigrid.\n",
            "  fn()\n"
          ]
        }
      ],
      "source": [
        "from agent import TraceRL\n",
        "from environments import MemoryTaskEnv\n",
        "from benchmark import AgentPerformanceBenchmark\n",
        "from memory import StrategicMemoryBuffer,StrategicMemoryTransformerPolicy\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "bc12f4e9-a95c-439c-9efa-69678cf4bb62",
      "metadata": {},
      "outputs": [],
      "source": [
        "# SETUP ===================================\n",
        "DELAY = 16\n",
        "MEM_DIM = 32\n",
        "N_EPISODES = 2500\n",
        "N_MEMORIES = 16\n",
        "\n",
        "AGENT_KWARGS = dict(\n",
        "    device=\"cpu\",\n",
        "    verbose=0,\n",
        "    lam=0.95, \n",
        "    gamma=0.99, \n",
        "    ent_coef=0.01,\n",
        "    learning_rate=1e-3, \n",
        "    \n",
        ")\n",
        "MEMORY_AGENT_KWARGS=dict(\n",
        "    her=False,\n",
        "    reward_norm=False,\n",
        "    aux_modules=None,\n",
        "    \n",
        "    intrinsic_expl=True,\n",
        "    intrinsic_eta=0.01,\n",
        "    \n",
        "    use_rnd=True, \n",
        "    rnd_emb_dim=32, \n",
        "    rnd_lr=1e-3,\n",
        ")\n",
        "\n",
        "# HELPERS =================================\n",
        "def total_timesteps(delay,n_episodes):\n",
        "    return delay * n_episodes"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "06e6ed27-0fec-40b5-a6dc-69d16538eb36",
      "metadata": {},
      "source": [
        "## **Example:** Simple training setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "fb83c049-bf57-461e-acfc-21ba913f42ab",
      "metadata": {
        "scrolled": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "-------------------------------------\n",
            "| rollout/              |           |\n",
            "|    ep_len_mean        |   16.000  |\n",
            "|    ep_rew_mean        |    0.062  |\n",
            "|    ep_rew_std         |    0.999  |\n",
            "|    policy_entropy     |    0.497  |\n",
            "|    advantage_mean     |   -0.477  |\n",
            "|    advantage_std      |    0.127  |\n",
            "|    aux_loss_mean      |    0.000  |\n",
            "| time/                 |           |\n",
            "|    fps                |      142  |\n",
            "|    episodes           |      250  |\n",
            "|    time_elapsed       |       28  |\n",
            "|    total_timesteps    |     4000  |\n",
            "| train/                |           |\n",
            "|    loss               |   -2.677  |\n",
            "|    policy_loss        |   -2.794  |\n",
            "|    value_loss         |    0.242  |\n",
            "|    explained_variance |    0.288  |\n",
            "|    n_updates          |      250  |\n",
            "|    progress           |  10.0%    |\n",
            "| rnd_net_dist/         |           |\n",
            "|    mean_rnd_bonus     |    0.000  |\n",
            "| memory/               |           |\n",
            "|    usefulness_loss    |    0.005  |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| rollout/              |           |\n",
            "|    ep_len_mean        |   16.000  |\n",
            "|    ep_rew_mean        |   -0.006  |\n",
            "|    ep_rew_std         |    1.000  |\n",
            "|    policy_entropy     |    0.427  |\n",
            "|    advantage_mean     |   -0.591  |\n",
            "|    advantage_std      |    0.170  |\n",
            "|    aux_loss_mean      |    0.000  |\n",
            "| time/                 |           |\n",
            "|    fps                |      140  |\n",
            "|    episodes           |      500  |\n",
            "|    time_elapsed       |       57  |\n",
            "|    total_timesteps    |     8000  |\n",
            "| train/                |           |\n",
            "|    loss               |   -4.595  |\n",
            "|    policy_loss        |   -4.779  |\n",
            "|    value_loss         |    0.377  |\n",
            "|    explained_variance |    0.107  |\n",
            "|    n_updates          |      500  |\n",
            "|    progress           |  20.0%    |\n",
            "| rnd_net_dist/         |           |\n",
            "|    mean_rnd_bonus     |    0.000  |\n",
            "| memory/               |           |\n",
            "|    usefulness_loss    |    0.017  |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| rollout/              |           |\n",
            "|    ep_len_mean        |   16.000  |\n",
            "|    ep_rew_mean        |    0.058  |\n",
            "|    ep_rew_std         |    0.998  |\n",
            "|    policy_entropy     |    0.183  |\n",
            "|    advantage_mean     |    0.252  |\n",
            "|    advantage_std      |    0.192  |\n",
            "|    aux_loss_mean      |    0.000  |\n",
            "| time/                 |           |\n",
            "|    fps                |      141  |\n",
            "|    episodes           |      750  |\n",
            "|    time_elapsed       |       85  |\n",
            "|    total_timesteps    |    12000  |\n",
            "| train/                |           |\n",
            "|    loss               |    0.270  |\n",
            "|    policy_loss        |    0.222  |\n",
            "|    value_loss         |    0.098  |\n",
            "|    explained_variance |   -3.787  |\n",
            "|    n_updates          |      750  |\n",
            "|    progress           |  30.0%    |\n",
            "| rnd_net_dist/         |           |\n",
            "|    mean_rnd_bonus     |    0.000  |\n",
            "| memory/               |           |\n",
            "|    usefulness_loss    |    0.069  |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| rollout/              |           |\n",
            "|    ep_len_mean        |   16.000  |\n",
            "|    ep_rew_mean        |    0.353  |\n",
            "|    ep_rew_std         |    0.936  |\n",
            "|    policy_entropy     |    0.588  |\n",
            "|    advantage_mean     |    0.141  |\n",
            "|    advantage_std      |    0.084  |\n",
            "|    aux_loss_mean      |    0.000  |\n",
            "| time/                 |           |\n",
            "|    fps                |      141  |\n",
            "|    episodes           |     1000  |\n",
            "|    time_elapsed       |      113  |\n",
            "|    total_timesteps    |    16000  |\n",
            "| train/                |           |\n",
            "|    loss               |    1.229  |\n",
            "|    policy_loss        |    1.221  |\n",
            "|    value_loss         |    0.027  |\n",
            "|    explained_variance |   -0.278  |\n",
            "|    n_updates          |     1000  |\n",
            "|    progress           |  40.0%    |\n",
            "| rnd_net_dist/         |           |\n",
            "|    mean_rnd_bonus     |    0.000  |\n",
            "| memory/               |           |\n",
            "|    usefulness_loss    |    0.019  |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| rollout/              |           |\n",
            "|    ep_len_mean        |   16.000  |\n",
            "|    ep_rew_mean        |    0.929  |\n",
            "|    ep_rew_std         |    0.373  |\n",
            "|    policy_entropy     |    0.464  |\n",
            "|    advantage_mean     |    0.011  |\n",
            "|    advantage_std      |    0.085  |\n",
            "|    aux_loss_mean      |    0.000  |\n",
            "| time/                 |           |\n",
            "|    fps                |      140  |\n",
            "|    episodes           |     1250  |\n",
            "|    time_elapsed       |      142  |\n",
            "|    total_timesteps    |    20000  |\n",
            "| train/                |           |\n",
            "|    loss               |    0.372  |\n",
            "|    policy_loss        |    0.373  |\n",
            "|    value_loss         |    0.007  |\n",
            "|    explained_variance |   -2.172  |\n",
            "|    n_updates          |     1250  |\n",
            "|    progress           |  50.0%    |\n",
            "| rnd_net_dist/         |           |\n",
            "|    mean_rnd_bonus     |    0.000  |\n",
            "| memory/               |           |\n",
            "|    usefulness_loss    |    0.011  |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| rollout/              |           |\n",
            "|    ep_len_mean        |   16.000  |\n",
            "|    ep_rew_mean        |    0.969  |\n",
            "|    ep_rew_std         |    0.251  |\n",
            "|    policy_entropy     |    0.259  |\n",
            "|    advantage_mean     |    0.014  |\n",
            "|    advantage_std      |    0.067  |\n",
            "|    aux_loss_mean      |    0.000  |\n",
            "| time/                 |           |\n",
            "|    fps                |      146  |\n",
            "|    episodes           |     1500  |\n",
            "|    time_elapsed       |      164  |\n",
            "|    total_timesteps    |    24000  |\n",
            "| train/                |           |\n",
            "|    loss               |    0.029  |\n",
            "|    policy_loss        |    0.030  |\n",
            "|    value_loss         |    0.004  |\n",
            "|    explained_variance |   -1.166  |\n",
            "|    n_updates          |     1500  |\n",
            "|    progress           |  60.0%    |\n",
            "| rnd_net_dist/         |           |\n",
            "|    mean_rnd_bonus     |    0.000  |\n",
            "| memory/               |           |\n",
            "|    usefulness_loss    |    0.006  |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| rollout/              |           |\n",
            "|    ep_len_mean        |   16.000  |\n",
            "|    ep_rew_mean        |    0.969  |\n",
            "|    ep_rew_std         |    0.251  |\n",
            "|    policy_entropy     |    0.049  |\n",
            "|    advantage_mean     |   -0.013  |\n",
            "|    advantage_std      |    0.040  |\n",
            "|    aux_loss_mean      |    0.000  |\n",
            "| time/                 |           |\n",
            "|    fps                |      150  |\n",
            "|    episodes           |     1750  |\n",
            "|    time_elapsed       |      186  |\n",
            "|    total_timesteps    |    28000  |\n",
            "| train/                |           |\n",
            "|    loss               |    0.005  |\n",
            "|    policy_loss        |    0.004  |\n",
            "|    value_loss         |    0.002  |\n",
            "|    explained_variance |    0.057  |\n",
            "|    n_updates          |     1750  |\n",
            "|    progress           |  70.0%    |\n",
            "| rnd_net_dist/         |           |\n",
            "|    mean_rnd_bonus     |    0.000  |\n",
            "| memory/               |           |\n",
            "|    usefulness_loss    |    0.003  |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| rollout/              |           |\n",
            "|    ep_len_mean        |   16.000  |\n",
            "|    ep_rew_mean        |    0.985  |\n",
            "|    ep_rew_std         |    0.178  |\n",
            "|    policy_entropy     |    0.128  |\n",
            "|    advantage_mean     |    0.019  |\n",
            "|    advantage_std      |    0.034  |\n",
            "|    aux_loss_mean      |    0.000  |\n",
            "| time/                 |           |\n",
            "|    fps                |      154  |\n",
            "|    episodes           |     2000  |\n",
            "|    time_elapsed       |      207  |\n",
            "|    total_timesteps    |    32000  |\n",
            "| train/                |           |\n",
            "|    loss               |    0.031  |\n",
            "|    policy_loss        |    0.031  |\n",
            "|    value_loss         |    0.001  |\n",
            "|    explained_variance |    0.497  |\n",
            "|    n_updates          |     2000  |\n",
            "|    progress           |  80.0%    |\n",
            "| rnd_net_dist/         |           |\n",
            "|    mean_rnd_bonus     |    0.000  |\n",
            "| memory/               |           |\n",
            "|    usefulness_loss    |    0.005  |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| rollout/              |           |\n",
            "|    ep_len_mean        |   16.000  |\n",
            "|    ep_rew_mean        |    1.001  |\n",
            "|    ep_rew_std         |    0.000  |\n",
            "|    policy_entropy     |    0.127  |\n",
            "|    advantage_mean     |    0.005  |\n",
            "|    advantage_std      |    0.031  |\n",
            "|    aux_loss_mean      |    0.000  |\n",
            "| time/                 |           |\n",
            "|    fps                |      157  |\n",
            "|    episodes           |     2250  |\n",
            "|    time_elapsed       |      229  |\n",
            "|    total_timesteps    |    36000  |\n",
            "| train/                |           |\n",
            "|    loss               |    0.054  |\n",
            "|    policy_loss        |    0.055  |\n",
            "|    value_loss         |    0.001  |\n",
            "|    explained_variance |    0.531  |\n",
            "|    n_updates          |     2250  |\n",
            "|    progress           |  90.0%    |\n",
            "| rnd_net_dist/         |           |\n",
            "|    mean_rnd_bonus     |    0.000  |\n",
            "| memory/               |           |\n",
            "|    usefulness_loss    |    0.004  |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| rollout/              |           |\n",
            "|    ep_len_mean        |   16.000  |\n",
            "|    ep_rew_mean        |    1.001  |\n",
            "|    ep_rew_std         |    0.000  |\n",
            "|    policy_entropy     |    0.124  |\n",
            "|    advantage_mean     |    0.001  |\n",
            "|    advantage_std      |    0.046  |\n",
            "|    aux_loss_mean      |    0.000  |\n",
            "| time/                 |           |\n",
            "|    fps                |      159  |\n",
            "|    episodes           |     2500  |\n",
            "|    time_elapsed       |      250  |\n",
            "|    total_timesteps    |    40000  |\n",
            "| train/                |           |\n",
            "|    loss               |    0.332  |\n",
            "|    policy_loss        |    0.332  |\n",
            "|    value_loss         |    0.002  |\n",
            "|    explained_variance |   -0.073  |\n",
            "|    n_updates          |     2500  |\n",
            "|    progress           |  100.0%   |\n",
            "| rnd_net_dist/         |           |\n",
            "|    mean_rnd_bonus     |    0.000  |\n",
            "| memory/               |           |\n",
            "|    usefulness_loss    |    0.004  |\n",
            "-------------------------------------\n",
            "Training complete. Total episodes: 2500, total steps: 40000\n"
          ]
        }
      ],
      "source": [
        "# ENVIRONMENT =============================\n",
        "env = MemoryTaskEnv(delay=DELAY, difficulty=0)\n",
        "\n",
        "# MEMORY BUFFER ===========================\n",
        "memory = StrategicMemoryBuffer(\n",
        "    obs_dim=env.observation_space.shape[0],\n",
        "    action_dim=1,          # For Discrete(2)\n",
        "    mem_dim=MEM_DIM,\n",
        "    max_entries=N_MEMORIES,\n",
        "    device=\"cpu\"\n",
        ")\n",
        "\n",
        "# POLICY NETWORK (use class) ==============\n",
        "policy = StrategicMemoryTransformerPolicy\n",
        "\n",
        "# (optional) AUXILIARY MODULES ============\n",
        "\"\"\"\n",
        "aux_modules = [\n",
        "    CueAuxModule(feat_dim=MEM_DIM*2, n_classes=2),\n",
        "    ConfidenceAuxModule(feat_dim=MEM_DIM*2)\n",
        "]\n",
        "\"\"\"\n",
        "\n",
        "# AGENT SETUP =============================\n",
        "agent = TraceRL(\n",
        "    policy_class=policy,\n",
        "    env=env,\n",
        "    memory=memory,\n",
        "    memory_learn_retention=True,    \n",
        "    memory_retention_coef=0.01,   \n",
        "    # aux_modules=aux_modules,  \n",
        "    device=\"cpu\",\n",
        "    verbose=1,\n",
        "    lam=0.95, \n",
        "    gamma=0.99, \n",
        "    ent_coef=0.01,\n",
        "    learning_rate=1e-3, \n",
        "    \n",
        "    **MEMORY_AGENT_KWARGS\n",
        ")\n",
        "\n",
        "# TRAIN THE AGENT =========================\n",
        "agent.learn(\n",
        "    total_timesteps=total_timesteps(DELAY, N_EPISODES),\n",
        "    log_interval=250\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9af2188b-d2b4-40f3-9de9-220c1bc935f5",
      "metadata": {},
      "source": [
        "## Benchmark this agent against a regular PPO and a RecurentPPO\n",
        "\n",
        "Will be used a environment that requires the agent to remeber past observations to decide what to do on the last action.\n",
        "\n",
        "The reward is 1 or -1 if the agent uses the same action as the first item of the first observation , any other steps get 0 reward so the causal/effect is very delayed"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c7e26209-9156-4ebd-b90a-77aef5e4b5c8",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Training in EASY mode with delay of 4 steps\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Finalizing Results: 100%|██████████| 7/7 [01:22<00:00, 11.81s/step]             \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "╭────┬──────────────────────┬─────────┬────────┬───────────────┬──────────────┬────────────────╮\n",
            "│    │ Agent                │   Delay │ Mode   │   Mean Ep Rew │   Std Ep Rew │   Duration (s) │\n",
            "├────┼──────────────────────┼─────────┼────────┼───────────────┼──────────────┼────────────────┤\n",
            "│  0 │ PPO                  │       4 │ EASY   │             0 │            1 │        8.36426 │\n",
            "│  1 │ RecurrentPPO         │       4 │ EASY   │             0 │            1 │       26.2419  │\n",
            "│  2 │ TraceRL │       4 │ EASY   │             0 │            1 │       47.7484  │\n",
            "╰────┴──────────────────────┴─────────┴────────┴───────────────┴──────────────┴────────────────╯\n",
            "\n",
            "Training in HARD mode with delay of 4 steps\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Finalizing Results: 100%|██████████| 7/7 [01:43<00:00, 14.75s/step]             \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "╭────┬──────────────────────┬─────────┬────────┬───────────────┬──────────────┬────────────────╮\n",
            "│    │ Agent                │   Delay │ Mode   │   Mean Ep Rew │   Std Ep Rew │   Duration (s) │\n",
            "├────┼──────────────────────┼─────────┼────────┼───────────────┼──────────────┼────────────────┤\n",
            "│  0 │ PPO                  │       4 │ HARD   │           0   │     1        │        10.5363 │\n",
            "│  1 │ RecurrentPPO         │       4 │ HARD   │          -0.1 │     0.994987 │        34.2714 │\n",
            "│  2 │ TraceRL │       4 │ HARD   │           0.2 │     0.979796 │        58.0726 │\n",
            "╰────┴──────────────────────┴─────────┴────────┴───────────────┴──────────────┴────────────────╯\n",
            "\n",
            "Training in EASY mode with delay of 32 steps\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training TraceRL:   0%|          | 0/3 [00:00<?, ?step/s]"
          ]
        }
      ],
      "source": [
        "# BATCH EXPERIMENT SETUP ==================\n",
        "if __name__ == \"__main__\":\n",
        "    EXPERIMENTS = [\n",
        "        dict(delay=4, n_train_episodes=2000, total_timesteps=total_timesteps(4,2500), difficulty=0, mode_name=\"EASY\", verbose=0, eval_base=True),\n",
        "        dict(delay=4, n_train_episodes=5000, total_timesteps=total_timesteps(4,3500), difficulty=1, mode_name=\"HARD\", verbose=0, eval_base=True),\n",
        "        dict(delay=16, n_train_episodes=7500, total_timesteps=total_timesteps(16,3500), difficulty=0, mode_name=\"EASY\", verbose=0, eval_base=False),\n",
        "        dict(delay=32, n_train_episodes=7500, total_timesteps=total_timesteps(32,5000), difficulty=1, mode_name=\"EASY\", verbose=0, eval_base=False),\n",
        "        #dict(delay=64, n_train_episodes=15000, total_timesteps=15000*64, difficulty=0, mode_name=\"HARD\", verbose=0, eval_base=False),\n",
        "        dict(delay=256, n_train_episodes=20000, total_timesteps=total_timesteps(256,10000), difficulty=0, mode_name=\"HARD\", verbose=1, eval_base=False),\n",
        "    ]\n",
        "\n",
        "    # Custom memory agent config \n",
        "    memory_agent_config = dict(\n",
        "        action_dim=1,          # For Discrete(2)\n",
        "        mem_dim=MEM_DIM,\n",
        "        max_entries=N_MEMORIES,\n",
        "        policy_class=StrategicMemoryTransformerPolicy,\n",
        "        **AGENT_KWARGS,\n",
        "        **MEMORY_AGENT_KWARGS\n",
        "       \n",
        "    )\n",
        "\n",
        "    results = []\n",
        "    for exp in EXPERIMENTS:\n",
        "        benchmark = AgentPerformanceBenchmark(exp, memory_agent_config=memory_agent_config)\n",
        "        results.append(benchmark.run())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0eb00544-9550-4b94-9ce5-3a22776651b1",
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a533d0bd-5145-47c5-b6ca-d64748d76ec3",
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python [conda env:trading]",
      "language": "python",
      "name": "conda-env-trading-py"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.23"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
