{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f92b2a72-5fe0-4b47-8868-34cfacaf0d34",
      "metadata": {},
      "outputs": [],
      "source": [
        "from agent import TraceRL\n",
        "from environments import MemoryTaskEnv\n",
        "from benchmark import AgentPerformanceBenchmark\n",
        "from memory import StrategicMemoryBuffer,StrategicMemoryTransformerPolicy\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bc12f4e9-a95c-439c-9efa-69678cf4bb62",
      "metadata": {},
      "outputs": [],
      "source": [
        "# SETUP ===================================\n",
        "DELAY = 16\n",
        "MEM_DIM = 32\n",
        "N_EPISODES = 2500\n",
        "N_MEMORIES = 16\n",
        "\n",
        "AGENT_KWARGS = dict(\n",
        "    device=\"cpu\",\n",
        "    verbose=0,\n",
        "    lam=0.95, \n",
        "    gamma=0.99, \n",
        "    ent_coef=0.01,\n",
        "    learning_rate=1e-3, \n",
        "    \n",
        ")\n",
        "MEMORY_AGENT_KWARGS=dict(\n",
        "    her=False,\n",
        "    reward_norm=False,\n",
        "    aux_modules=None,\n",
        "    \n",
        "    intrinsic_expl=True,\n",
        "    intrinsic_eta=0.01,\n",
        "    \n",
        "    use_rnd=True, \n",
        "    rnd_emb_dim=32, \n",
        "    rnd_lr=1e-3,\n",
        ")\n",
        "\n",
        "# HELPERS =================================\n",
        "def total_timesteps(delay,n_episodes):\n",
        "    return delay * n_episodes"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "06e6ed27-0fec-40b5-a6dc-69d16538eb36",
      "metadata": {},
      "source": [
        "## **Example:** Simple training setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fb83c049-bf57-461e-acfc-21ba913f42ab",
      "metadata": {
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "# ENVIRONMENT =============================\n",
        "env = MemoryTaskEnv(delay=DELAY, difficulty=0)\n",
        "\n",
        "# MEMORY BUFFER ===========================\n",
        "memory = StrategicMemoryBuffer(\n",
        "    obs_dim=env.observation_space.shape[0],\n",
        "    action_dim=1,          # For Discrete(2)\n",
        "    mem_dim=MEM_DIM,\n",
        "    max_entries=N_MEMORIES,\n",
        "    device=\"cpu\"\n",
        ")\n",
        "\n",
        "# POLICY NETWORK (use class) ==============\n",
        "policy = StrategicMemoryTransformerPolicy\n",
        "\n",
        "# (optional) AUXILIARY MODULES ============\n",
        "\"\"\"\n",
        "aux_modules = [\n",
        "    CueAuxModule(feat_dim=MEM_DIM*2, n_classes=2),\n",
        "    ConfidenceAuxModule(feat_dim=MEM_DIM*2)\n",
        "]\n",
        "\"\"\"\n",
        "\n",
        "# AGENT SETUP =============================\n",
        "agent = TraceRL(\n",
        "    policy_class=policy,\n",
        "    env=env,\n",
        "    memory=memory,\n",
        "    memory_learn_retention=True,    \n",
        "    memory_retention_coef=0.01,   \n",
        "    # aux_modules=aux_modules,  \n",
        "    **AGENT_KWARGS,\n",
        "    **MEMORY_AGENT_KWARGS\n",
        ")\n",
        "\n",
        "# TRAIN THE AGENT =========================\n",
        "agent.learn(\n",
        "    total_timesteps=total_timesteps(DELAY, N_EPISODES),\n",
        "    log_interval=100\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9af2188b-d2b4-40f3-9de9-220c1bc935f5",
      "metadata": {},
      "source": [
        "## Benchmark this agent against a regular PPO and a RecurentPPO\n",
        "\n",
        "Will be used a environment that requires the agent to remeber past observations to decide what to do on the last action.\n",
        "\n",
        "The reward is 1 or -1 if the agent uses the same action as the first item of the first observation , any other steps get 0 reward so the causal/effect is very delayed"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c7e26209-9156-4ebd-b90a-77aef5e4b5c8",
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "\n",
        "\n",
        "# --- Batch experiment setup ---\n",
        "if __name__ == \"__main__\":\n",
        "    EXPERIMENTS = [\n",
        "        dict(delay=4, n_train_episodes=2000, total_timesteps=total_timesteps(4,2000), difficulty=0, mode_name=\"EASY\", verbose=0, eval_base=True),\n",
        "        dict(delay=4, n_train_episodes=5000, total_timesteps=total_timesteps(4,2500), difficulty=1, mode_name=\"HARD\", verbose=0, eval_base=True),\n",
        "        dict(delay=32, n_train_episodes=7500, total_timesteps=total_timesteps(32,3000), difficulty=0, mode_name=\"EASY\", verbose=0, eval_base=False),\n",
        "        dict(delay=32, n_train_episodes=7500, total_timesteps=total_timesteps(32,3500), difficulty=1, mode_name=\"EASY\", verbose=0, eval_base=False),\n",
        "        #dict(delay=64, n_train_episodes=15000, total_timesteps=15000*64, difficulty=0, mode_name=\"HARD\", verbose=0, eval_base=False),\n",
        "        dict(delay=256, n_train_episodes=20000, total_timesteps=total_timesteps(256,5000), difficulty=0, mode_name=\"HARD\", verbose=0, eval_base=False),\n",
        "    ]\n",
        "\n",
        "    # --- Custom memory agent config (edit as needed) ---\n",
        "    memory_agent_config = dict(\n",
        "        action_dim=1,          # For Discrete(2)\n",
        "        mem_dim=MEM_DIM,\n",
        "        max_entries=N_MEMORIES,\n",
        "        policy_class=StrategicMemoryTransformerPolicy,\n",
        "        **AGENT_KWARGS,\n",
        "        **MEMORY_AGENT_KWARGS\n",
        "        # Add more settings if needed\n",
        "    )\n",
        "\n",
        "    results = []\n",
        "    for exp in EXPERIMENTS:\n",
        "        # For each experiment, pass memory agent config\n",
        "        benchmark = AgentPerformanceBenchmark(exp, memory_agent_config=memory_agent_config)\n",
        "        results.append(benchmark.run())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0eb00544-9550-4b94-9ce5-3a22776651b1",
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a533d0bd-5145-47c5-b6ca-d64748d76ec3",
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python [conda env:trading]",
      "language": "python",
      "name": "conda-env-trading-py"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.23"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
