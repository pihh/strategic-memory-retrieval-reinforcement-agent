{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "073e0b13-4d37-4135-a2cd-f49bc22f6a17",
   "metadata": {},
   "source": [
    "# Strategic Memory RL: An Agent With Active, Learnable Memory\n",
    "\n",
    "Strategic Memory RL is a reinforcement learning agent that actively learns what to remember and what to forget through a learnable external memory. It enables the agent to make strategic, long-term decisions without any human hints or shortcuts.\n",
    "\n",
    "## The Problem it solves:\n",
    "Standard RL agents struggle with tasks where the optimal action depends on events or cues seen far in the past. This agent is designed to handle these long-term credit assignment and memory-based decision tasks—crucial for real-world problems where important information may only be revealed at the beginning of an episode and needed much later.\n",
    "\n",
    "---\n",
    "\n",
    "## Why This Agent Stands Out\n",
    "\n",
    "* **First to jointly optimize both memory retention (what to keep/discard) and retrieval (what to attend to) in a single, end-to-end RL agent**.\n",
    "* **Flexible plug-and-play memory**: Can be swapped for many memory architectures (transformers, graph attention, learned compression).\n",
    "* **No task-specific hacks**: Outperforms the above on classic RL memory benchmarks *without using any domain knowledge* or “cheat” features.\n",
    "* **Interpretable, practical, and scalable**: Suitable for real-world problems where “what matters” is unknown and must be discovered.\n",
    "\n",
    "---\n",
    "\n",
    "## **Expanded Comparison Table**\n",
    "\n",
    "| Feature / Method              | LSTM PPO     | DNC/NTM        | Decision Transformer | GTrXL             | NEC / DND | Neural Map | **Strategic Memory Agent**  |\n",
    "| ----------------------------- | ------------ | -------------- | -------------------- | ----------------- | --------- | ---------- | --------------------------- |\n",
    "| Core Memory Type              | Hidden state | External R/W   | In-Context (GPT)     | Segment history   | kNN table | 2D spatial | Episodic buffer + retention |\n",
    "| Memory Retention              | Fades        | Manual/learned | None                 | History window    | FIFO      | Manual     | *Learnable, optimized*      |\n",
    "| Retrieval                     | Implicit     | Soft/explicit  | Implicit             | History attention | kNN/soft  | Soft/read  | *Soft attention*            |\n",
    "| Retention Learning            | No           | Partial        | No                   | No                | No        | No         | **Yes**                     |\n",
    "| Interpretable Recall          | No           | Hard           | No                   | Some              | Some      | No         | **Yes (attention, use)**    |\n",
    "| Persistent Memory             | No           | Partial        | No                   | Partial           | Yes       | Yes        | **Yes**                     |\n",
    "| Sequence Length               | Short/medium | Short          | *Long*               | *Long*            | Medium    | Medium     | *Long*                      |\n",
    "| No Hints/Flags                | Yes          | Yes            | Yes                  | Yes               | Yes       | Yes        | **Yes**                     |\n",
    "| Outperforms on Delayed Reward | ✗            | ±              | ±                    | ±                 | ±         | ±          | **✓✓✓**                     |\n",
    "\n",
    "---\n",
    "\n",
    "## Expanded Literature & Reference Models\n",
    "\n",
    "This agent builds upon and advances the following lines of research:\n",
    "\n",
    "| **Approach / Paper**                                               | **Core Idea**                                   | **Key Weakness vs This**                                        |\n",
    "| ------------------------------------------------------------------ | ----------------------------------------------- | --------------------------------------------------------------- |\n",
    "| **DQN/LSTM-based RL**<br>Hausknecht & Stone, 2015                  | RNN hidden state as memory                      | Struggles with long delays, limited memory                      |\n",
    "| **Neural Episodic Control**<br>Pritzel et al., 2017                | Non-parametric DND table, kNN retrieval         | No learnable retention, no end-to-end training                  |\n",
    "| **Differentiable Neural Computer**<br>Graves et al., 2016          | RNN w/ differentiable read/write memory         | Expensive, hard to scale, hard to tune                          |\n",
    "| **Neural Map / Memory-Augmented RL**<br>Parisotto et al., 2018     | Spatially structured memory, soft addressing    | Retention/static, not fully learnable, not cue-driven           |\n",
    "| **Unsupervised Predictive Memory**<br>Wayne et al., 2018           | Latent predictive memory for meta-RL            | Memory not explicitly strategic or retained                     |\n",
    "| **MERLIN**<br>Wayne et al., 2018                                   | Latent memory with unsupervised auxiliary tasks | Retention not explicit, memory not strategic                    |\n",
    "| **Decision Transformer**<br>Chen et al., 2021                      | Uses a GPT-style transformer over trajectory    | No explicit, persistent external memory; not episodic retrieval |\n",
    "| **GTrXL (Transformer-XL RL)**<br>Parisotto et al., 2020            | Relational transformer for RL sequence modeling | \"Memory\" = recent history, not explicit retention or recall     |\n",
    "| **MVP: Memory Value Propagation**<br>Oh et al., 2020               | Learnable memory with value propagation         | Not as interpretable, not retention-focused                     |\n",
    "| **Recurrent Independent Mechanisms (RIMs)**<br>Goyal et al., 2021  | Modular memory units, attention-based gating    | No persistent, recallable episodic buffer                       |\n",
    "| **Active Memory / Episodic Control (EC)**<br>Blundell et al., 2016 | Episodic memory with tabular kNN access         | No differentiable retention, no meta-learning                   |\n",
    "\n",
    "---\n",
    "\n",
    "## **Additional References**\n",
    "\n",
    "* **Hausknecht & Stone, 2015**: “Deep Recurrent Q-Learning for Partially Observable MDPs”\n",
    "* **Pritzel et al., 2017**: “Neural Episodic Control”, [arXiv:1703.01988](https://arxiv.org/abs/1703.01988)\n",
    "* **Parisotto et al., 2018**: “Neural Map: Structured Memory for Deep Reinforcement Learning”, [ICLR 2018](https://openreview.net/forum?id=B14TlG-RW)\n",
    "* **Wayne et al., 2018**: “Unsupervised Predictive Memory in a Goal-Directed Agent”, [arXiv:1803.10760](https://arxiv.org/abs/1803.10760)\n",
    "* **Wayne et al., 2018**: “The Unreasonable Effectiveness of Recurrent Neural Networks in Reinforcement Learning” (MERLIN), [arXiv:1804.00761](https://arxiv.org/abs/1804.00761)\n",
    "* **Chen et al., 2021**: “Decision Transformer: Reinforcement Learning via Sequence Modeling”, [arXiv:2106.01345](https://arxiv.org/abs/2106.01345)\n",
    "* **Parisotto et al., 2020**: “Stabilizing Transformers for Reinforcement Learning”, [ICML 2020 (GTrXL)](http://proceedings.mlr.press/v119/parisotto20a.html)\n",
    "* **Oh et al., 2020**: “Value Propagation Networks”, [ICLR 2020](https://openreview.net/forum?id=B1xSperKvB)\n",
    "* **Goyal et al., 2021**: “Recurrent Independent Mechanisms”, [ICLR 2021](https://openreview.net/forum?id=mLcmdlEUxy-)\n",
    "* **Blundell et al., 2016**: “Model-Free Episodic Control”, [arXiv:1606.04460](https://arxiv.org/abs/1606.04460)\n",
    "* **Graves et al., 2016**: “Hybrid computing using a neural network with dynamic external memory” (DNC), [Nature 2016](https://www.nature.com/articles/nature20101)\n",
    "* **Sukhbaatar et al., 2015**: “End-To-End Memory Networks”, [arXiv:1503.08895](https://arxiv.org/abs/1503.08895)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f92b2a72-5fe0-4b47-8868-34cfacaf0d34",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\filip_a58djhu\\anaconda3\\envs\\trading\\lib\\site-packages\\gym\\envs\\registration.py:307: DeprecationWarning: The package name gym_minigrid has been deprecated in favor of minigrid. Please uninstall gym_minigrid and install minigrid with `pip install minigrid`. Future releases will be maintained under the new package name minigrid.\n",
      "  fn()\n"
     ]
    }
   ],
   "source": [
    "from agent import StrategicMemoryAgent\n",
    "from environments import MemoryTaskEnv\n",
    "from benchmark import AgentPerformanceBenchmark\n",
    "from memory import StrategicMemoryBuffer,StrategicMemoryTransformerPolicy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bc12f4e9-a95c-439c-9efa-69678cf4bb62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SETUP ===================================\n",
    "DELAY = 16\n",
    "MEM_DIM = 32\n",
    "N_EPISODES = 2500\n",
    "N_MEMORIES = 16\n",
    "\n",
    "AGENT_KWARGS = dict(\n",
    "    device=\"cpu\",\n",
    "    verbose=0,\n",
    "    lam=0.95, \n",
    "    gamma=0.99, \n",
    "    ent_coef=0.01,\n",
    "    learning_rate=1e-3, \n",
    "    \n",
    ")\n",
    "MEMORY_AGENT_KWARGS=dict(\n",
    "    her=False,\n",
    "    reward_norm=False,\n",
    "    aux_modules=None,\n",
    "    \n",
    "    intrinsic_expl=True,\n",
    "    intrinsic_eta=0.01,\n",
    "    \n",
    "    use_rnd=True, \n",
    "    rnd_emb_dim=32, \n",
    "    rnd_lr=1e-3,\n",
    ")\n",
    "\n",
    "# HELPERS =================================\n",
    "def total_timesteps(delay,n_episodes):\n",
    "    return delay * n_episodes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06e6ed27-0fec-40b5-a6dc-69d16538eb36",
   "metadata": {},
   "source": [
    "## **Example:** Simple training setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fb83c049-bf57-461e-acfc-21ba913f42ab",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        |   16.000  |\n",
      "|    ep_rew_mean        |    0.062  |\n",
      "|    ep_rew_std         |    0.999  |\n",
      "|    policy_entropy     |    0.497  |\n",
      "|    advantage_mean     |   -0.477  |\n",
      "|    advantage_std      |    0.127  |\n",
      "|    aux_loss_mean      |    0.000  |\n",
      "| time/                 |           |\n",
      "|    fps                |      142  |\n",
      "|    episodes           |      250  |\n",
      "|    time_elapsed       |       28  |\n",
      "|    total_timesteps    |     4000  |\n",
      "| train/                |           |\n",
      "|    loss               |   -2.677  |\n",
      "|    policy_loss        |   -2.794  |\n",
      "|    value_loss         |    0.242  |\n",
      "|    explained_variance |    0.288  |\n",
      "|    n_updates          |      250  |\n",
      "|    progress           |  10.0%    |\n",
      "| rnd_net_dist/         |           |\n",
      "|    mean_rnd_bonus     |    0.000  |\n",
      "| memory/               |           |\n",
      "|    usefulness_loss    |    0.005  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        |   16.000  |\n",
      "|    ep_rew_mean        |   -0.006  |\n",
      "|    ep_rew_std         |    1.000  |\n",
      "|    policy_entropy     |    0.427  |\n",
      "|    advantage_mean     |   -0.591  |\n",
      "|    advantage_std      |    0.170  |\n",
      "|    aux_loss_mean      |    0.000  |\n",
      "| time/                 |           |\n",
      "|    fps                |      140  |\n",
      "|    episodes           |      500  |\n",
      "|    time_elapsed       |       57  |\n",
      "|    total_timesteps    |     8000  |\n",
      "| train/                |           |\n",
      "|    loss               |   -4.595  |\n",
      "|    policy_loss        |   -4.779  |\n",
      "|    value_loss         |    0.377  |\n",
      "|    explained_variance |    0.107  |\n",
      "|    n_updates          |      500  |\n",
      "|    progress           |  20.0%    |\n",
      "| rnd_net_dist/         |           |\n",
      "|    mean_rnd_bonus     |    0.000  |\n",
      "| memory/               |           |\n",
      "|    usefulness_loss    |    0.017  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        |   16.000  |\n",
      "|    ep_rew_mean        |    0.058  |\n",
      "|    ep_rew_std         |    0.998  |\n",
      "|    policy_entropy     |    0.183  |\n",
      "|    advantage_mean     |    0.252  |\n",
      "|    advantage_std      |    0.192  |\n",
      "|    aux_loss_mean      |    0.000  |\n",
      "| time/                 |           |\n",
      "|    fps                |      141  |\n",
      "|    episodes           |      750  |\n",
      "|    time_elapsed       |       85  |\n",
      "|    total_timesteps    |    12000  |\n",
      "| train/                |           |\n",
      "|    loss               |    0.270  |\n",
      "|    policy_loss        |    0.222  |\n",
      "|    value_loss         |    0.098  |\n",
      "|    explained_variance |   -3.787  |\n",
      "|    n_updates          |      750  |\n",
      "|    progress           |  30.0%    |\n",
      "| rnd_net_dist/         |           |\n",
      "|    mean_rnd_bonus     |    0.000  |\n",
      "| memory/               |           |\n",
      "|    usefulness_loss    |    0.069  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        |   16.000  |\n",
      "|    ep_rew_mean        |    0.353  |\n",
      "|    ep_rew_std         |    0.936  |\n",
      "|    policy_entropy     |    0.588  |\n",
      "|    advantage_mean     |    0.141  |\n",
      "|    advantage_std      |    0.084  |\n",
      "|    aux_loss_mean      |    0.000  |\n",
      "| time/                 |           |\n",
      "|    fps                |      141  |\n",
      "|    episodes           |     1000  |\n",
      "|    time_elapsed       |      113  |\n",
      "|    total_timesteps    |    16000  |\n",
      "| train/                |           |\n",
      "|    loss               |    1.229  |\n",
      "|    policy_loss        |    1.221  |\n",
      "|    value_loss         |    0.027  |\n",
      "|    explained_variance |   -0.278  |\n",
      "|    n_updates          |     1000  |\n",
      "|    progress           |  40.0%    |\n",
      "| rnd_net_dist/         |           |\n",
      "|    mean_rnd_bonus     |    0.000  |\n",
      "| memory/               |           |\n",
      "|    usefulness_loss    |    0.019  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        |   16.000  |\n",
      "|    ep_rew_mean        |    0.929  |\n",
      "|    ep_rew_std         |    0.373  |\n",
      "|    policy_entropy     |    0.464  |\n",
      "|    advantage_mean     |    0.011  |\n",
      "|    advantage_std      |    0.085  |\n",
      "|    aux_loss_mean      |    0.000  |\n",
      "| time/                 |           |\n",
      "|    fps                |      140  |\n",
      "|    episodes           |     1250  |\n",
      "|    time_elapsed       |      142  |\n",
      "|    total_timesteps    |    20000  |\n",
      "| train/                |           |\n",
      "|    loss               |    0.372  |\n",
      "|    policy_loss        |    0.373  |\n",
      "|    value_loss         |    0.007  |\n",
      "|    explained_variance |   -2.172  |\n",
      "|    n_updates          |     1250  |\n",
      "|    progress           |  50.0%    |\n",
      "| rnd_net_dist/         |           |\n",
      "|    mean_rnd_bonus     |    0.000  |\n",
      "| memory/               |           |\n",
      "|    usefulness_loss    |    0.011  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        |   16.000  |\n",
      "|    ep_rew_mean        |    0.969  |\n",
      "|    ep_rew_std         |    0.251  |\n",
      "|    policy_entropy     |    0.259  |\n",
      "|    advantage_mean     |    0.014  |\n",
      "|    advantage_std      |    0.067  |\n",
      "|    aux_loss_mean      |    0.000  |\n",
      "| time/                 |           |\n",
      "|    fps                |      146  |\n",
      "|    episodes           |     1500  |\n",
      "|    time_elapsed       |      164  |\n",
      "|    total_timesteps    |    24000  |\n",
      "| train/                |           |\n",
      "|    loss               |    0.029  |\n",
      "|    policy_loss        |    0.030  |\n",
      "|    value_loss         |    0.004  |\n",
      "|    explained_variance |   -1.166  |\n",
      "|    n_updates          |     1500  |\n",
      "|    progress           |  60.0%    |\n",
      "| rnd_net_dist/         |           |\n",
      "|    mean_rnd_bonus     |    0.000  |\n",
      "| memory/               |           |\n",
      "|    usefulness_loss    |    0.006  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        |   16.000  |\n",
      "|    ep_rew_mean        |    0.969  |\n",
      "|    ep_rew_std         |    0.251  |\n",
      "|    policy_entropy     |    0.049  |\n",
      "|    advantage_mean     |   -0.013  |\n",
      "|    advantage_std      |    0.040  |\n",
      "|    aux_loss_mean      |    0.000  |\n",
      "| time/                 |           |\n",
      "|    fps                |      150  |\n",
      "|    episodes           |     1750  |\n",
      "|    time_elapsed       |      186  |\n",
      "|    total_timesteps    |    28000  |\n",
      "| train/                |           |\n",
      "|    loss               |    0.005  |\n",
      "|    policy_loss        |    0.004  |\n",
      "|    value_loss         |    0.002  |\n",
      "|    explained_variance |    0.057  |\n",
      "|    n_updates          |     1750  |\n",
      "|    progress           |  70.0%    |\n",
      "| rnd_net_dist/         |           |\n",
      "|    mean_rnd_bonus     |    0.000  |\n",
      "| memory/               |           |\n",
      "|    usefulness_loss    |    0.003  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        |   16.000  |\n",
      "|    ep_rew_mean        |    0.985  |\n",
      "|    ep_rew_std         |    0.178  |\n",
      "|    policy_entropy     |    0.128  |\n",
      "|    advantage_mean     |    0.019  |\n",
      "|    advantage_std      |    0.034  |\n",
      "|    aux_loss_mean      |    0.000  |\n",
      "| time/                 |           |\n",
      "|    fps                |      154  |\n",
      "|    episodes           |     2000  |\n",
      "|    time_elapsed       |      207  |\n",
      "|    total_timesteps    |    32000  |\n",
      "| train/                |           |\n",
      "|    loss               |    0.031  |\n",
      "|    policy_loss        |    0.031  |\n",
      "|    value_loss         |    0.001  |\n",
      "|    explained_variance |    0.497  |\n",
      "|    n_updates          |     2000  |\n",
      "|    progress           |  80.0%    |\n",
      "| rnd_net_dist/         |           |\n",
      "|    mean_rnd_bonus     |    0.000  |\n",
      "| memory/               |           |\n",
      "|    usefulness_loss    |    0.005  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        |   16.000  |\n",
      "|    ep_rew_mean        |    1.001  |\n",
      "|    ep_rew_std         |    0.000  |\n",
      "|    policy_entropy     |    0.127  |\n",
      "|    advantage_mean     |    0.005  |\n",
      "|    advantage_std      |    0.031  |\n",
      "|    aux_loss_mean      |    0.000  |\n",
      "| time/                 |           |\n",
      "|    fps                |      157  |\n",
      "|    episodes           |     2250  |\n",
      "|    time_elapsed       |      229  |\n",
      "|    total_timesteps    |    36000  |\n",
      "| train/                |           |\n",
      "|    loss               |    0.054  |\n",
      "|    policy_loss        |    0.055  |\n",
      "|    value_loss         |    0.001  |\n",
      "|    explained_variance |    0.531  |\n",
      "|    n_updates          |     2250  |\n",
      "|    progress           |  90.0%    |\n",
      "| rnd_net_dist/         |           |\n",
      "|    mean_rnd_bonus     |    0.000  |\n",
      "| memory/               |           |\n",
      "|    usefulness_loss    |    0.004  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        |   16.000  |\n",
      "|    ep_rew_mean        |    1.001  |\n",
      "|    ep_rew_std         |    0.000  |\n",
      "|    policy_entropy     |    0.124  |\n",
      "|    advantage_mean     |    0.001  |\n",
      "|    advantage_std      |    0.046  |\n",
      "|    aux_loss_mean      |    0.000  |\n",
      "| time/                 |           |\n",
      "|    fps                |      159  |\n",
      "|    episodes           |     2500  |\n",
      "|    time_elapsed       |      250  |\n",
      "|    total_timesteps    |    40000  |\n",
      "| train/                |           |\n",
      "|    loss               |    0.332  |\n",
      "|    policy_loss        |    0.332  |\n",
      "|    value_loss         |    0.002  |\n",
      "|    explained_variance |   -0.073  |\n",
      "|    n_updates          |     2500  |\n",
      "|    progress           |  100.0%   |\n",
      "| rnd_net_dist/         |           |\n",
      "|    mean_rnd_bonus     |    0.000  |\n",
      "| memory/               |           |\n",
      "|    usefulness_loss    |    0.004  |\n",
      "-------------------------------------\n",
      "Training complete. Total episodes: 2500, total steps: 40000\n"
     ]
    }
   ],
   "source": [
    "# ENVIRONMENT =============================\n",
    "env = MemoryTaskEnv(delay=DELAY, difficulty=0)\n",
    "\n",
    "# MEMORY BUFFER ===========================\n",
    "memory = StrategicMemoryBuffer(\n",
    "    obs_dim=env.observation_space.shape[0],\n",
    "    action_dim=1,          # For Discrete(2)\n",
    "    mem_dim=MEM_DIM,\n",
    "    max_entries=N_MEMORIES,\n",
    "    device=\"cpu\"\n",
    ")\n",
    "\n",
    "# POLICY NETWORK (use class) ==============\n",
    "policy = StrategicMemoryTransformerPolicy\n",
    "\n",
    "# (optional) AUXILIARY MODULES ============\n",
    "\"\"\"\n",
    "aux_modules = [\n",
    "    CueAuxModule(feat_dim=MEM_DIM*2, n_classes=2),\n",
    "    ConfidenceAuxModule(feat_dim=MEM_DIM*2)\n",
    "]\n",
    "\"\"\"\n",
    "\n",
    "# AGENT SETUP =============================\n",
    "agent = StrategicMemoryAgent(\n",
    "    policy_class=policy,\n",
    "    env=env,\n",
    "    memory=memory,\n",
    "    memory_learn_retention=True,    \n",
    "    memory_retention_coef=0.01,   \n",
    "    # aux_modules=aux_modules,  \n",
    "    device=\"cpu\",\n",
    "    verbose=1,\n",
    "    lam=0.95, \n",
    "    gamma=0.99, \n",
    "    ent_coef=0.01,\n",
    "    learning_rate=1e-3, \n",
    "    \n",
    "    **MEMORY_AGENT_KWARGS\n",
    ")\n",
    "\n",
    "# TRAIN THE AGENT =========================\n",
    "agent.learn(\n",
    "    total_timesteps=total_timesteps(DELAY, N_EPISODES),\n",
    "    log_interval=250\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9af2188b-d2b4-40f3-9de9-220c1bc935f5",
   "metadata": {},
   "source": [
    "## Benchmark this agent against a regular PPO and a RecurentPPO\n",
    "\n",
    "Will be used a environment that requires the agent to remeber past observations to decide what to do on the last action.\n",
    "\n",
    "The reward is 1 or -1 if the agent uses the same action as the first item of the first observation , any other steps get 0 reward so the causal/effect is very delayed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c7e26209-9156-4ebd-b90a-77aef5e4b5c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training in EASY mode with delay of 4 steps\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Finalizing Results: 100%|██████████| 7/7 [02:19<00:00, 19.92s/step]             \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "╭────┬──────────────────────┬─────────┬────────┬───────────────┬──────────────┬────────────────╮\n",
      "│    │ Agent                │   Delay │ Mode   │   Mean Ep Rew │   Std Ep Rew │   Duration (s) │\n",
      "├────┼──────────────────────┼─────────┼────────┼───────────────┼──────────────┼────────────────┤\n",
      "│  0 │ PPO                  │       4 │ EASY   │           0   │     1        │        15.3959 │\n",
      "│  1 │ RecurrentPPO         │       4 │ EASY   │           0   │     1        │        45.3466 │\n",
      "│  2 │ StrategicMemoryAgent │       4 │ EASY   │           0.1 │     0.994987 │        78.2219 │\n",
      "╰────┴──────────────────────┴─────────┴────────┴───────────────┴──────────────┴────────────────╯\n",
      "\n",
      "Training in HARD mode with delay of 4 steps\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Finalizing Results: 100%|██████████| 7/7 [03:00<00:00, 25.78s/step]             \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "╭────┬──────────────────────┬─────────┬────────┬───────────────┬──────────────┬────────────────╮\n",
      "│    │ Agent                │   Delay │ Mode   │   Mean Ep Rew │   Std Ep Rew │   Duration (s) │\n",
      "├────┼──────────────────────┼─────────┼────────┼───────────────┼──────────────┼────────────────┤\n",
      "│  0 │ PPO                  │       4 │ HARD   │             0 │            1 │        20.1596 │\n",
      "│  1 │ RecurrentPPO         │       4 │ HARD   │             0 │            1 │        59.2795 │\n",
      "│  2 │ StrategicMemoryAgent │       4 │ HARD   │             0 │            1 │       100.685  │\n",
      "╰────┴──────────────────────┴─────────┴────────┴───────────────┴──────────────┴────────────────╯\n",
      "\n",
      "Training in EASY mode with delay of 16 steps\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating StrategicMemoryAgent:  33%|███▎      | 1/3 [05:05<10:10, 305.13s/step]\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "index out of range in self",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 26\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m exp \u001b[38;5;129;01min\u001b[39;00m EXPERIMENTS:\n\u001b[0;32m     25\u001b[0m     benchmark \u001b[38;5;241m=\u001b[39m AgentPerformanceBenchmark(exp, memory_agent_config\u001b[38;5;241m=\u001b[39mmemory_agent_config)\n\u001b[1;32m---> 26\u001b[0m     results\u001b[38;5;241m.\u001b[39mappend(\u001b[43mbenchmark\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[1;32m~\\Dev\\strategic-memory-retrieval-reinforcement-agent\\benchmark.py:186\u001b[0m, in \u001b[0;36mAgentPerformanceBenchmark.run\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    184\u001b[0m \u001b[38;5;66;03m# EVALUATE\u001b[39;00m\n\u001b[0;32m    185\u001b[0m pbar\u001b[38;5;241m.\u001b[39mset_description(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEvaluating \u001b[39m\u001b[38;5;132;01m{\u001b[39;00magent_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 186\u001b[0m mean, std \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43magent_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m    187\u001b[0m pbar\u001b[38;5;241m.\u001b[39mupdate(\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m    188\u001b[0m results\u001b[38;5;241m.\u001b[39mappend({\n\u001b[0;32m    189\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv_config,\n\u001b[0;32m    190\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124magent\u001b[39m\u001b[38;5;124m'\u001b[39m: agent_name,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    193\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mduration\u001b[39m\u001b[38;5;124m'\u001b[39m: duration\n\u001b[0;32m    194\u001b[0m })\n",
      "File \u001b[1;32m~\\Dev\\strategic-memory-retrieval-reinforcement-agent\\benchmark.py:125\u001b[0m, in \u001b[0;36mAgentPerformanceBenchmark.evaluate\u001b[1;34m(self, model, model_name, deterministic, verbose)\u001b[0m\n\u001b[0;32m    123\u001b[0m total_reward \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.0\u001b[39m\n\u001b[0;32m    124\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m done:\n\u001b[1;32m--> 125\u001b[0m     action \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdeterministic\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdeterministic\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    126\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(action, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[0;32m    127\u001b[0m         action \u001b[38;5;241m=\u001b[39m action[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[1;32m~\\Dev\\strategic-memory-retrieval-reinforcement-agent\\agent.py:397\u001b[0m, in \u001b[0;36mStrategicMemoryAgent.predict\u001b[1;34m(self, obs, deterministic, done, reward)\u001b[0m\n\u001b[0;32m    395\u001b[0m rewards_float \u001b[38;5;241m=\u001b[39m [r \u001b[38;5;28;01mfor\u001b[39;00m _, _, r \u001b[38;5;129;01min\u001b[39;00m context_traj]\n\u001b[0;32m    396\u001b[0m obs_stack \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mstack([torch\u001b[38;5;241m.\u001b[39mtensor(o, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat32, device\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice) \u001b[38;5;28;01mfor\u001b[39;00m o, _, _ \u001b[38;5;129;01min\u001b[39;00m context_traj])\n\u001b[1;32m--> 397\u001b[0m logits, _, _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpolicy\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    398\u001b[0m \u001b[43m    \u001b[49m\u001b[43mobs_stack\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mobs_t\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    399\u001b[0m \u001b[43m    \u001b[49m\u001b[43mactions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mactions_int\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    400\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrewards\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrewards_float\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    401\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    402\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m deterministic:\n\u001b[0;32m    403\u001b[0m     action \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39margmax(logits)\u001b[38;5;241m.\u001b[39mitem()\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\trading\\lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\trading\\lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32m~\\Dev\\strategic-memory-retrieval-reinforcement-agent\\memory.py:176\u001b[0m, in \u001b[0;36mStrategicMemoryTransformerPolicy.forward\u001b[1;34m(self, trajectory, obs_t, actions, rewards)\u001b[0m\n\u001b[0;32m    174\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membed(trajectory)                 \u001b[38;5;66;03m# [T, mem_dim]\u001b[39;00m\n\u001b[0;32m    175\u001b[0m pos \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39marange(T, device\u001b[38;5;241m=\u001b[39mtrajectory\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m--> 176\u001b[0m x \u001b[38;5;241m=\u001b[39m x \u001b[38;5;241m+\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpos_embed\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpos\u001b[49m\u001b[43m)\u001b[49m                \u001b[38;5;66;03m# Add position encoding\u001b[39;00m\n\u001b[0;32m    177\u001b[0m x \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m)                         \u001b[38;5;66;03m# [1, T, mem_dim]\u001b[39;00m\n\u001b[0;32m    178\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransformer(x)                    \u001b[38;5;66;03m# [1, T, mem_dim]\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\trading\\lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\trading\\lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\trading\\lib\\site-packages\\torch\\nn\\modules\\sparse.py:190\u001b[0m, in \u001b[0;36mEmbedding.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    189\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 190\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membedding\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    191\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    192\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    193\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding_idx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    194\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_norm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    195\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnorm_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    196\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscale_grad_by_freq\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    197\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msparse\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    198\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\trading\\lib\\site-packages\\torch\\nn\\functional.py:2551\u001b[0m, in \u001b[0;36membedding\u001b[1;34m(input, weight, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse)\u001b[0m\n\u001b[0;32m   2545\u001b[0m     \u001b[38;5;66;03m# Note [embedding_renorm set_grad_enabled]\u001b[39;00m\n\u001b[0;32m   2546\u001b[0m     \u001b[38;5;66;03m# XXX: equivalent to\u001b[39;00m\n\u001b[0;32m   2547\u001b[0m     \u001b[38;5;66;03m# with torch.no_grad():\u001b[39;00m\n\u001b[0;32m   2548\u001b[0m     \u001b[38;5;66;03m#   torch.embedding_renorm_\u001b[39;00m\n\u001b[0;32m   2549\u001b[0m     \u001b[38;5;66;03m# remove once script supports set_grad_enabled\u001b[39;00m\n\u001b[0;32m   2550\u001b[0m     _no_grad_embedding_renorm_(weight, \u001b[38;5;28minput\u001b[39m, max_norm, norm_type)\n\u001b[1;32m-> 2551\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membedding\u001b[49m\u001b[43m(\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpadding_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscale_grad_by_freq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msparse\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mIndexError\u001b[0m: index out of range in self"
     ]
    }
   ],
   "source": [
    "# BATCH EXPERIMENT SETUP ==================\n",
    "if __name__ == \"__main__\":\n",
    "    EXPERIMENTS = [\n",
    "        dict(delay=4, n_train_episodes=2000, total_timesteps=total_timesteps(4,2500), difficulty=0, mode_name=\"EASY\", verbose=0, eval_base=True),\n",
    "        dict(delay=4, n_train_episodes=5000, total_timesteps=total_timesteps(4,3500), difficulty=1, mode_name=\"HARD\", verbose=0, eval_base=True),\n",
    "        dict(delay=16, n_train_episodes=7500, total_timesteps=total_timesteps(16,3500), difficulty=0, mode_name=\"EASY\", verbose=0, eval_base=False),\n",
    "        dict(delay=32, n_train_episodes=7500, total_timesteps=total_timesteps(32,5000), difficulty=1, mode_name=\"EASY\", verbose=0, eval_base=False),\n",
    "        #dict(delay=64, n_train_episodes=15000, total_timesteps=15000*64, difficulty=0, mode_name=\"HARD\", verbose=0, eval_base=False),\n",
    "        dict(delay=256, n_train_episodes=20000, total_timesteps=total_timesteps(256,10000), difficulty=0, mode_name=\"HARD\", verbose=1, eval_base=False),\n",
    "    ]\n",
    "\n",
    "    # Custom memory agent config \n",
    "    memory_agent_config = dict(\n",
    "        action_dim=1,          # For Discrete(2)\n",
    "        mem_dim=MEM_DIM,\n",
    "        max_entries=N_MEMORIES,\n",
    "        policy_class=StrategicMemoryTransformerPolicy,\n",
    "        **AGENT_KWARGS,\n",
    "        **MEMORY_AGENT_KWARGS\n",
    "       \n",
    "    )\n",
    "\n",
    "    results = []\n",
    "    for exp in EXPERIMENTS:\n",
    "        benchmark = AgentPerformanceBenchmark(exp, memory_agent_config=memory_agent_config)\n",
    "        results.append(benchmark.run())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0eb00544-9550-4b94-9ce5-3a22776651b1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a533d0bd-5145-47c5-b6ca-d64748d76ec3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:trading]",
   "language": "python",
   "name": "conda-env-trading-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
