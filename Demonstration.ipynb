{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "073e0b13-4d37-4135-a2cd-f49bc22f6a17",
   "metadata": {},
   "source": [
    "# TraceRL\n",
    "#### Strategic Recall for Reinforcement Learning. An Agent With Active, Learnable Memory\n",
    "\n",
    "--- \n",
    "\n",
    "\n",
    "**TraceRL** is a reinforcement learning (RL) agent designed for **environments where optimal decisions require remembering and leveraging information from the distant past**—not just recent history. The agent maintains an **external, actively-managed episodic memory** where it stores compressed summaries of entire experiences (trajectories) and learns **which memories to retain and which to discard** as training progresses.\n",
    "\n",
    "### The Problem:\n",
    "\n",
    "Classic RL agents—like DQN, LSTM PPO, or even transformers—struggle when rewards are delayed, sparse, or depend on events far in the past. In such tasks, remembering the right event, state, or action at the right time is crucial for success. Most RL methods either forget, overfit to short-term cues, or retain irrelevant information, resulting in poor performance on long-horizon, memory-based tasks.\n",
    "\n",
    "### The Solution:\n",
    "\n",
    "TraceRL **actively learns**:\n",
    "\n",
    "- **What to store:** Which episodes or sequences are worth keeping in memory.\n",
    "- **What to forget:** Which are unhelpful and can be safely discarded.\n",
    "- **How to retrieve:** At every decision point, the agent uses attention to retrieve relevant past experiences from memory and integrates them with the current observation before acting.\n",
    "\n",
    "**All of this is trained end-to-end**, so the agent autonomously discovers how to use its memory buffer for strategic decision-making—**no hints, flags, or engineered memory cues are required**.\n",
    "\n",
    "---\n",
    "\n",
    "## **Comparison Table**\n",
    "\n",
    "| Feature / Method              | LSTM PPO     | DNC/NTM        | Decision Transformer | GTrXL             | NEC / DND | Neural Map | **TraceRL** |\n",
    "| ----------------------------- | ------------ | -------------- | -------------------- | ----------------- | --------- | ---------- | ------------------------------ |\n",
    "| Core Memory Type              | Hidden state | External R/W   | In-Context (GPT)     | Segment history   | kNN table | 2D spatial | Episodic buffer + retention    |\n",
    "| Memory Retention              | Fades        | Manual/learned | None                 | History window    | FIFO      | Manual     | _Learnable, optimized_         |\n",
    "| Retrieval                     | Implicit     | Soft/explicit  | Implicit             | History attention | kNN/soft  | Soft/read  | _Soft attention_               |\n",
    "| Retention Learning            | No           | Partial        | No                   | No                | No        | No         | **Yes**                        |\n",
    "| Interpretable Recall          | No           | Hard           | No                   | Some              | Some      | No         | **Yes (attention, use)**       |\n",
    "| Persistent Memory             | No           | Partial        | No                   | Partial           | Yes       | Yes        | **Yes**                        |\n",
    "| Sequence Length               | Short/medium | Short          | _Long_               | _Long_            | Medium    | Medium     | _Long_                         |\n",
    "| No Hints/Flags                | Yes          | Yes            | Yes                  | Yes               | Yes       | Yes        | **Yes**                        |\n",
    "| Outperforms on Delayed Reward | ✗            | ±              | ±                    | ±                 | ±         | ±          | **✓✓✓**                        |\n",
    "\n",
    "---\n",
    "\n",
    "## Literature & Reference Models\n",
    "\n",
    "This agent builds upon and advances the following lines of research:\n",
    "\n",
    "| **Approach / Paper**                                               | **Core Idea**                                   | **Key Weakness vs TraceRL**                                        |\n",
    "| ------------------------------------------------------------------ | ----------------------------------------------- | --------------------------------------------------------------- |\n",
    "| **DQN/LSTM-based RL**<br>Hausknecht & Stone, 2015                  | RNN hidden state as memory                      | Struggles with long delays, limited memory                      |\n",
    "| **Neural Episodic Control**<br>Pritzel et al., 2017                | Non-parametric DND table, kNN retrieval         | No learnable retention, no end-to-end training                  |\n",
    "| **Differentiable Neural Computer**<br>Graves et al., 2016          | RNN w/ differentiable read/write memory         | Expensive, hard to scale, hard to tune                          |\n",
    "| **Neural Map / Memory-Augmented RL**<br>Parisotto et al., 2018     | Spatially structured memory, soft addressing    | Retention/static, not fully learnable, not cue-driven           |\n",
    "| **Unsupervised Predictive Memory**<br>Wayne et al., 2018           | Latent predictive memory for meta-RL            | Memory not explicitly strategic or retained                     |\n",
    "| **MERLIN**<br>Wayne et al., 2018                                   | Latent memory with unsupervised auxiliary tasks | Retention not explicit, memory not strategic                    |\n",
    "| **Decision Transformer**<br>Chen et al., 2021                      | Uses a GPT-style transformer over trajectory    | No explicit, persistent external memory; not episodic retrieval |\n",
    "| **GTrXL (Transformer-XL RL)**<br>Parisotto et al., 2020            | Relational transformer for RL sequence modeling | \"Memory\" = recent history, not explicit retention or recall     |\n",
    "| **MVP: Memory Value Propagation**<br>Oh et al., 2020               | Learnable memory with value propagation         | Not as interpretable, not retention-focused                     |\n",
    "| **Recurrent Independent Mechanisms (RIMs)**<br>Goyal et al., 2021  | Modular memory units, attention-based gating    | No persistent, recallable episodic buffer                       |\n",
    "| **Active Memory / Episodic Control (EC)**<br>Blundell et al., 2016 | Episodic memory with tabular kNN access         | No differentiable retention, no meta-learning                   |\n",
    "\n",
    "---\n",
    "\n",
    "## **Additional References**\n",
    "\n",
    "- **Hausknecht & Stone, 2015**: “Deep Recurrent Q-Learning for Partially Observable MDPs”\n",
    "- **Pritzel et al., 2017**: “Neural Episodic Control”, [arXiv:1703.01988](https://arxiv.org/abs/1703.01988)\n",
    "- **Parisotto et al., 2018**: “Neural Map: Structured Memory for Deep Reinforcement Learning”, [ICLR 2018](https://openreview.net/forum?id=B14TlG-RW)\n",
    "- **Wayne et al., 2018**: “Unsupervised Predictive Memory in a Goal-Directed Agent”, [arXiv:1803.10760](https://arxiv.org/abs/1803.10760)\n",
    "- **Wayne et al., 2018**: “The Unreasonable Effectiveness of Recurrent Neural Networks in Reinforcement Learning” (MERLIN), [arXiv:1804.00761](https://arxiv.org/abs/1804.00761)\n",
    "- **Chen et al., 2021**: “Decision Transformer: Reinforcement Learning via Sequence Modeling”, [arXiv:2106.01345](https://arxiv.org/abs/2106.01345)\n",
    "- **Parisotto et al., 2020**: “Stabilizing Transformers for Reinforcement Learning”, [ICML 2020 (GTrXL)](http://proceedings.mlr.press/v119/parisotto20a.html)\n",
    "- **Oh et al., 2020**: “Value Propagation Networks”, [ICLR 2020](https://openreview.net/forum?id=B1xSperKvB)\n",
    "- **Goyal et al., 2021**: “Recurrent Independent Mechanisms”, [ICLR 2021](https://openreview.net/forum?id=mLcmdlEUxy-)\n",
    "- **Blundell et al., 2016**: “Model-Free Episodic Control”, [arXiv:1606.04460](https://arxiv.org/abs/1606.04460)\n",
    "- **Graves et al., 2016**: “Hybrid computing using a neural network with dynamic external memory” (DNC), [Nature 2016](https://www.nature.com/articles/nature20101)\n",
    "- **Sukhbaatar et al., 2015**: “End-To-End Memory Networks”, [arXiv:1503.08895](https://arxiv.org/abs/1503.08895)\n",
    "\n",
    "---\n",
    "\n",
    "## TL;DR;\n",
    "\n",
    "- **First to jointly optimize both memory retention (what to keep/discard) and retrieval (what to attend to) in a single, end-to-end RL agent**.\n",
    "- **Flexible plug-and-play memory**: Can be swapped for many memory architectures (transformers, graph attention, learned compression).\n",
    "- **No task-specific hacks**: Outperforms the above on classic RL memory benchmarks _without using any domain knowledge_ or “cheat” features.\n",
    "- **Interpretable, practical, and scalable**: Suitable for real-world problems where “what matters” is unknown and must be discovered.\n",
    "\n",
    "---\n",
    "\n",
    "**Author:** Filipe Sá  \n",
    "**Contact:** filipemotasa@hotmail.com | [GitHub](https://github.com/pihh/)\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f92b2a72-5fe0-4b47-8868-34cfacaf0d34",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from agent import TraceRL\n",
    "from environments import MemoryTaskEnv\n",
    "from benchmark import AgentPerformanceBenchmark\n",
    "from memory import StrategicMemoryBuffer,StrategicMemoryTransformerPolicy\n",
    "from constants import DEFAULT_DEVICE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bc12f4e9-a95c-439c-9efa-69678cf4bb62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SETUP ===================================\n",
    "DELAY = 4\n",
    "MEM_DIM = 32\n",
    "N_EPISODES = 2500\n",
    "N_MEMORIES = 32\n",
    "\n",
    "AGENT_KWARGS = dict(\n",
    "    device=DEFAULT_DEVICE,\n",
    "    verbose=0,\n",
    "    lam=0.95, \n",
    "    gamma=0.99, \n",
    "    ent_coef=0.01,\n",
    "    learning_rate=1e-3, \n",
    "    \n",
    ")\n",
    "MEMORY_AGENT_KWARGS=dict(\n",
    "    her=False,\n",
    "    reward_norm=False,\n",
    "    aux_modules=None,\n",
    "    \n",
    "    intrinsic_expl=False,\n",
    "    intrinsic_eta=0.01,\n",
    "    \n",
    "    use_rnd=False, \n",
    "    rnd_emb_dim=32, \n",
    "    rnd_lr=1e-3,\n",
    ")\n",
    "\n",
    "# HELPERS =================================\n",
    "def total_timesteps(delay,n_episodes):\n",
    "    return delay * n_episodes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06e6ed27-0fec-40b5-a6dc-69d16538eb36",
   "metadata": {},
   "source": [
    "## **Example:** Simple training setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fb83c049-bf57-461e-acfc-21ba913f42ab",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        |    4.000  |\n",
      "|    ep_rew_mean        |    0.112  |\n",
      "|    ep_rew_std         |    0.994  |\n",
      "|    policy_entropy     |    0.139  |\n",
      "|    advantage_mean     |    0.113  |\n",
      "|    advantage_std      |    0.034  |\n",
      "|    aux_loss_mean      |    0.000  |\n",
      "| time/                 |           |\n",
      "|    fps                |      124  |\n",
      "|    episodes           |      250  |\n",
      "|    time_elapsed       |        8  |\n",
      "|    total_timesteps    |     1000  |\n",
      "| train/                |           |\n",
      "|    loss               |    0.020  |\n",
      "|    policy_loss        |    0.015  |\n",
      "|    value_loss         |    0.014  |\n",
      "|    explained_variance |   -1.786  |\n",
      "|    n_updates          |      250  |\n",
      "|    progress           |    6.250  |\n",
      "| memory/               |           |\n",
      "|    usefulness_loss    |    0.012  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        |    4.000  |\n",
      "|    ep_rew_mean        |    0.728  |\n",
      "|    ep_rew_std         |    0.686  |\n",
      "|    policy_entropy     |    0.416  |\n",
      "|    advantage_mean     |    0.091  |\n",
      "|    advantage_std      |    0.168  |\n",
      "|    aux_loss_mean      |    0.000  |\n",
      "| time/                 |           |\n",
      "|    fps                |      124  |\n",
      "|    episodes           |      500  |\n",
      "|    time_elapsed       |       16  |\n",
      "|    total_timesteps    |     2000  |\n",
      "| train/                |           |\n",
      "|    loss               |    0.160  |\n",
      "|    policy_loss        |    0.149  |\n",
      "|    value_loss         |    0.030  |\n",
      "|    explained_variance | -124.413  |\n",
      "|    n_updates          |      500  |\n",
      "|    progress           |   12.500  |\n",
      "| memory/               |           |\n",
      "|    usefulness_loss    |    0.004  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        |    4.000  |\n",
      "|    ep_rew_mean        |    0.960  |\n",
      "|    ep_rew_std         |    0.280  |\n",
      "|    policy_entropy     |    0.050  |\n",
      "|    advantage_mean     |   -0.051  |\n",
      "|    advantage_std      |    0.059  |\n",
      "|    aux_loss_mean      |    0.000  |\n",
      "| time/                 |           |\n",
      "|    fps                |      124  |\n",
      "|    episodes           |      750  |\n",
      "|    time_elapsed       |       24  |\n",
      "|    total_timesteps    |     3000  |\n",
      "| train/                |           |\n",
      "|    loss               |   -0.004  |\n",
      "|    policy_loss        |   -0.006  |\n",
      "|    value_loss         |    0.005  |\n",
      "|    explained_variance |  -26.461  |\n",
      "|    n_updates          |      750  |\n",
      "|    progress           |   18.750  |\n",
      "| memory/               |           |\n",
      "|    usefulness_loss    |    0.004  |\n",
      "-------------------------------------\n",
      "╭──────────────────────┬───────╮\n",
      "│ Early Stop           │       │\n",
      "├──────────────────────┼───────┤\n",
      "│ Train duration       │ 27s   │\n",
      "│ Avg episode duration │ 0.03s │\n",
      "│ Rolling ep rew mean  │ 1.00  │\n",
      "│ Rolling ep rew std   │ 0.00  │\n",
      "│ Rolling ep length    │ 4.00  │\n",
      "│ N updates            │ 817   │\n",
      "╰──────────────────────┴───────╯\n"
     ]
    }
   ],
   "source": [
    "# ENVIRONMENT =============================\n",
    "env = MemoryTaskEnv(delay=DELAY, difficulty=0)\n",
    "\n",
    "# MEMORY BUFFER ===========================\n",
    "memory = StrategicMemoryBuffer(\n",
    "    obs_dim=env.observation_space.shape[0],\n",
    "    action_dim=1,          # For Discrete(2)\n",
    "    mem_dim=MEM_DIM,\n",
    "    max_entries=N_MEMORIES,\n",
    "    device=DEFAULT_DEVICE\n",
    ")\n",
    "\n",
    "# POLICY NETWORK (use class) ==============\n",
    "policy = StrategicMemoryTransformerPolicy\n",
    "\n",
    "# (optional) AUXILIARY MODULES ============\n",
    "\"\"\"\n",
    "aux_modules = [\n",
    "    CueAuxModule(feat_dim=MEM_DIM*2, n_classes=2),\n",
    "    ConfidenceAuxModule(feat_dim=MEM_DIM*2)\n",
    "]\n",
    "\"\"\"\n",
    "\n",
    "# AGENT SETUP =============================\n",
    "agent = TraceRL(\n",
    "    policy_class=policy,\n",
    "    env=env,\n",
    "    memory=memory,\n",
    "    memory_learn_retention=True,    \n",
    "    memory_retention_coef=0.01,   \n",
    "    # aux_modules=aux_modules,  \n",
    "    device=DEFAULT_DEVICE,\n",
    "    verbose=1,\n",
    "    lam=0.95, \n",
    "    gamma=0.99, \n",
    "    ent_coef=0.01,\n",
    "    learning_rate=1e-3, \n",
    "    \n",
    "    **MEMORY_AGENT_KWARGS\n",
    ")\n",
    "\n",
    "# TRAIN THE AGENT =========================\n",
    "agent.learn(\n",
    "    total_timesteps=total_timesteps(DELAY, 4000),\n",
    "    log_interval=250\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9af2188b-d2b4-40f3-9de9-220c1bc935f5",
   "metadata": {},
   "source": [
    "## Benchmark Overview\n",
    "This benchmark systematically evaluates this agent on a synthetic memory task with varying levels of difficulty and delay. Each experiment defines a unique scenario (e.g., short vs. long memory delays, easy vs. hard distractors) and trains the agent to solve it using a fixed number of episodes and timesteps.\n",
    "\n",
    "The goal is to compare the performance and generalization as task complexity increases. \n",
    "\n",
    "Optionally, the benchmark can compare this agent againts baseline models (e.g., standard PPO , Recurrent PPO (LstmPPO) under the same conditions. \n",
    "\n",
    "Results can be used to diagnose strengths and limitations of the agent, inform ablations, and guide further development."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7e26209-9156-4ebd-b90a-77aef5e4b5c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training in EASY mode with delay of 4 steps\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating TraceRL:  71%|███████▏  | 5/7 [01:40<00:36, 18.29s/step]     "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "╭──────────────────────┬───────╮\n",
      "│ Early Stop           │       │\n",
      "├──────────────────────┼───────┤\n",
      "│ Train duration       │ 19s   │\n",
      "│ Avg episode duration │ 0.03s │\n",
      "│ Rolling ep rew mean  │ 1.00  │\n",
      "│ Rolling ep rew std   │ 0.00  │\n",
      "│ Rolling ep length    │ 4.00  │\n",
      "│ N updates            │ 554   │\n",
      "╰──────────────────────┴───────╯\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Finalizing Results: 100%|██████████| 7/7 [01:40<00:00, 14.40s/step]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "╭────┬──────────────┬─────────┬────────┬───────────────┬──────────────┬────────────────╮\n",
      "│    │ Agent        │   Delay │ Mode   │   Mean Ep Rew │   Std Ep Rew │   Duration (s) │\n",
      "├────┼──────────────┼─────────┼────────┼───────────────┼──────────────┼────────────────┤\n",
      "│  0 │ PPO          │       4 │ EASY   │             0 │            1 │        28.5175 │\n",
      "│  1 │ RecurrentPPO │       4 │ EASY   │             0 │            1 │        51.787  │\n",
      "│  2 │ TraceRL      │       4 │ EASY   │             1 │            0 │        19.9993 │\n",
      "╰────┴──────────────┴─────────┴────────┴───────────────┴──────────────┴────────────────╯\n",
      "\n",
      "Training in HARD mode with delay of 4 steps\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating TraceRL:  71%|███████▏  | 5/7 [02:02<00:51, 25.91s/step]     "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "╭──────────────────────┬───────╮\n",
      "│ Early Stop           │       │\n",
      "├──────────────────────┼───────┤\n",
      "│ Train duration       │ 42s   │\n",
      "│ Avg episode duration │ 0.04s │\n",
      "│ Rolling ep rew mean  │ 1.00  │\n",
      "│ Rolling ep rew std   │ 0.00  │\n",
      "│ Rolling ep length    │ 4.00  │\n",
      "│ N updates            │ 950   │\n",
      "╰──────────────────────┴───────╯\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Finalizing Results: 100%|██████████| 7/7 [02:03<00:00, 17.61s/step]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "╭────┬──────────────┬─────────┬────────┬───────────────┬──────────────┬────────────────╮\n",
      "│    │ Agent        │   Delay │ Mode   │   Mean Ep Rew │   Std Ep Rew │   Duration (s) │\n",
      "├────┼──────────────┼─────────┼────────┼───────────────┼──────────────┼────────────────┤\n",
      "│  0 │ PPO          │       4 │ HARD   │           0   │     1        │        25.5765 │\n",
      "│  1 │ RecurrentPPO │       4 │ HARD   │           0.1 │     0.994987 │        54.4171 │\n",
      "│  2 │ TraceRL      │       4 │ HARD   │           1   │     0        │        42.7836 │\n",
      "╰────┴──────────────┴─────────┴────────┴───────────────┴──────────────┴────────────────╯\n",
      "\n",
      "Training in EASY mode with delay of 16 steps\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating TraceRL:  33%|███▎      | 1/3 [01:42<03:25, 102.98s/step]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "╭──────────────────────┬───────╮\n",
      "│ Early Stop           │       │\n",
      "├──────────────────────┼───────┤\n",
      "│ Train duration       │ 102s  │\n",
      "│ Avg episode duration │ 0.20s │\n",
      "│ Rolling ep rew mean  │ 1.00  │\n",
      "│ Rolling ep rew std   │ 0.00  │\n",
      "│ Rolling ep length    │ 16.00 │\n",
      "│ N updates            │ 520   │\n",
      "╰──────────────────────┴───────╯\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Finalizing Results: 100%|██████████| 3/3 [01:44<00:00, 34.79s/step] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "╭────┬─────────┬─────────┬────────┬───────────────┬──────────────┬────────────────╮\n",
      "│    │ Agent   │   Delay │ Mode   │   Mean Ep Rew │   Std Ep Rew │   Duration (s) │\n",
      "├────┼─────────┼─────────┼────────┼───────────────┼──────────────┼────────────────┤\n",
      "│  0 │ TraceRL │      16 │ EASY   │             1 │            0 │        102.984 │\n",
      "╰────┴─────────┴─────────┴────────┴───────────────┴──────────────┴────────────────╯\n",
      "\n",
      "Training in HARD mode with delay of 32 steps\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating TraceRL:  71%|███████▏  | 5/7 [1:28:13<31:27, 943.89s/step]        "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "╭──────────────────────┬───────╮\n",
      "│ Early Stop           │       │\n",
      "├──────────────────────┼───────┤\n",
      "│ Train duration       │ 512s  │\n",
      "│ Avg episode duration │ 0.44s │\n",
      "│ Rolling ep rew mean  │ 1.00  │\n",
      "│ Rolling ep rew std   │ 0.00  │\n",
      "│ Rolling ep length    │ 32.00 │\n",
      "│ N updates            │ 1173  │\n",
      "╰──────────────────────┴───────╯\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Finalizing Results: 100%|██████████| 7/7 [1:28:19<00:00, 757.01s/step]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "╭────┬──────────────┬─────────┬────────┬───────────────┬──────────────┬────────────────╮\n",
      "│    │ Agent        │   Delay │ Mode   │   Mean Ep Rew │   Std Ep Rew │   Duration (s) │\n",
      "├────┼──────────────┼─────────┼────────┼───────────────┼──────────────┼────────────────┤\n",
      "│  0 │ PPO          │      32 │ HARD   │             0 │            1 │        952.798 │\n",
      "│  1 │ RecurrentPPO │      32 │ HARD   │             0 │            1 │       3826.46  │\n",
      "│  2 │ TraceRL      │      32 │ HARD   │             1 │            0 │        512.687 │\n",
      "╰────┴──────────────┴─────────┴────────┴───────────────┴──────────────┴────────────────╯\n",
      "\n",
      "Training in HARD mode with delay of 256 steps\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training TraceRL:   0%|          | 0/3 [00:00<?, ?step/s]  "
     ]
    }
   ],
   "source": [
    "# BATCH EXPERIMENT SETUP ==================\n",
    "if __name__ == \"__main__\":\n",
    "    EXPERIMENTS = [\n",
    "        dict(delay=4, n_train_episodes=2000, total_timesteps=total_timesteps(4,2500), difficulty=0, mode_name=\"EASY\", verbose=0, eval_base=True),\n",
    "        dict(delay=4, n_train_episodes=5000, total_timesteps=total_timesteps(4,2500), difficulty=1, mode_name=\"HARD\", verbose=0, eval_base=True),\n",
    "        dict(delay=16, n_train_episodes=7500, total_timesteps=total_timesteps(16,3500), difficulty=0, mode_name=\"EASY\", verbose=0, eval_base=False),\n",
    "        dict(delay=32, n_train_episodes=7500, total_timesteps=total_timesteps(32,10000), difficulty=1, mode_name=\"HARD\", verbose=0, eval_base=True),\n",
    "        #dict(delay=64, n_train_episodes=15000, total_timesteps=15000*64, difficulty=0, mode_name=\"HARD\", verbose=0, eval_base=False),\n",
    "        dict(delay=256, n_train_episodes=20000, total_timesteps=total_timesteps(256,10000), difficulty=0, mode_name=\"HARD\", verbose=1, eval_base=False),\n",
    "    ]\n",
    "\n",
    "    # Custom memory agent config \n",
    "    memory_agent_config = dict(\n",
    "        action_dim=1,          # For Discrete(2)\n",
    "        mem_dim=MEM_DIM,\n",
    "        max_entries=N_MEMORIES,\n",
    "        policy_class=StrategicMemoryTransformerPolicy,\n",
    "        **AGENT_KWARGS,\n",
    "        **MEMORY_AGENT_KWARGS\n",
    "       \n",
    "    )\n",
    "\n",
    "    results = []\n",
    "    for exp in EXPERIMENTS:\n",
    "        benchmark = AgentPerformanceBenchmark(exp, memory_agent_config=memory_agent_config)\n",
    "        results.append(benchmark.run())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0eb00544-9550-4b94-9ce5-3a22776651b1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0586214b-d966-439f-94ea-6a42c315ffec",
   "metadata": {},
   "source": [
    "## Whats in the future?\n",
    "\n",
    "### Already detects:\n",
    "* Repeating event sequences (“if this and then that within 3 steps, reward later”)\n",
    "\n",
    "* Rare event triggers (“when you see X-Y-Z in order, prepare for Z+N”)\n",
    "\n",
    "* Long-term cues (“pattern at t=1…t=3, outcome at t=10”)\n",
    "\n",
    "* Short sub-patterns that are only meaningful in context\n",
    "\n",
    "### Next improvements:\n",
    "\n",
    "* Store rolling subtrajectories or multi-scale embeddings.\n",
    "\n",
    "* Use “motif mining” or sub-sequence attention in memory.\n",
    "\n",
    "* Allow retrieval by partial match, not just whole-trajectory.\n",
    "\n",
    "### Next step:\n",
    "\n",
    "* Motif mining:\n",
    "  * Trajectory Encoder - Transformer that maps a trajectory/subtrajectory to a fixed-dimensional vector.\n",
    "  * Motif Memory Bank - A differentiable, learnable set of motif embeddings (shape: [num_motifs, mem_dim]). Optionally, each motif also learns a value or usefulness score.\n",
    "\n",
    "  * Motif Mining Head - Module that, during or after a trajectory, produces a set of candidate motifs (sub-trajectories). Encodes them (with the encoder), clusters or scores them, and updates the memory bank (with gradient flow if possible). Motif selection can be based on similarity, usefulness (to reward), or novelty.\n",
    "\n",
    "  * Differentiable DTW (Soft-DTW) - This allows for “fuzzy” alignment scores between current trajectory and stored motifs, all within the computation graph.\n",
    "\n",
    "  * Motif Attention Integration "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4796524b-7269-4843-a0d6-0a2a6a63c3fc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05d9a291-4358-42e1-b082-a3f1cc2cae57",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:trading]",
   "language": "python",
   "name": "conda-env-trading-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
