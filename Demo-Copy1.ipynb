{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f92b2a72-5fe0-4b47-8868-34cfacaf0d34",
   "metadata": {},
   "outputs": [],
   "source": [
    "from agent import StrategicMemoryAgent\n",
    "from environments import MemoryTaskEnv\n",
    "from benchmark import AgentPerformanceBenchmark\n",
    "from memory import StrategicMemoryBuffer,StrategicMemoryTransformerPolicy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bc12f4e9-a95c-439c-9efa-69678cf4bb62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SETUP ===================================\n",
    "DELAY = 16\n",
    "MEM_DIM = 32\n",
    "N_EPISODES = 2500\n",
    "N_MEMORIES = 16\n",
    "\n",
    "AGENT_KWARGS = dict(\n",
    "    device=\"cpu\",\n",
    "    verbose=0,\n",
    "    lam=0.95, \n",
    "    gamma=0.99, \n",
    "    ent_coef=0.01,\n",
    "    learning_rate=1e-3, \n",
    "    \n",
    ")\n",
    "MEMORY_AGENT_KWARGS=dict(\n",
    "    her=False,\n",
    "    reward_norm=False,\n",
    "    aux_modules=None,\n",
    "    \n",
    "    intrinsic_expl=True,\n",
    "    intrinsic_eta=0.01,\n",
    "    \n",
    "    use_rnd=True, \n",
    "    rnd_emb_dim=32, \n",
    "    rnd_lr=1e-3,\n",
    ")\n",
    "\n",
    "# HELPERS =================================\n",
    "def total_timesteps(delay,n_episodes):\n",
    "    return delay * n_episodes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06e6ed27-0fec-40b5-a6dc-69d16538eb36",
   "metadata": {},
   "source": [
    "## **Example:** Simple training setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb83c049-bf57-461e-acfc-21ba913f42ab",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# ENVIRONMENT =============================\n",
    "env = MemoryTaskEnv(delay=DELAY, difficulty=0)\n",
    "\n",
    "# MEMORY BUFFER ===========================\n",
    "memory = StrategicMemoryBuffer(\n",
    "    obs_dim=env.observation_space.shape[0],\n",
    "    action_dim=1,          # For Discrete(2)\n",
    "    mem_dim=MEM_DIM,\n",
    "    max_entries=N_MEMORIES,\n",
    "    device=\"cpu\"\n",
    ")\n",
    "\n",
    "# POLICY NETWORK (use class) ==============\n",
    "policy = StrategicMemoryTransformerPolicy\n",
    "\n",
    "# (optional) AUXILIARY MODULES ============\n",
    "\"\"\"\n",
    "aux_modules = [\n",
    "    CueAuxModule(feat_dim=MEM_DIM*2, n_classes=2),\n",
    "    ConfidenceAuxModule(feat_dim=MEM_DIM*2)\n",
    "]\n",
    "\"\"\"\n",
    "\n",
    "# AGENT SETUP =============================\n",
    "agent = StrategicMemoryAgent(\n",
    "    policy_class=policy,\n",
    "    env=env,\n",
    "    memory=memory,\n",
    "    memory_learn_retention=True,    \n",
    "    memory_retention_coef=0.01,   \n",
    "    # aux_modules=aux_modules,  \n",
    "    **AGENT_KWARGS,\n",
    "    **MEMORY_AGENT_KWARGS\n",
    ")\n",
    "\n",
    "# TRAIN THE AGENT =========================\n",
    "agent.learn(\n",
    "    total_timesteps=total_timesteps(DELAY, N_EPISODES),\n",
    "    log_interval=100\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9af2188b-d2b4-40f3-9de9-220c1bc935f5",
   "metadata": {},
   "source": [
    "## Benchmark this agent against a regular PPO and a RecurentPPO\n",
    "\n",
    "Will be used a environment that requires the agent to remeber past observations to decide what to do on the last action.\n",
    "\n",
    "The reward is 1 or -1 if the agent uses the same action as the first item of the first observation , any other steps get 0 reward so the causal/effect is very delayed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c7e26209-9156-4ebd-b90a-77aef5e4b5c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training in EASY mode with delay of 4 steps\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training StrategicMemoryAgent:  57%|█████▋    | 4/7 [00:31<00:32, 10.78s/step]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        |    4.000  |\n",
      "|    ep_rew_mean        |    0.131  |\n",
      "|    ep_rew_std         |    0.992  |\n",
      "|    policy_entropy     |    0.439  |\n",
      "|    advantage_mean     |    1.513  |\n",
      "|    advantage_std      |    0.378  |\n",
      "|    aux_loss_mean      |    0.000  |\n",
      "| time/                 |           |\n",
      "|    fps                |      249  |\n",
      "|    episodes           |      250  |\n",
      "|    time_elapsed       |        4  |\n",
      "|    total_timesteps    |     1000  |\n",
      "| train/                |           |\n",
      "|    loss               |    6.677  |\n",
      "|    policy_loss        |    5.483  |\n",
      "|    value_loss         |    2.398  |\n",
      "|    explained_variance |  -10.056  |\n",
      "|    n_updates          |      250  |\n",
      "|    progress           |  12.50%   |\n",
      "| rnd_net_dist/         |           |\n",
      "|    mean_rnd_bonus     |    0.000  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        |    4.000  |\n",
      "|    ep_rew_mean        |    0.385  |\n",
      "|    ep_rew_std         |    0.923  |\n",
      "|    policy_entropy     |    0.087  |\n",
      "|    advantage_mean     |    0.322  |\n",
      "|    advantage_std      |    0.327  |\n",
      "|    aux_loss_mean      |    0.000  |\n",
      "| time/                 |           |\n",
      "|    fps                |      199  |\n",
      "|    episodes           |      500  |\n",
      "|    time_elapsed       |       10  |\n",
      "|    total_timesteps    |     2000  |\n",
      "| train/                |           |\n",
      "|    loss               |    0.119  |\n",
      "|    policy_loss        |    0.028  |\n",
      "|    value_loss         |    0.184  |\n",
      "|    explained_variance |  -89.339  |\n",
      "|    n_updates          |      500  |\n",
      "|    progress           |  25.00%   |\n",
      "| rnd_net_dist/         |           |\n",
      "|    mean_rnd_bonus     |    0.000  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        |    4.000  |\n",
      "|    ep_rew_mean        |    0.929  |\n",
      "|    ep_rew_std         |    0.373  |\n",
      "|    policy_entropy     |    0.192  |\n",
      "|    advantage_mean     |    0.130  |\n",
      "|    advantage_std      |    0.091  |\n",
      "|    aux_loss_mean      |    0.000  |\n",
      "| time/                 |           |\n",
      "|    fps                |      199  |\n",
      "|    episodes           |      750  |\n",
      "|    time_elapsed       |       15  |\n",
      "|    total_timesteps    |     3000  |\n",
      "| train/                |           |\n",
      "|    loss               |    0.029  |\n",
      "|    policy_loss        |    0.019  |\n",
      "|    value_loss         |    0.023  |\n",
      "|    explained_variance |  -13.891  |\n",
      "|    n_updates          |      750  |\n",
      "|    progress           |  37.50%   |\n",
      "| rnd_net_dist/         |           |\n",
      "|    mean_rnd_bonus     |    0.000  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        |    4.000  |\n",
      "|    ep_rew_mean        |    0.809  |\n",
      "|    ep_rew_std         |    0.589  |\n",
      "|    policy_entropy     |    0.065  |\n",
      "|    advantage_mean     |   -0.804  |\n",
      "|    advantage_std      |    0.263  |\n",
      "|    aux_loss_mean      |    0.000  |\n",
      "| time/                 |           |\n",
      "|    fps                |      199  |\n",
      "|    episodes           |     1000  |\n",
      "|    time_elapsed       |       20  |\n",
      "|    total_timesteps    |     4000  |\n",
      "| train/                |           |\n",
      "|    loss               |    0.312  |\n",
      "|    policy_loss        |   -0.037  |\n",
      "|    value_loss         |    0.698  |\n",
      "|    explained_variance |  -20.400  |\n",
      "|    n_updates          |     1000  |\n",
      "|    progress           |  50.00%   |\n",
      "| rnd_net_dist/         |           |\n",
      "|    mean_rnd_bonus     |    0.000  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        |    4.000  |\n",
      "|    ep_rew_mean        |    0.857  |\n",
      "|    ep_rew_std         |    0.517  |\n",
      "|    policy_entropy     |    0.015  |\n",
      "|    advantage_mean     |   -0.051  |\n",
      "|    advantage_std      |    0.131  |\n",
      "|    aux_loss_mean      |    0.000  |\n",
      "| time/                 |           |\n",
      "|    fps                |      199  |\n",
      "|    episodes           |     1250  |\n",
      "|    time_elapsed       |       25  |\n",
      "|    total_timesteps    |     5000  |\n",
      "| train/                |           |\n",
      "|    loss               |    0.007  |\n",
      "|    policy_loss        |   -0.000  |\n",
      "|    value_loss         |    0.015  |\n",
      "|    explained_variance | -461.652  |\n",
      "|    n_updates          |     1250  |\n",
      "|    progress           |  62.50%   |\n",
      "| rnd_net_dist/         |           |\n",
      "|    mean_rnd_bonus     |    0.000  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        |    4.000  |\n",
      "|    ep_rew_mean        |    0.993  |\n",
      "|    ep_rew_std         |    0.126  |\n",
      "|    policy_entropy     |    0.014  |\n",
      "|    advantage_mean     |   -0.025  |\n",
      "|    advantage_std      |    0.015  |\n",
      "|    aux_loss_mean      |    0.000  |\n",
      "| time/                 |           |\n",
      "|    fps                |      193  |\n",
      "|    episodes           |     1500  |\n",
      "|    time_elapsed       |       31  |\n",
      "|    total_timesteps    |     6000  |\n",
      "| train/                |           |\n",
      "|    loss               |    0.000  |\n",
      "|    policy_loss        |   -0.000  |\n",
      "|    value_loss         |    0.001  |\n",
      "|    explained_variance |   -1.002  |\n",
      "|    n_updates          |     1500  |\n",
      "|    progress           |  75.00%   |\n",
      "| rnd_net_dist/         |           |\n",
      "|    mean_rnd_bonus     |    0.000  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        |    4.000  |\n",
      "|    ep_rew_mean        |    0.984  |\n",
      "|    ep_rew_std         |    0.178  |\n",
      "|    policy_entropy     |    0.101  |\n",
      "|    advantage_mean     |    0.047  |\n",
      "|    advantage_std      |    0.052  |\n",
      "|    aux_loss_mean      |    0.000  |\n",
      "| time/                 |           |\n",
      "|    fps                |      194  |\n",
      "|    episodes           |     1750  |\n",
      "|    time_elapsed       |       36  |\n",
      "|    total_timesteps    |     7000  |\n",
      "| train/                |           |\n",
      "|    loss               |    0.364  |\n",
      "|    policy_loss        |    0.363  |\n",
      "|    value_loss         |    0.004  |\n",
      "|    explained_variance |  -12.307  |\n",
      "|    n_updates          |     1750  |\n",
      "|    progress           |  87.50%   |\n",
      "| rnd_net_dist/         |           |\n",
      "|    mean_rnd_bonus     |    0.000  |\n",
      "-------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating StrategicMemoryAgent:  71%|███████▏  | 5/7 [01:13<00:31, 16.00s/step]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        |    4.000  |\n",
      "|    ep_rew_mean        |    0.992  |\n",
      "|    ep_rew_std         |    0.126  |\n",
      "|    policy_entropy     |    0.063  |\n",
      "|    advantage_mean     |   -0.019  |\n",
      "|    advantage_std      |    0.134  |\n",
      "|    aux_loss_mean      |    0.000  |\n",
      "| time/                 |           |\n",
      "|    fps                |      195  |\n",
      "|    episodes           |     2000  |\n",
      "|    time_elapsed       |       41  |\n",
      "|    total_timesteps    |     8000  |\n",
      "| train/                |           |\n",
      "|    loss               |    0.001  |\n",
      "|    policy_loss        |   -0.006  |\n",
      "|    value_loss         |    0.014  |\n",
      "|    explained_variance |  -71.993  |\n",
      "|    n_updates          |     2000  |\n",
      "|    progress           | 100.00%   |\n",
      "| rnd_net_dist/         |           |\n",
      "|    mean_rnd_bonus     |    0.000  |\n",
      "-------------------------------------\n",
      "Training complete. Total episodes: 2000, total steps: 8000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Finalizing Results: 100%|██████████| 7/7 [01:13<00:00, 10.49s/step]             "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "╭────┬──────────────────────┬─────────┬────────┬───────────────┬──────────────┬────────────────╮\n",
      "│    │ Agent                │   Delay │ Mode   │   Mean Ep Rew │   Std Ep Rew │   Duration (s) │\n",
      "├────┼──────────────────────┼─────────┼────────┼───────────────┼──────────────┼────────────────┤\n",
      "│  0 │ PPO                  │       4 │ EASY   │           0   │     1        │        7.40373 │\n",
      "│  1 │ RecurrentPPO         │       4 │ EASY   │           0   │     1        │       23.9085  │\n",
      "│  2 │ StrategicMemoryAgent │       4 │ EASY   │          -0.2 │     0.979796 │       41.7879  │\n",
      "╰────┴──────────────────────┴─────────┴────────┴───────────────┴──────────────┴────────────────╯\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "# --- Batch experiment setup ---\n",
    "if __name__ == \"__main__\":\n",
    "    EXPERIMENTS = [\n",
    "        dict(delay=4, n_train_episodes=2000, total_timesteps=total_timesteps(4,2000), difficulty=0, mode_name=\"EASY\", verbose=0, eval_base=True),\n",
    "        dict(delay=4, n_train_episodes=5000, total_timesteps=total_timesteps(4,2500), difficulty=1, mode_name=\"HARD\", verbose=0, eval_base=True),\n",
    "        dict(delay=32, n_train_episodes=7500, total_timesteps=total_timesteps(32,3000), difficulty=0, mode_name=\"EASY\", verbose=0, eval_base=False),\n",
    "        dict(delay=32, n_train_episodes=7500, total_timesteps=total_timesteps(32,3500), difficulty=1, mode_name=\"EASY\", verbose=0, eval_base=False),\n",
    "        #dict(delay=64, n_train_episodes=15000, total_timesteps=15000*64, difficulty=0, mode_name=\"HARD\", verbose=0, eval_base=False),\n",
    "        dict(delay=128, n_train_episodes=20000, total_timesteps=20000*128, difficulty=0, mode_name=\"HARD\", verbose=0, eval_base=False),\n",
    "    ]\n",
    "\n",
    "    # --- Custom memory agent config (edit as needed) ---\n",
    "    memory_agent_config = dict(\n",
    "        action_dim=1,          # For Discrete(2)\n",
    "        mem_dim=MEM_DIM,\n",
    "        max_entries=N_MEMORIES,\n",
    "        policy_class=StrategicMemoryTransformerPolicy,\n",
    "        **AGENT_KWARGS,\n",
    "        **MEMORY_AGENT_KWARGS\n",
    "        # Add more settings if needed\n",
    "    )\n",
    "\n",
    "    results = []\n",
    "    for exp in EXPERIMENTS:\n",
    "        # For each experiment, pass memory agent config\n",
    "        benchmark = AgentPerformanceBenchmark(exp, memory_agent_config=memory_agent_config)\n",
    "        results.append(benchmark.run())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0eb00544-9550-4b94-9ce5-3a22776651b1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a533d0bd-5145-47c5-b6ca-d64748d76ec3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:trading]",
   "language": "python",
   "name": "conda-env-trading-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
