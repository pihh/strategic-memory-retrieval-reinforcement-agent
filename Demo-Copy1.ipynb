{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f92b2a72-5fe0-4b47-8868-34cfacaf0d34",
   "metadata": {},
   "outputs": [],
   "source": [
    "from environments import MemoryTaskEnv\n",
    "from agent import StrategicMemoryAgent\n",
    "from memory import StrategicMemoryBuffer,MemoryTransformerPolicy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bc12f4e9-a95c-439c-9efa-69678cf4bb62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SETUP ===================================\n",
    "DELAY = 8\n",
    "MEM_DIM = 32\n",
    "N_EPISODES = 2500\n",
    "N_MEMORIES = 16\n",
    "\n",
    "AGENT_KWARGS = dict(\n",
    "    device=\"cpu\",\n",
    "    verbose=1,\n",
    "    lam=0.95, \n",
    "    gamma=0.99, \n",
    "    ent_coef=0.01,\n",
    "    learning_rate=1e-3, \n",
    "    \n",
    "    her=False,\n",
    "    reward_norm=False,\n",
    "    aux_modules=None,\n",
    "    \n",
    "    intrinsic_expl=True,\n",
    "    intrinsic_eta=0.01,\n",
    "    \n",
    "    use_rnd=True, \n",
    "    rnd_emb_dim=32, \n",
    "    rnd_lr=1e-3,\n",
    ")\n",
    "# HELPERS =================================\n",
    "def total_timesteps(delay,n_episodes):\n",
    "    return delay * n_episodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fb83c049-bf57-461e-acfc-21ba913f42ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\filip_a58djhu\\Dev\\strategic-memory-retrieval-reinforcement-agent\\memory.py:53: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at C:\\b\\abs_dd5ga5p9x7\\croot\\libtorch_1746637520098\\work\\torch\\csrc\\utils\\tensor_new.cpp:281.)\n",
      "  traj = torch.tensor(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        |    8.000  |\n",
      "|    ep_rew_mean        |   -0.133  |\n",
      "|    ep_rew_std         |    0.990  |\n",
      "|    policy_entropy     |    0.083  |\n",
      "|    advantage_mean     |    0.613  |\n",
      "|    advantage_std      |    0.128  |\n",
      "|    aux_loss_mean      |    0.000  |\n",
      "| time/                 |           |\n",
      "|    fps                |      266  |\n",
      "|    episodes           |      100  |\n",
      "|    time_elapsed       |        3  |\n",
      "|    total_timesteps    |      800  |\n",
      "| train/                |           |\n",
      "|    loss               |    0.271  |\n",
      "|    policy_loss        |    0.084  |\n",
      "|    value_loss         |    0.390  |\n",
      "|    explained_variance |   -0.754  |\n",
      "|    n_updates          |      100  |\n",
      "| rnd_net_dist/         |           |\n",
      "|    mean_rnd_bonus     |    0.001  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        |    8.000  |\n",
      "|    ep_rew_mean        |   -0.018  |\n",
      "|    ep_rew_std         |    1.000  |\n",
      "|    policy_entropy     |    0.119  |\n",
      "|    advantage_mean     |    0.170  |\n",
      "|    advantage_std      |    0.168  |\n",
      "|    aux_loss_mean      |    0.000  |\n",
      "| time/                 |           |\n",
      "|    fps                |      228  |\n",
      "|    episodes           |      200  |\n",
      "|    time_elapsed       |        7  |\n",
      "|    total_timesteps    |     1600  |\n",
      "| train/                |           |\n",
      "|    loss               |    0.002  |\n",
      "|    policy_loss        |   -0.013  |\n",
      "|    value_loss         |    0.054  |\n",
      "|    explained_variance |-1054.291  |\n",
      "|    n_updates          |      200  |\n",
      "| rnd_net_dist/         |           |\n",
      "|    mean_rnd_bonus     |    0.000  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        |    8.000  |\n",
      "|    ep_rew_mean        |    0.002  |\n",
      "|    ep_rew_std         |    1.000  |\n",
      "|    policy_entropy     |    0.309  |\n",
      "|    advantage_mean     |    0.024  |\n",
      "|    advantage_std      |    0.087  |\n",
      "|    aux_loss_mean      |    0.000  |\n",
      "| time/                 |           |\n",
      "|    fps                |      218  |\n",
      "|    episodes           |      300  |\n",
      "|    time_elapsed       |       11  |\n",
      "|    total_timesteps    |     2400  |\n",
      "| train/                |           |\n",
      "|    loss               |    0.043  |\n",
      "|    policy_loss        |    0.070  |\n",
      "|    value_loss         |    0.007  |\n",
      "|    explained_variance |  -12.726  |\n",
      "|    n_updates          |      300  |\n",
      "| rnd_net_dist/         |           |\n",
      "|    mean_rnd_bonus     |    0.000  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        |    8.000  |\n",
      "|    ep_rew_mean        |    0.362  |\n",
      "|    ep_rew_std         |    0.933  |\n",
      "|    policy_entropy     |    0.151  |\n",
      "|    advantage_mean     |    0.090  |\n",
      "|    advantage_std      |    0.153  |\n",
      "|    aux_loss_mean      |    0.000  |\n",
      "| time/                 |           |\n",
      "|    fps                |      213  |\n",
      "|    episodes           |      400  |\n",
      "|    time_elapsed       |       15  |\n",
      "|    total_timesteps    |     3200  |\n",
      "| train/                |           |\n",
      "|    loss               |   -0.006  |\n",
      "|    policy_loss        |   -0.005  |\n",
      "|    value_loss         |    0.029  |\n",
      "|    explained_variance |  -13.893  |\n",
      "|    n_updates          |      400  |\n",
      "| rnd_net_dist/         |           |\n",
      "|    mean_rnd_bonus     |    0.000  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        |    8.000  |\n",
      "|    ep_rew_mean        |    0.301  |\n",
      "|    ep_rew_std         |    0.954  |\n",
      "|    policy_entropy     |    0.167  |\n",
      "|    advantage_mean     |    0.219  |\n",
      "|    advantage_std      |    0.142  |\n",
      "|    aux_loss_mean      |    0.000  |\n",
      "| time/                 |           |\n",
      "|    fps                |      210  |\n",
      "|    episodes           |      500  |\n",
      "|    time_elapsed       |       19  |\n",
      "|    total_timesteps    |     4000  |\n",
      "| train/                |           |\n",
      "|    loss               |    0.191  |\n",
      "|    policy_loss        |    0.175  |\n",
      "|    value_loss         |    0.066  |\n",
      "|    explained_variance |   -9.510  |\n",
      "|    n_updates          |      500  |\n",
      "| rnd_net_dist/         |           |\n",
      "|    mean_rnd_bonus     |    0.000  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        |    8.000  |\n",
      "|    ep_rew_mean        |    0.941  |\n",
      "|    ep_rew_std         |    0.341  |\n",
      "|    policy_entropy     |    0.114  |\n",
      "|    advantage_mean     |   -0.028  |\n",
      "|    advantage_std      |    0.051  |\n",
      "|    aux_loss_mean      |    0.000  |\n",
      "| time/                 |           |\n",
      "|    fps                |      208  |\n",
      "|    episodes           |      600  |\n",
      "|    time_elapsed       |       23  |\n",
      "|    total_timesteps    |     4800  |\n",
      "| train/                |           |\n",
      "|    loss               |   -0.442  |\n",
      "|    policy_loss        |   -0.432  |\n",
      "|    value_loss         |    0.003  |\n",
      "|    explained_variance |   -4.587  |\n",
      "|    n_updates          |      600  |\n",
      "| rnd_net_dist/         |           |\n",
      "|    mean_rnd_bonus     |    0.000  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        |    8.000  |\n",
      "|    ep_rew_mean        |    0.921  |\n",
      "|    ep_rew_std         |    0.392  |\n",
      "|    policy_entropy     |    0.085  |\n",
      "|    advantage_mean     |   -0.047  |\n",
      "|    advantage_std      |    0.060  |\n",
      "|    aux_loss_mean      |    0.000  |\n",
      "| time/                 |           |\n",
      "|    fps                |      207  |\n",
      "|    episodes           |      700  |\n",
      "|    time_elapsed       |       27  |\n",
      "|    total_timesteps    |     5600  |\n",
      "| train/                |           |\n",
      "|    loss               |   -0.011  |\n",
      "|    policy_loss        |   -0.005  |\n",
      "|    value_loss         |    0.005  |\n",
      "|    explained_variance |  -10.942  |\n",
      "|    n_updates          |      700  |\n",
      "| rnd_net_dist/         |           |\n",
      "|    mean_rnd_bonus     |    0.000  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        |    8.000  |\n",
      "|    ep_rew_mean        |    1.001  |\n",
      "|    ep_rew_std         |    0.000  |\n",
      "|    policy_entropy     |    0.092  |\n",
      "|    advantage_mean     |   -0.032  |\n",
      "|    advantage_std      |    0.056  |\n",
      "|    aux_loss_mean      |    0.000  |\n",
      "| time/                 |           |\n",
      "|    fps                |      206  |\n",
      "|    episodes           |      800  |\n",
      "|    time_elapsed       |       31  |\n",
      "|    total_timesteps    |     6400  |\n",
      "| train/                |           |\n",
      "|    loss               |   -0.198  |\n",
      "|    policy_loss        |   -0.191  |\n",
      "|    value_loss         |    0.004  |\n",
      "|    explained_variance |   -6.239  |\n",
      "|    n_updates          |      800  |\n",
      "| rnd_net_dist/         |           |\n",
      "|    mean_rnd_bonus     |    0.000  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        |    8.000  |\n",
      "|    ep_rew_mean        |    0.981  |\n",
      "|    ep_rew_std         |    0.199  |\n",
      "|    policy_entropy     |    0.145  |\n",
      "|    advantage_mean     |    0.055  |\n",
      "|    advantage_std      |    0.052  |\n",
      "|    aux_loss_mean      |    0.000  |\n",
      "| time/                 |           |\n",
      "|    fps                |      205  |\n",
      "|    episodes           |      900  |\n",
      "|    time_elapsed       |       35  |\n",
      "|    total_timesteps    |     7200  |\n",
      "| train/                |           |\n",
      "|    loss               |   -0.024  |\n",
      "|    policy_loss        |   -0.012  |\n",
      "|    value_loss         |    0.005  |\n",
      "|    explained_variance |   -1.575  |\n",
      "|    n_updates          |      900  |\n",
      "| rnd_net_dist/         |           |\n",
      "|    mean_rnd_bonus     |    0.000  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        |    8.000  |\n",
      "|    ep_rew_mean        |    1.001  |\n",
      "|    ep_rew_std         |    0.000  |\n",
      "|    policy_entropy     |    0.100  |\n",
      "|    advantage_mean     |    0.025  |\n",
      "|    advantage_std      |    0.074  |\n",
      "|    aux_loss_mean      |    0.000  |\n",
      "| time/                 |           |\n",
      "|    fps                |      205  |\n",
      "|    episodes           |     1000  |\n",
      "|    time_elapsed       |       39  |\n",
      "|    total_timesteps    |     8000  |\n",
      "| train/                |           |\n",
      "|    loss               |   -0.051  |\n",
      "|    policy_loss        |   -0.044  |\n",
      "|    value_loss         |    0.005  |\n",
      "|    explained_variance |   -5.363  |\n",
      "|    n_updates          |     1000  |\n",
      "| rnd_net_dist/         |           |\n",
      "|    mean_rnd_bonus     |    0.000  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        |    8.000  |\n",
      "|    ep_rew_mean        |    1.001  |\n",
      "|    ep_rew_std         |    0.000  |\n",
      "|    policy_entropy     |    0.102  |\n",
      "|    advantage_mean     |   -0.003  |\n",
      "|    advantage_std      |    0.047  |\n",
      "|    aux_loss_mean      |    0.000  |\n",
      "| time/                 |           |\n",
      "|    fps                |      204  |\n",
      "|    episodes           |     1100  |\n",
      "|    time_elapsed       |       43  |\n",
      "|    total_timesteps    |     8800  |\n",
      "| train/                |           |\n",
      "|    loss               |   -0.072  |\n",
      "|    policy_loss        |   -0.062  |\n",
      "|    value_loss         |    0.002  |\n",
      "|    explained_variance |   -2.626  |\n",
      "|    n_updates          |     1100  |\n",
      "| rnd_net_dist/         |           |\n",
      "|    mean_rnd_bonus     |    0.000  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        |    8.000  |\n",
      "|    ep_rew_mean        |    1.001  |\n",
      "|    ep_rew_std         |    0.000  |\n",
      "|    policy_entropy     |    0.131  |\n",
      "|    advantage_mean     |    0.064  |\n",
      "|    advantage_std      |    0.036  |\n",
      "|    aux_loss_mean      |    0.000  |\n",
      "| time/                 |           |\n",
      "|    fps                |      208  |\n",
      "|    episodes           |     1200  |\n",
      "|    time_elapsed       |       46  |\n",
      "|    total_timesteps    |     9600  |\n",
      "| train/                |           |\n",
      "|    loss               |    0.029  |\n",
      "|    policy_loss        |    0.039  |\n",
      "|    value_loss         |    0.005  |\n",
      "|    explained_variance |   -0.376  |\n",
      "|    n_updates          |     1200  |\n",
      "| rnd_net_dist/         |           |\n",
      "|    mean_rnd_bonus     |    0.000  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        |    8.000  |\n",
      "|    ep_rew_mean        |    1.001  |\n",
      "|    ep_rew_std         |    0.000  |\n",
      "|    policy_entropy     |    0.138  |\n",
      "|    advantage_mean     |   -0.011  |\n",
      "|    advantage_std      |    0.041  |\n",
      "|    aux_loss_mean      |    0.000  |\n",
      "| time/                 |           |\n",
      "|    fps                |      203  |\n",
      "|    episodes           |     1300  |\n",
      "|    time_elapsed       |       51  |\n",
      "|    total_timesteps    |    10400  |\n",
      "| train/                |           |\n",
      "|    loss               |   -0.041  |\n",
      "|    policy_loss        |   -0.028  |\n",
      "|    value_loss         |    0.002  |\n",
      "|    explained_variance |   -2.106  |\n",
      "|    n_updates          |     1300  |\n",
      "| rnd_net_dist/         |           |\n",
      "|    mean_rnd_bonus     |    0.000  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        |    8.000  |\n",
      "|    ep_rew_mean        |    0.981  |\n",
      "|    ep_rew_std         |    0.199  |\n",
      "|    policy_entropy     |    0.109  |\n",
      "|    advantage_mean     |   -0.015  |\n",
      "|    advantage_std      |    0.050  |\n",
      "|    aux_loss_mean      |    0.000  |\n",
      "| time/                 |           |\n",
      "|    fps                |      207  |\n",
      "|    episodes           |     1400  |\n",
      "|    time_elapsed       |       54  |\n",
      "|    total_timesteps    |    11200  |\n",
      "| train/                |           |\n",
      "|    loss               |    0.006  |\n",
      "|    policy_loss        |    0.016  |\n",
      "|    value_loss         |    0.002  |\n",
      "|    explained_variance |   -4.459  |\n",
      "|    n_updates          |     1400  |\n",
      "| rnd_net_dist/         |           |\n",
      "|    mean_rnd_bonus     |    0.000  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        |    8.000  |\n",
      "|    ep_rew_mean        |    0.981  |\n",
      "|    ep_rew_std         |    0.199  |\n",
      "|    policy_entropy     |    0.122  |\n",
      "|    advantage_mean     |   -0.004  |\n",
      "|    advantage_std      |    0.052  |\n",
      "|    aux_loss_mean      |    0.000  |\n",
      "| time/                 |           |\n",
      "|    fps                |      206  |\n",
      "|    episodes           |     1500  |\n",
      "|    time_elapsed       |       58  |\n",
      "|    total_timesteps    |    12000  |\n",
      "| train/                |           |\n",
      "|    loss               |   -0.014  |\n",
      "|    policy_loss        |   -0.003  |\n",
      "|    value_loss         |    0.002  |\n",
      "|    explained_variance |   -3.744  |\n",
      "|    n_updates          |     1500  |\n",
      "| rnd_net_dist/         |           |\n",
      "|    mean_rnd_bonus     |    0.000  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        |    8.000  |\n",
      "|    ep_rew_mean        |    1.001  |\n",
      "|    ep_rew_std         |    0.000  |\n",
      "|    policy_entropy     |    0.102  |\n",
      "|    advantage_mean     |   -0.002  |\n",
      "|    advantage_std      |    0.045  |\n",
      "|    aux_loss_mean      |    0.000  |\n",
      "| time/                 |           |\n",
      "|    fps                |      206  |\n",
      "|    episodes           |     1600  |\n",
      "|    time_elapsed       |       62  |\n",
      "|    total_timesteps    |    12800  |\n",
      "| train/                |           |\n",
      "|    loss               |   -0.070  |\n",
      "|    policy_loss        |   -0.060  |\n",
      "|    value_loss         |    0.002  |\n",
      "|    explained_variance |   -2.223  |\n",
      "|    n_updates          |     1600  |\n",
      "| rnd_net_dist/         |           |\n",
      "|    mean_rnd_bonus     |    0.000  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        |    8.000  |\n",
      "|    ep_rew_mean        |    1.001  |\n",
      "|    ep_rew_std         |    0.000  |\n",
      "|    policy_entropy     |    0.104  |\n",
      "|    advantage_mean     |   -0.007  |\n",
      "|    advantage_std      |    0.029  |\n",
      "|    aux_loss_mean      |    0.000  |\n",
      "| time/                 |           |\n",
      "|    fps                |      206  |\n",
      "|    episodes           |     1700  |\n",
      "|    time_elapsed       |       66  |\n",
      "|    total_timesteps    |    13600  |\n",
      "| train/                |           |\n",
      "|    loss               |   -0.053  |\n",
      "|    policy_loss        |   -0.043  |\n",
      "|    value_loss         |    0.001  |\n",
      "|    explained_variance |   -0.443  |\n",
      "|    n_updates          |     1700  |\n",
      "| rnd_net_dist/         |           |\n",
      "|    mean_rnd_bonus     |    0.000  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        |    8.000  |\n",
      "|    ep_rew_mean        |    0.981  |\n",
      "|    ep_rew_std         |    0.199  |\n",
      "|    policy_entropy     |    0.106  |\n",
      "|    advantage_mean     |    0.003  |\n",
      "|    advantage_std      |    0.036  |\n",
      "|    aux_loss_mean      |    0.000  |\n",
      "| time/                 |           |\n",
      "|    fps                |      205  |\n",
      "|    episodes           |     1800  |\n",
      "|    time_elapsed       |       70  |\n",
      "|    total_timesteps    |    14400  |\n",
      "| train/                |           |\n",
      "|    loss               |    0.005  |\n",
      "|    policy_loss        |    0.015  |\n",
      "|    value_loss         |    0.001  |\n",
      "|    explained_variance |   -1.504  |\n",
      "|    n_updates          |     1800  |\n",
      "| rnd_net_dist/         |           |\n",
      "|    mean_rnd_bonus     |    0.000  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        |    8.000  |\n",
      "|    ep_rew_mean        |    1.001  |\n",
      "|    ep_rew_std         |    0.000  |\n",
      "|    policy_entropy     |    0.101  |\n",
      "|    advantage_mean     |    0.032  |\n",
      "|    advantage_std      |    0.041  |\n",
      "|    aux_loss_mean      |    0.000  |\n",
      "| time/                 |           |\n",
      "|    fps                |      205  |\n",
      "|    episodes           |     1900  |\n",
      "|    time_elapsed       |       74  |\n",
      "|    total_timesteps    |    15200  |\n",
      "| train/                |           |\n",
      "|    loss               |   -0.029  |\n",
      "|    policy_loss        |   -0.020  |\n",
      "|    value_loss         |    0.003  |\n",
      "|    explained_variance |   -1.111  |\n",
      "|    n_updates          |     1900  |\n",
      "| rnd_net_dist/         |           |\n",
      "|    mean_rnd_bonus     |    0.000  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        |    8.000  |\n",
      "|    ep_rew_mean        |    0.981  |\n",
      "|    ep_rew_std         |    0.199  |\n",
      "|    policy_entropy     |    0.107  |\n",
      "|    advantage_mean     |    0.059  |\n",
      "|    advantage_std      |    0.046  |\n",
      "|    aux_loss_mean      |    0.000  |\n",
      "| time/                 |           |\n",
      "|    fps                |      205  |\n",
      "|    episodes           |     2000  |\n",
      "|    time_elapsed       |       78  |\n",
      "|    total_timesteps    |    16000  |\n",
      "| train/                |           |\n",
      "|    loss               |    0.076  |\n",
      "|    policy_loss        |    0.084  |\n",
      "|    value_loss         |    0.005  |\n",
      "|    explained_variance |   -1.379  |\n",
      "|    n_updates          |     2000  |\n",
      "| rnd_net_dist/         |           |\n",
      "|    mean_rnd_bonus     |    0.000  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        |    8.000  |\n",
      "|    ep_rew_mean        |    1.001  |\n",
      "|    ep_rew_std         |    0.000  |\n",
      "|    policy_entropy     |    0.096  |\n",
      "|    advantage_mean     |    0.036  |\n",
      "|    advantage_std      |    0.017  |\n",
      "|    aux_loss_mean      |    0.000  |\n",
      "| time/                 |           |\n",
      "|    fps                |      204  |\n",
      "|    episodes           |     2100  |\n",
      "|    time_elapsed       |       82  |\n",
      "|    total_timesteps    |    16800  |\n",
      "| train/                |           |\n",
      "|    loss               |    0.002  |\n",
      "|    policy_loss        |    0.010  |\n",
      "|    value_loss         |    0.002  |\n",
      "|    explained_variance |    0.639  |\n",
      "|    n_updates          |     2100  |\n",
      "| rnd_net_dist/         |           |\n",
      "|    mean_rnd_bonus     |    0.000  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        |    8.000  |\n",
      "|    ep_rew_mean        |    1.001  |\n",
      "|    ep_rew_std         |    0.000  |\n",
      "|    policy_entropy     |    0.109  |\n",
      "|    advantage_mean     |    0.001  |\n",
      "|    advantage_std      |    0.027  |\n",
      "|    aux_loss_mean      |    0.000  |\n",
      "| time/                 |           |\n",
      "|    fps                |      204  |\n",
      "|    episodes           |     2200  |\n",
      "|    time_elapsed       |       86  |\n",
      "|    total_timesteps    |    17600  |\n",
      "| train/                |           |\n",
      "|    loss               |    0.018  |\n",
      "|    policy_loss        |    0.028  |\n",
      "|    value_loss         |    0.001  |\n",
      "|    explained_variance |   -0.332  |\n",
      "|    n_updates          |     2200  |\n",
      "| rnd_net_dist/         |           |\n",
      "|    mean_rnd_bonus     |    0.000  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        |    8.000  |\n",
      "|    ep_rew_mean        |    0.981  |\n",
      "|    ep_rew_std         |    0.199  |\n",
      "|    policy_entropy     |    0.103  |\n",
      "|    advantage_mean     |   -0.003  |\n",
      "|    advantage_std      |    0.039  |\n",
      "|    aux_loss_mean      |    0.000  |\n",
      "| time/                 |           |\n",
      "|    fps                |      204  |\n",
      "|    episodes           |     2300  |\n",
      "|    time_elapsed       |       90  |\n",
      "|    total_timesteps    |    18400  |\n",
      "| train/                |           |\n",
      "|    loss               |    0.038  |\n",
      "|    policy_loss        |    0.048  |\n",
      "|    value_loss         |    0.001  |\n",
      "|    explained_variance |   -1.996  |\n",
      "|    n_updates          |     2300  |\n",
      "| rnd_net_dist/         |           |\n",
      "|    mean_rnd_bonus     |    0.000  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        |    8.000  |\n",
      "|    ep_rew_mean        |    0.981  |\n",
      "|    ep_rew_std         |    0.199  |\n",
      "|    policy_entropy     |    0.110  |\n",
      "|    advantage_mean     |   -0.039  |\n",
      "|    advantage_std      |    0.034  |\n",
      "|    aux_loss_mean      |    0.000  |\n",
      "| time/                 |           |\n",
      "|    fps                |      204  |\n",
      "|    episodes           |     2400  |\n",
      "|    time_elapsed       |       94  |\n",
      "|    total_timesteps    |    19200  |\n",
      "| train/                |           |\n",
      "|    loss               |   -0.009  |\n",
      "|    policy_loss        |    0.000  |\n",
      "|    value_loss         |    0.003  |\n",
      "|    explained_variance |   -2.482  |\n",
      "|    n_updates          |     2400  |\n",
      "| rnd_net_dist/         |           |\n",
      "|    mean_rnd_bonus     |    0.000  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        |    8.000  |\n",
      "|    ep_rew_mean        |    1.001  |\n",
      "|    ep_rew_std         |    0.000  |\n",
      "|    policy_entropy     |    0.103  |\n",
      "|    advantage_mean     |    0.001  |\n",
      "|    advantage_std      |    0.034  |\n",
      "|    aux_loss_mean      |    0.000  |\n",
      "| time/                 |           |\n",
      "|    fps                |      204  |\n",
      "|    episodes           |     2500  |\n",
      "|    time_elapsed       |       98  |\n",
      "|    total_timesteps    |    20000  |\n",
      "| train/                |           |\n",
      "|    loss               |   -0.073  |\n",
      "|    policy_loss        |   -0.063  |\n",
      "|    value_loss         |    0.001  |\n",
      "|    explained_variance |   -0.917  |\n",
      "|    n_updates          |     2500  |\n",
      "| rnd_net_dist/         |           |\n",
      "|    mean_rnd_bonus     |    0.000  |\n",
      "-------------------------------------\n",
      "Training complete. Total episodes: 2500, total steps: 20000\n"
     ]
    }
   ],
   "source": [
    "# ENVIRONMENT =============================\n",
    "\n",
    "env = MemoryTaskEnv(delay=DELAY, difficulty=0)\n",
    "\n",
    "# MEMORY BUFFER ===========================\n",
    "# POLICY NETWORK (use class) ==============\n",
    "# AGENT SETUP =============================\n",
    "# TRAIN THE AGENT =========================\n",
    "memory = StrategicMemoryBuffer(\n",
    "    obs_dim=env.observation_space.shape[0],\n",
    "    action_dim=1,          # For Discrete(2)\n",
    "    mem_dim=MEM_DIM,\n",
    "    max_entries=N_MEMORIES,\n",
    "    device=\"cpu\"\n",
    ")\n",
    "\n",
    "# Policy Network (use class, not instance)\n",
    "policy = MemoryTransformerPolicy\n",
    "\n",
    "# (Optional) Auxiliary Modules \n",
    "# aux_modules = [\n",
    "#     CueAuxModule(feat_dim=MEM_DIM*2, n_classes=2),\n",
    "#     ConfidenceAuxModule(feat_dim=MEM_DIM*2)\n",
    "# ]\n",
    "\n",
    "# Agent Setup\n",
    "agent = StrategicMemoryAgent(\n",
    "    policy_class=policy,\n",
    "    env=env,\n",
    "    memory=memory,\n",
    "    # aux_modules=aux_modules,  \n",
    "    **AGENT_KWARGS\n",
    ")\n",
    "\n",
    "# Train the agent \n",
    "agent.learn(\n",
    "    total_timesteps=total_timesteps(DELAY, N_EPISODES),\n",
    "    log_interval=100\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7e26209-9156-4ebd-b90a-77aef5e4b5c8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:trading]",
   "language": "python",
   "name": "conda-env-trading-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
